{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import falconn\n",
    "import pyphi\n",
    "import requests\n",
    "import random\n",
    "import psycopg2\n",
    "from configparser import ConfigParser\n",
    "import pickle\n",
    "import csv\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "target_dir = \"/home/jupyter/notebooks/PoC/data-preparation/output/understanding_data/\"\n",
    "\n",
    "def config(filename='prepare_data.ini', section='phonetic'):\n",
    "    parser = ConfigParser()\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    params = config()\n",
    "    conn = psycopg2.connect(**params)\n",
    "    print('Connected to the PostgreSQL database...')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def read_given_names():\n",
    "    conn = db_connect()\n",
    "    sql_result = pd.DataFrame()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM (\n",
    "                SELECT ENG, ARB, SUM(COUNT) AS COUNT FROM (\n",
    "                    SELECT ENG, ARB, COUNT FROM GIVEN_NAMES_MASTER\n",
    "                    WHERE ENG IS NOT NULL AND ENG != '' AND ARB IS NOT NULL AND ARB != ''\n",
    "                ) AS SUB GROUP BY ENG, ARB\n",
    "                ORDER BY COUNT DESC\n",
    "            ) AS S\n",
    "            UNION ALL\n",
    "            SELECT * FROM (\n",
    "                SELECT ENG, ARB, SUM(COUNT) AS COUNT FROM (\n",
    "                    SELECT ENG, ARB, COUNT FROM FAMILY_NAMES_MASTER\n",
    "                    WHERE ENG IS NOT NULL AND ENG != '' AND ARB IS NOT NULL AND ARB != ''\n",
    "                ) AS SUB GROUP BY ENG, ARB\n",
    "                ORDER BY COUNT DESC\n",
    "            ) AS S\n",
    "            UNION ALL\n",
    "            SELECT * FROM (\n",
    "                SELECT ENG, ARB, SUM(FREQ) AS COUNT FROM (\n",
    "                    SELECT ENG, ARB, FREQ FROM GIVEN_NAMES_DAN\n",
    "                    WHERE ENG IS NOT NULL AND ENG != '' AND ARB IS NOT NULL AND ARB != ''\n",
    "                ) AS SUB GROUP BY ENG, ARB\n",
    "                ORDER BY COUNT DESC\n",
    "            ) AS S\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        sql_result = pd.read_sql(query, con=conn)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return sql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_top_given_names(min_count):\n",
    "    global top_given_names, all_eng_given_names, all_arb_given_names, low_accuracy_names\n",
    "    given_names = read_given_names()\n",
    "    given_names['trimmed_eng'] = given_names['eng'].str.strip()\n",
    "    given_names['trimmed_arb'] = given_names['arb'].str.strip()\n",
    "\n",
    "    #given_names['trimmed_eng'] = given_names['eng'].map(lambda x: x.strip(' '))\n",
    "    #given_names['trimmed_arb'] = given_names['arb'].map(lambda x: x.strip(' '))\n",
    "    given_names['count'] = given_names.groupby(['trimmed_eng', 'trimmed_arb'])['count'].transform('sum')\n",
    "    given_names = given_names.drop_duplicates(subset=['trimmed_eng', 'trimmed_arb'], keep='first')\n",
    "    \n",
    "    low_accuracy_threshold = 20\n",
    "    if min_count < low_accuracy_threshold:\n",
    "        top_given_names = given_names[given_names['count'] >= low_accuracy_threshold]\n",
    "        top_given_names = top_given_names.copy()\n",
    "        low_accuracy_names = given_names[(given_names['count'] >= min_count) & (given_names['count'] < low_accuracy_threshold)]\n",
    "        low_accuracy_names = low_accuracy_names.copy()\n",
    "    else:\n",
    "        top_given_names = given_names[given_names['count'] >= min_count]\n",
    "        top_given_names = top_given_names.copy()\n",
    "        low_accuracy_names = pd.DataFrame(columns=['eng', 'arb', 'count', 'trimmed_eng', 'trimmed_arb', 'eng_variants', 'arb_variants'])\n",
    "    \n",
    "    all_eng_given_names = list(set(top_given_names['trimmed_eng'].tolist()))\n",
    "    all_arb_given_names = list(set(top_given_names['trimmed_arb'].tolist()))\n",
    "    #random.shuffle(all_eng_given_names)\n",
    "    #random.shuffle(all_arb_given_names)\n",
    "    \n",
    "def format_variants_list(s):\n",
    "    return [','.join(sorted(set(s.tolist())))]\n",
    "\n",
    "def format_variants_list2(s):\n",
    "    lis = s.tolist()\n",
    "    flat_list = [item for sublist in lis for item in sublist.split(',')]\n",
    "\n",
    "    return ','.join(sorted(set(flat_list)))\n",
    "\n",
    "def get_variants_count(s):\n",
    "    return len(set(s.tolist()))\n",
    "\n",
    "def get_yamli_arabic_varinats(name):\n",
    "    name = ''.join(e for e in name if e.isalnum())\n",
    "    url = 'http://api.yamli.com/transliterate.ashx?tool=api&account_id=&prot=http%3A&hostname=fuzzyarabic.herokuapp.com&path=%2F&build=5515&sxhr_id=9&word=' + name \n",
    "    \n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if len(response.text) > 66:\n",
    "        arr = pyphi.jsonify.loads(response.text[62:-4])['data']\n",
    "        data = pyphi.jsonify.loads(arr)['r']\n",
    "        variants = data.split('|')\n",
    "    else:\n",
    "        variants = []\n",
    "    variants = [''.join(e for e in name if e.isalnum() and not e.isdigit()) for name in variants]\n",
    "    return variants\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(target_dir + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(target_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "i = 0\n",
    "def add_to_yamli_dict(eng):\n",
    "    global i\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=len(all_eng_given_names)))\n",
    "    eng_names_with_yamli_dict[eng] = get_yamli_arabic_varinats(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_falconn_index(name, lhs_table, number, names, vectors):\n",
    "    query = call_embedding_ws(name)\n",
    "    response = lhs_table.find_k_nearest_neighbors(query[0], k=number)\n",
    "    \n",
    "    return process_lhs_table_response(query, response, names, vectors)\n",
    "\n",
    "def process_lhs_table_response(query, response, names, vectors):\n",
    "    df = pd.DataFrame(index=range(len(response)), columns=['id', 'name', 'cosine'])   \n",
    "    i = 0\n",
    "    \n",
    "    for resp in response:\n",
    "        name = names.get_value(resp, 'name')\n",
    "        cos = cos_similarity(query, vectors[resp])\n",
    "        df.set_value(index=i, col='id', value=resp)\n",
    "        df.set_value(index=i, col='name', value=name)\n",
    "        df.set_value(index=i, col='cosine', value=cos)\n",
    "        i = i + 1\n",
    "     \n",
    "    df = df.sort_values(by='cosine', ascending=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def cos_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "    return dot_product\n",
    "\n",
    "def call_embedding_ws(names):\n",
    "    names = [item.lower() for item in names]\n",
    "    \n",
    "#    url = 'http://54.36.53.127:8009/embedding'\n",
    "    url = 'http://127.0.0.1:8009/embedding'\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    response = requests.post(url, json=names)\n",
    "    \n",
    "    arr = pyphi.jsonify.loads(response.text)\n",
    "    x = np.array(arr)\n",
    "    x = x.astype(dtype=np.float32)\n",
    "    \n",
    "    return x\n",
    "\n",
    "input_path = 'data/src_distinct_names/src'\n",
    "output_path = 'data/src_distinct_names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "directory = os.fsencode(output_path)\n",
    "names_dict = {}\n",
    "vectors_dict = {}\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".csv\"): \n",
    "        print(filename)\n",
    "        \n",
    "        names_dict[filename] = pd.read_csv(output_path + '/' + filename, sep='|', names=['name', 'vector_string'], header=None)\n",
    "        \n",
    "        vector_strings = names_dict[filename]['vector_string'].as_matrix()\n",
    "        vectors = np.zeros(shape=(len(vector_strings),256))\n",
    "        i = 0\n",
    "        for v in vector_strings:\n",
    "            try:\n",
    "                x = np.fromstring(v, dtype=np.float32, sep=',')\n",
    "                vectors[i] = x\n",
    "                i = i+1\n",
    "            except:\n",
    "                print(\"{i} {v}\".format(i=i, v=v))\n",
    "                raise\n",
    "            \n",
    "        vectors_dict[filename] = vectors.astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eng_names_merged =  pd.DataFrame(columns=['name', 'vector_string'])\n",
    "arb_names_merged = pd.DataFrame(columns=['name', 'vector_string'])\n",
    "\n",
    "\n",
    "for key, value in names_dict.items():\n",
    "    if 'eng' in key:\n",
    "        eng_names_merged = eng_names_merged.append(value).reset_index(drop=True)\n",
    "\n",
    "eng_names_merged = eng_names_merged.drop_duplicates(['name']).reset_index(drop=True)\n",
    "eng_names_merged = eng_names_merged.reset_index(drop=True)\n",
    "        \n",
    "vector_strings = eng_names_merged['vector_string'].as_matrix()\n",
    "eng_vectors_merged = np.zeros(shape=(len(vector_strings),256))\n",
    "i = 0\n",
    "for v in vector_strings:\n",
    "    x = np.fromstring(v, dtype=np.float32, sep=',')\n",
    "    eng_vectors_merged[i] = x\n",
    "    i = i+1\n",
    "    \n",
    "eng_vectors_merged = eng_vectors_merged.astype(dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "for key, value in names_dict.items():\n",
    "    if 'arb' in key:\n",
    "        arb_names_merged = arb_names_merged.append(value).reset_index(drop=True)\n",
    "\n",
    "arb_names_merged = arb_names_merged.drop_duplicates(['name']).reset_index(drop=True)\n",
    "arb_names_merged = arb_names_merged.reset_index(drop=True)\n",
    "        \n",
    "vector_strings = arb_names_merged['vector_string'].as_matrix()\n",
    "arb_vectors_merged = np.zeros(shape=(len(vector_strings),256))\n",
    "i = 0\n",
    "for v in vector_strings:\n",
    "    x = np.fromstring(v, dtype=np.float32, sep=',')\n",
    "    arb_vectors_merged[i] = x\n",
    "    i = i+1\n",
    "arb_vectors_merged = arb_vectors_merged.astype(dtype=np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "number_of_tables = 50\n",
    "assert eng_vectors_merged.dtype == np.float32\n",
    "assert arb_vectors_merged.dtype == np.float32\n",
    "\n",
    "params_cp = falconn.LSHConstructionParameters()\n",
    "params_cp.dimension = len(eng_vectors_merged[0])\n",
    "params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
    "params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
    "params_cp.l = number_of_tables\n",
    "# we set one rotation, since the data is dense enough,\n",
    "# for sparse data set it to 2\n",
    "params_cp.num_rotations = 2\n",
    "params_cp.seed = 5721840\n",
    "# we want to use all the available threads to set up\n",
    "params_cp.num_setup_threads = 0\n",
    "params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
    "# we build 24-bit hashes so that each table has\n",
    "# 2^24 bins; this is a good choise since 2^24 is of the same\n",
    "# order of magnitude as the number of data points\n",
    "falconn.compute_number_of_hash_functions(18, params_cp)\n",
    "\n",
    "eng_merged_table = falconn.LSHIndex(params_cp)\n",
    "eng_merged_table.setup(eng_vectors_merged)\n",
    "\n",
    "eng_merged_query_object = eng_merged_table.construct_query_object()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params_cp = falconn.LSHConstructionParameters()\n",
    "params_cp.dimension = len(arb_vectors_merged[0])\n",
    "params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
    "params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
    "params_cp.l = number_of_tables\n",
    "# we set one rotation, since the data is dense enough,\n",
    "# for sparse data set it to 2\n",
    "params_cp.num_rotations = 2\n",
    "params_cp.seed = 5721840\n",
    "# we want to use all the available threads to set up\n",
    "params_cp.num_setup_threads = 0\n",
    "params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
    "# we build 24-bit hashes so that each table has\n",
    "# 2^24 bins; this is a good choise since 2^24 is of the same\n",
    "# order of magnitude as the number of data points\n",
    "falconn.compute_number_of_hash_functions(18, params_cp)\n",
    "\n",
    "arb_merged_table = falconn.LSHIndex(params_cp)\n",
    "arb_merged_table.setup(arb_vectors_merged)\n",
    "\n",
    "arb_merged_query_object = arb_merged_table.construct_query_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the PostgreSQL database...\n",
      "226614\n",
      "115182\n",
      "80855\n",
      "CPU times: user 3.61 s, sys: 116 ms, total: 3.73 s\n",
      "Wall time: 9.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_top_given_names(20)\n",
    "print(len(top_given_names))\n",
    "print(len(set(all_eng_given_names)))\n",
    "print(len(set(all_arb_given_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 28.7 ms, total: 16.3 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_len = len(top_given_names)\n",
    "eng_names_with_yamli_dict = load_obj('eng_names_with_yamli_dict')\n",
    "\n",
    "for index, row in top_given_names.iterrows():\n",
    "    name = row['trimmed_eng']\n",
    "    \n",
    "    yamli_variants = eng_names_with_yamli_dict[name]\n",
    "    top_given_names.set_value(index, 'yamli_variants', ','.join(set(yamli_variants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 s, sys: 210 ms, total: 41.2 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eng_name_with_variants = top_given_names.copy()\n",
    "eng_name_with_variants['arb_variants'] = eng_name_with_variants.groupby('trimmed_eng')['arb'].transform(format_variants_list)\n",
    "#eng_name_with_variants['arb_variants'] = eng_name_with_variants.groupby('trimmed_eng')['arb'].transform(format_variants_list)\n",
    "#eng_name_with_variants['arb_variants_count'] = eng_name_with_variants.groupby('trimmed_eng')['arb'].transform(get_variants_count)\n",
    "#eng_name_with_variants['eng_variants'] = eng_name_with_variants.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "\n",
    "arb_name_with_variants = top_given_names.copy()\n",
    "arb_name_with_variants['eng_variants'] = arb_name_with_variants.groupby('trimmed_arb')['eng'].transform(format_variants_list)\n",
    "#arb_name_with_variants['arb_variants_count'] = arb_name_with_variants.groupby('trimmed_arb')['eng'].transform(get_variants_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 1.31 s, total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cols_to_use = eng_name_with_variants.columns.difference(arb_name_with_variants.columns)\n",
    "#eee = pd.merge(eng_name_with_variants, arb_name_with_variants, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "#eee = (pd.merge(eng_name_with_variants, arb_name_with_variants, on='trimmed_eng', how='outer'))\n",
    "aaa = (pd.merge(arb_name_with_variants, eng_name_with_variants, on='trimmed_arb', how='outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eee = eee.drop_duplicates(subset=['trimmed_eng','eng_variants'], keep='first')\n",
    "eee['eng_variants'] = eee.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "eee = eee.drop_duplicates(subset=['trimmed_eng','eng_variants'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del eee['trimmed_eng']\n",
    "del eee['arb_x']\n",
    "del eee['count_x']\n",
    "del eee['trimmed_arb_x']\n",
    "del eee['eng_y']\n",
    "del eee['arb_y']\n",
    "del eee['count_y']\n",
    "del eee['trimmed_arb_y']\n",
    "del eee['yamli_variants_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eee.to_csv(target_dir + 'eng_names_with_variants.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.7 ms, sys: 5.65 ms, total: 67.3 ms\n",
      "Wall time: 64.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aaa = aaa.drop_duplicates(subset=['trimmed_arb','arb_variants'], keep='first')\n",
    "aaa['arb_variants'] = aaa.groupby('trimmed_arb')['arb_variants'].transform(format_variants_list2)\n",
    "aaa = aaa.drop_duplicates(subset=['trimmed_arb','arb_variants'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del aaa['trimmed_arb']\n",
    "del aaa['eng_x']\n",
    "del aaa['count_x']\n",
    "del aaa['trimmed_eng_x']\n",
    "del aaa['eng_y']\n",
    "del aaa['arb_y']\n",
    "del aaa['count_y']\n",
    "del aaa['trimmed_eng_y']\n",
    "del aaa['yamli_variants_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaa.to_csv(target_dir + 'arb_names_with_variants.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_names_with_yamli_dict = {key: [] for key in all_eng_given_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eng_merged_query_object' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-47d4abf7f5f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery_falconn_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'خالد'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_merged_query_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_names_merged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_vectors_merged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eng_merged_query_object' is not defined"
     ]
    }
   ],
   "source": [
    "query_falconn_index(['خالد'], eng_merged_query_object, 50, eng_names_merged, eng_vectors_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "pool = ThreadPool(20)\n",
    "results = pool.map(add_to_yamli_dict, all_eng_given_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for name, arr in eng_names_with_yamli_dict.items():\n",
    "    if len(arr) < 1:\n",
    "        i += 1\n",
    "        print(name)\n",
    "        add_to_yamli_dict(name)\n",
    "        \n",
    "save_obj(eng_names_with_yamli_dict, 'eng_names_with_yamli_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_eng_model_predictions_dict = {key: [] for key in all_eng_given_names}\n",
    "eng_arb_model_predictions_dict = {key: [] for key in all_eng_given_names}\n",
    "arb_eng_model_predictions_dict = {key: [] for key in all_arb_given_names}\n",
    "arb_arb_model_predictions_dict = {key: [] for key in all_arb_given_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "i = 0\n",
    "for name in all_eng_given_names:\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=len(all_eng_given_names)))\n",
    "    eng_predictions = query_falconn_index([name], eng_merged_query_object, 50, eng_names_merged, eng_vectors_merged)\n",
    "    eng_predictions = eng_predictions['name'].tolist()\n",
    "    eng_predictions = [a.strip(' ') for a in eng_predictions]\n",
    "    eng_eng_model_predictions_dict[name] = eng_predictions\n",
    "\n",
    "    arb_predictions = query_falconn_index([name], arb_merged_query_object, 50, arb_names_merged, arb_vectors_merged)\n",
    "    arb_predictions = arb_predictions['name'].tolist()\n",
    "    arb_predictions = [a.strip(' ') for a in arb_predictions]\n",
    "    eng_arb_model_predictions_dict[name] = arb_predictions\n",
    "\n",
    "save_obj(eng_arb_model_predictions_dict, 'eng_arb_model_predictions_dict')\n",
    "save_obj(eng_eng_model_predictions_dict, 'eng_eng_model_predictions_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for name in all_arb_given_names:\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=len(all_eng_given_names)))\n",
    "    eng_predictions = query_falconn_index([name], eng_merged_query_object, 50, eng_names_merged, eng_vectors_merged)\n",
    "    eng_predictions = eng_predictions['name'].tolist()\n",
    "    eng_predictions = [a.strip(' ') for a in eng_predictions]\n",
    "    arb_eng_model_predictions_dict[name] = eng_predictions\n",
    "\n",
    "    arb_predictions = query_falconn_index([name], arb_merged_query_object, 50, arb_names_merged, arb_vectors_merged)\n",
    "    arb_predictions = arb_predictions['name'].tolist()\n",
    "    arb_predictions = [a.strip(' ') for a in arb_predictions]\n",
    "    arb_arb_model_predictions_dict[name] = arb_predictions\n",
    "\n",
    "save_obj(arb_arb_model_predictions_dict, 'arb_arb_model_predictions_dict')\n",
    "save_obj(arb_eng_model_predictions_dict, 'arb_eng_model_predictions_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for name, arr in eng_eng_model_predictions_dict.items():\n",
    "    if len(arr) < 1:\n",
    "        i += 1\n",
    "        print(name)\n",
    "        add_to_eng_based_model_dict(name)\n",
    "        \n",
    "save_obj(eng_eng_model_predictions_dict, 'eng_eng_model_predictions_dict')\n",
    "\n",
    "for name, arr in eng_arb_model_predictions_dict.items():\n",
    "    if len(arr) < 1:\n",
    "        i += 1\n",
    "        print(name)\n",
    "        add_to_eng_based_model_dict(name)\n",
    "        \n",
    "save_obj(eng_arb_model_predictions_dict, 'eng_arb_model_predictions_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.28 s, sys: 247 ms, total: 6.52 s\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eng_names_with_yamli_dict = load_obj('eng_names_with_yamli_dict')\n",
    "eng_arb_model_predictions_dict = load_obj('eng_arb_model_predictions_dict')\n",
    "eng_eng_model_predictions_dict = load_obj('eng_eng_model_predictions_dict')\n",
    "arb_arb_model_predictions_dict = load_obj('arb_arb_model_predictions_dict')\n",
    "arb_eng_model_predictions_dict = load_obj('arb_eng_model_predictions_dict')\n",
    "\n",
    "eng_names_with_variants = pd.read_csv(target_dir + 'eng_names_with_variants.tsv',sep='\\t', header=None, names=['name', 'yamli_variants', 'arb_variants', 'eng_variants', 'eng_model_predictions', 'arb_model_predictions'])\n",
    "arb_names_with_variants = pd.read_csv(target_dir + 'arb_names_with_variants.tsv',sep='\\t', header=None, names=['name', 'yamli_variants', 'eng_variants', 'arb_variants', 'eng_model_predictions', 'arb_model_predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 out of 115182\n",
      "processed 20000 out of 115182\n",
      "processed 30000 out of 115182\n",
      "processed 40000 out of 115182\n",
      "processed 50000 out of 115182\n",
      "processed 60000 out of 115182\n",
      "processed 70000 out of 115182\n",
      "processed 80000 out of 115182\n",
      "processed 90000 out of 115182\n",
      "processed 100000 out of 115182\n",
      "processed 110000 out of 115182\n"
     ]
    }
   ],
   "source": [
    "df = eng_names_with_variants\n",
    "i = 0\n",
    "set_len = len(df)\n",
    "for index, row in df.iterrows():\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=set_len))\n",
    "        \n",
    "    name = row['name'].strip(' ')\n",
    "    eng_variants = row['eng_variants'].split(',')\n",
    "    arb_variants = row['arb_variants'].split(',')\n",
    "    arb_variants = list(set([araby.strip_tashkeel(arb) for arb in arb_variants]))\n",
    "    yamli_variants = row['yamli_variants'].split(',')\n",
    "    yamli_variants = list(set([araby.strip_tashkeel(arb) for arb in yamli_variants]))\n",
    "\n",
    "    eng_eng_model_predictions = list(set(set(eng_eng_model_predictions_dict[name]) - set(eng_variants)))\n",
    "    arb_model = eng_arb_model_predictions_dict[name]\n",
    "    arb_model = list(set([araby.strip_tashkeel(arb) for arb in arb_model]))\n",
    "    eng_arb_model_predictions = list(set(set(arb_model) - set(arb_variants) - set(yamli_variants)))\n",
    "    yamli_variants = set(yamli_variants) - set(arb_variants)    \n",
    "\n",
    "    df.set_value(index, 'yamli_variants', ','.join(set(yamli_variants)))\n",
    "    df.set_value(index, 'eng_eng_model_predictions', ','.join(set(eng_eng_model_predictions)))\n",
    "    df.set_value(index, 'eng_arb_model_predictions', ','.join(set(eng_arb_model_predictions)))\n",
    "    #df.set_value(index, 'arb_eng_model_predictions', ','.join(set(arb_eng_model_predictions)))\n",
    "    #df.set_value(index, 'arb_arb_model_predictions', ','.join(set(arb_arb_model_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_names_with_variants.to_csv(target_dir + 'eng_names_with_variants.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 out of 80855\n",
      "processed 20000 out of 80855\n",
      "processed 30000 out of 80855\n",
      "processed 40000 out of 80855\n",
      "processed 50000 out of 80855\n",
      "processed 60000 out of 80855\n",
      "processed 70000 out of 80855\n",
      "processed 80000 out of 80855\n"
     ]
    }
   ],
   "source": [
    "df = arb_names_with_variants\n",
    "i = 0\n",
    "set_len = len(df)\n",
    "for index, row in df.iterrows():\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=set_len))\n",
    "        \n",
    "    name = row['name'].strip(' ')\n",
    "    eng_variants = row['eng_variants'].split(',')\n",
    "    arb_variants = row['arb_variants'].split(',')\n",
    "    arb_variants = list(set([araby.strip_tashkeel(arb) for arb in arb_variants]))\n",
    "    yamli_variants = row['yamli_variants'].split(',')\n",
    "    yamli_variants = list(set([araby.strip_tashkeel(arb) for arb in yamli_variants]))\n",
    "\n",
    "    arb_eng_model_predictions = list(set(set(arb_eng_model_predictions_dict[name]) - set(eng_variants)))\n",
    "    arb_model = arb_arb_model_predictions_dict[name]\n",
    "    arb_model = list(set([araby.strip_tashkeel(arb) for arb in arb_model]))\n",
    "    arb_arb_model_predictions = list(set(set(arb_model) - set(arb_variants) - set(yamli_variants)))\n",
    "    yamli_variants = set(yamli_variants) - set(arb_variants)    \n",
    "\n",
    "    df.set_value(index, 'yamli_variants', ','.join(set(yamli_variants)))\n",
    "    df.set_value(index, 'arb_eng_model_predictions', ','.join(set(arb_eng_model_predictions)))\n",
    "    df.set_value(index, 'arb_arb_model_predictions', ','.join(set(arb_arb_model_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arb_names_with_variants.to_csv(target_dir + 'arb_names_with_variants.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del eee['arb_eng_model_predictions_x']\n",
    "del eee['arb_arb_model_predictions_x']\n",
    "del eee['eng_eng_model_predictions_x']\n",
    "del eee['eng_arb_model_predictions_x']\n",
    "del eee['arb_eng_model_predictions_y']\n",
    "del eee['arb_arb_model_predictions_y']\n",
    "del eee['eng_eng_model_predictions_y']\n",
    "del eee['eng_arb_model_predictions_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggg = pd.read_csv(target_dir + 'all_names_with_yamli.tsv', sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MEHMUDH,MOOMMA,MOHAMMD,MOHOMMED,MHAMDA,MOHAMADU,MUJISUKAMTI,MOHIMID,MOHAMEDI,MAHOMMED,MACHMUDH,MOHMEDO,MOWAMMAD,MOHAMADY,MAHAMMED,MOHAMDU,MOHMOUD,MHAMOUD,MAKHMUDH,MOHMAD,MOHMOOD,MHAMOOD,MOHMAED,MOHMUD,MOHAMADI,MOHAMDY,MAHMUDA,MUAHMDO,MAKHMUDA,MHAMDI,MAHAMUDA,MOHMADI,MUHAMADU,MAHAMADI,MUHMAD,MAKHMADI,MUSLIMAT,MOKHMAD,MOAHMMED,MAHMUDI,MOHAMEDY'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arb_names_with_variants.loc[0]['eng_model_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_names_with_variants = pd.read_csv(target_dir + 'eng_names_with_variants.tsv',sep='\\t', header=None, names=['name', 'yamli_variants', 'arb_variants', 'eng_variants', 'eng_model_predictions', 'arb_model_predictions'])\n",
    "arb_names_with_variants = pd.read_csv(target_dir + 'arb_names_with_variants.tsv',sep='\\t', header=None, names=['name', 'yamli_variants', 'eng_variants', 'arb_variants', 'eng_model_predictions', 'arb_model_predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_tashkeel_from_arb_variants(df):\n",
    "    for index, row in df.iterrows():        \n",
    "        arb_variants = row['arb_variants'].split(',')\n",
    "        arb_variants = list(set([araby.strip_tashkeel(arb) for arb in arb_variants]))\n",
    "        df.set_value(index, 'arb_variants', ','.join(set(arb_variants)))\n",
    "    return df\n",
    "\n",
    "eng_names_with_variants = strip_tashkeel_from_arb_variants(eng_names_with_variants).copy()\n",
    "arb_names_with_variants = strip_tashkeel_from_arb_variants(arb_names_with_variants).copy()\n",
    "\n",
    "arb_names_with_variants.to_csv(target_dir + 'arb_names_with_variants.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "eng_names_with_variants.to_csv(target_dir + 'eng_names_with_variants.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'محاماد,محمد,محمّد,مهامّد,مهمّد,موحاماد,موحامد,موحامّد,موحمد,موحمّد,موهاماد,موهامد,موهامّد,موهمّد,مُحاماد,مُهاماد,مُهامّاد,مُهامّد'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_names_with_variants.loc[0]['arb_variants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arb_names_with_variants = pd.read_csv(target_dir + 'arb_names_with_variants.tsv',sep='\\t', header=None, names=['name', 'yamli_variants', 'eng_variants', 'arb_variants', 'eng_model_predictions', 'arb_model_predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'محمض,موحماد,محامد,محاميد,محميد'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arb_names_with_variants.loc[0]['yamli_variants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
