{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from itertools import permutations, repeat, combinations, chain, product, cycle\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_names = {\n",
    "    1: {'eng': ['MARAM'], 'arb': ['مرآم', 'مرام']},\n",
    "    2: {'eng': ['MURAD', 'MORAD'], 'arb': ['مراد', 'مرآد']},\n",
    "    3: {'eng': ['ASEEL', 'ASEIL', 'ASSEEL'], 'arb': ['أصيل', 'اصيل']},\n",
    "    4: {'eng': ['ASALA', 'ASALAH'], 'arb': ['أصاله', 'أصالة']},\n",
    "    5: {'eng': ['MARIAH', 'MARIA', 'MARIYAH'], 'arb': ['ماريه', 'مارية', 'ماريا']},\n",
    "    6: {'eng': ['NASEEM', 'NASIM', 'NSEEM'], 'arb': ['نسيم']},\n",
    "    7: {'eng': ['TASNEEM', 'TASNIM'], 'arb': ['تسنيم']},\n",
    "    8: {'eng': ['JAMEEL', 'JAMIL'], 'arb': ['جميل']},\n",
    "    9: {'eng': ['JAMEELAH', 'JAMEELA', 'JAMILA'], 'arb': ['جميله', 'جميلة']},\n",
    "    10: {'eng': ['MONEER', 'MONIR', 'MUNEER', 'MUNIR'], 'arb': ['منير', 'مُنير']},\n",
    "    11: {'eng': ['MANAR', 'MANAAR'], 'arb': ['منار', 'مَنار', 'منآر']},\n",
    "    12: {'eng': ['MUNIRAH', 'MUNEERA', 'MONEERAH'], 'arb': ['منيرة', 'منيره']},\n",
    "    13: {'eng': ['SAAD', 'SA\\'D'], 'arb': ['سَعْد', 'سعد']},\n",
    "    14: {'eng': ['SUAD', 'SOAD', 'SO\\'AD', 'SUAAD', 'SOAAD'], 'arb': ['سعاد', 'سُعاد']},\n",
    "    15: {'eng': ['AMEER', 'AMIR'], 'arb': ['امير', 'أمير']},\n",
    "    16: {'eng': ['AMIRAH', 'AMEERAH'], 'arb': ['أميرة', 'أميره', 'اميرة', 'اميره']},\n",
    "    17: {'eng': ['YASEEN'], 'arb': ['ياسين', 'يس']},\n",
    "    18: {'eng': ['YASMEEN', 'YASMIN', 'JASMINE'], 'arb': ['ياسَميين', 'ياسمين']},\n",
    "    19: {'eng': ['KHALID', 'KALID'], 'arb': ['خالد', 'كالد']},\n",
    "    20: {'eng': ['KHALIDAH', 'KHALIDA', 'KHALEDA'], 'arb': ['كالده', 'خالده', 'كالدة', 'خالدة']},\n",
    "    21: {'eng': ['RAKAN', 'RAAKAN'], 'arb': ['راكان', 'ركان']},\n",
    "    22: {'eng': ['RAZAN'], 'arb': ['رازان', 'رزان']},\n",
    "    23: {'eng': ['RAAD', 'RA\\'D'], 'arb': ['رعد']},\n",
    "    24: {'eng': ['RAED', 'RA\\'ED'], 'arb': ['رائد']},\n",
    "    25: {'eng': ['RAGHAD'], 'arb': ['رغد']},\n",
    "    26: {'eng': ['MARWAN'], 'arb': ['مروان']},\n",
    "    27: {'eng': ['RAWAN'], 'arb': ['روان']},\n",
    "    28: {'eng': ['ELLA'], 'arb': ['ايلا', 'ايلّه', 'إيلا', 'ايلّا']},\n",
    "    29: {'eng': ['EMMA'], 'arb': ['ايما', 'ايمّه', 'إيما', 'ايمّا']},\n",
    "    30: {'eng': ['HEAVEN'], 'arb': ['إيفين', 'هيفن']},\n",
    "    31: {'eng': ['ANNABELLA'], 'arb': ['انّابيلّا', 'أنابيلا', 'أنّابيله']},\n",
    "    32: {'eng': ['SERENITY'], 'arb': ['سيرينيتي']},\n",
    "    33: {'eng': ['MADISON'], 'arb': ['ماديسون']},\n",
    "    34: {'eng': ['ISABELLA', 'IZABELLE'], 'arb': ['ايزابيلا', 'ايزابيله']},\n",
    "    35: {'eng': ['NAOMI'], 'arb': ['نَؤمي', 'نعومي']},\n",
    "    36: {'eng': ['ELIJAH', 'ILIJA'], 'arb': ['ايليجا', 'إيليجه']},\n",
    "    37: {'eng': ['NOAH'], 'arb': ['نوح']},\n",
    "    38: {'eng': ['ISAAK', 'ISSAC'], 'arb': ['إسحاق', 'اسحاق']},\n",
    "    39: {'eng': ['JONATHAN', 'JONATHON'], 'arb': ['جوناتان', 'جوناثن']},\n",
    "    40: {'eng': ['SEBASTIAN'], 'arb': ['سبايتيان', 'سيباستيان']},\n",
    "    41: {'eng': ['ANTHONY', 'ANTONY'], 'arb': ['أنثوني', 'أنتوني', 'انتوني']},\n",
    "    42: {'eng': ['ALEXANDAR', 'ALEKSANDAR', 'ALEXANDR'], 'arb': ['أليكس', 'آليكسندر', 'اليكسندر', 'اليسكاندر', 'أليكساندر']},\n",
    "    43: {'eng': ['DANIEL'], 'arb': ['دانيال', 'دانييل', 'دانييل']},\n",
    "    44: {'eng': ['NEVAEH'], 'arb': ['نيفايه']},\n",
    "    45: {'eng': ['SAMUEL'], 'arb': ['صاموئيل', 'صامويل']},\n",
    "    46: {'eng': ['MICHAEL', 'MIKHAEL', 'MICHAIL', 'MIKAEL'], 'arb': ['ميخائيل', 'ميكائيل']},\n",
    "    47: {'eng': ['WILLIAM'], 'arb': ['ويليام']},\n",
    "    48: {'eng': ['SENTIAGO', 'SANTIAGO'], 'arb': ['سانتاياغو', 'سنتياجو', 'سانتياجو']},\n",
    "    49: {'eng': ['JOSHUA'], 'arb': ['جوشوا', 'جوشواه']},\n",
    "    50: {'eng': ['DAVID'], 'arb': ['دايفيد', 'ديفيد']},\n",
    "    51: {'eng': ['MASON', 'MAASON'], 'arb': ['ماسون', 'ميسون']},\n",
    "    52: {'eng': ['TRINITY'], 'arb': ['ترينيتي']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 s, sys: 484 ms, total: 33.3 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_keys = all_names.keys()\n",
    "all_eng = [(key, item['eng']) for key, item in all_names.items()]\n",
    "all_eng = [[key, item] for key, sublist in all_eng for item in sublist]\n",
    "all_arb = [(key, item['arb']) for key, item in all_names.items()]\n",
    "all_arb = [[key, item] for key, sublist in all_arb for item in sublist]\n",
    "\n",
    "#1x1\n",
    "neg_eng_eng_1x1 = []\n",
    "neg_eng_arb_1x1 = []\n",
    "neg_arb_arb_1x1 = []\n",
    "pos_eng_eng_1x1 = []\n",
    "pos_eng_arb_1x1 = []\n",
    "pos_arb_arb_1x1 = []\n",
    "\n",
    "#2x2\n",
    "neg_eng_eng_1_1x1_1 = []\n",
    "neg_eng_arb_1_1x1_1 = []\n",
    "neg_arb_arb_1_1x1_1 = []\n",
    "pos_eng_eng_2x2 = []\n",
    "pos_eng_arb_2x2 = []\n",
    "pos_arb_arb_2x2 = []\n",
    "\n",
    "#2x3\n",
    "neg_eng_eng_1_1x1_2 = []\n",
    "neg_eng_arb_1_1x1_2 = []\n",
    "neg_arb_arb_1_1x1_2 = []\n",
    "pos_eng_eng_2x3 = []\n",
    "pos_eng_arb_2x3 = []\n",
    "pos_arb_arb_2x3 = []\n",
    "\n",
    "#3x3\n",
    "neg_eng_eng_3x3_unordered = []\n",
    "neg_eng_arb_3x3_unordered = []\n",
    "neg_arb_arb_3x3_unordered = []\n",
    "neg_eng_eng_2_1x2_1 = []\n",
    "neg_eng_arb_2_1x2_1 = []\n",
    "neg_arb_arb_2_1x2_1 = []\n",
    "pos_eng_eng_3x3 = []\n",
    "pos_eng_arb_3x3 = []\n",
    "pos_arb_arb_3x3 = []\n",
    "\n",
    "#2x4\n",
    "neg_eng_eng_1_1x1_3 = []\n",
    "neg_eng_arb_1_1x1_3 = []\n",
    "neg_arb_arb_1_1x1_3 = []\n",
    "pos_eng_eng_2x4 = []\n",
    "pos_eng_arb_2x4 = []\n",
    "pos_arb_arb_2x4 = []\n",
    "\n",
    "#3x4\n",
    "neg_eng_eng_2_1x2_2 = []\n",
    "neg_eng_arb_2_1x2_2 = []\n",
    "neg_arb_arb_2_1x2_2 = []\n",
    "pos_eng_eng_3x4 = []\n",
    "pos_eng_arb_3x4 = []\n",
    "pos_arb_arb_3x4 = []\n",
    "\n",
    "#4x4\n",
    "neg_eng_eng_4x4_unordered = []\n",
    "neg_eng_arb_4x4_unordered = []\n",
    "neg_arb_arb_4x4_unordered = []\n",
    "neg_eng_eng_3_1x3_1 = []\n",
    "neg_eng_arb_3_1x3_1 = []\n",
    "neg_arb_arb_3_1x3_1 = []\n",
    "pos_eng_eng_4x4 = []\n",
    "pos_eng_arb_4x4 = []\n",
    "pos_arb_arb_4x4 = []\n",
    "\n",
    "for key, item in all_names.items():\n",
    "    other_keys = [k for k in all_keys if k != key]\n",
    "    other_eng = [eng[1] for eng in all_eng if eng[0] != key]\n",
    "    other_arb = [arb[1] for arb in all_arb if arb[0] != key]\n",
    "    \n",
    "    rnd_keys = random.sample(other_keys, 3)\n",
    "    neg_eng_src = [eng[1] for eng in all_eng if eng[0] != key and eng[0] not in rnd_keys]\n",
    "    neg_arb_src = [arb[1] for arb in all_arb if arb[0] != key and arb[0] not in rnd_keys]\n",
    "    \n",
    "    #1x1\n",
    "    pos_eng_eng_1x1.extend(list(product(item['eng'], repeat=2)))\n",
    "    pos_eng_arb_1x1.extend(list(product(item['eng'], item['arb'])))\n",
    "    pos_arb_arb_1x1.extend(list(product(item['arb'], repeat=2)))\n",
    "    neg_eng_eng_1x1.extend(list(product(item['eng'], other_eng)))\n",
    "    neg_eng_arb_1x1.extend(list(product(item['eng'], other_arb)))\n",
    "    neg_arb_arb_1x1.extend(list(product(item['arb'], other_arb)))\n",
    "    \n",
    "    #2x2\n",
    "    eng_1_2 = list(product(item['eng'], all_names[rnd_keys[0]]['eng']))\n",
    "    eng_1_2 = [' '.join(item) for item in eng_1_2]\n",
    "    pos_eng_eng_2x2.extend(list(product(eng_1_2, repeat=2)))\n",
    "    arb_1_2 = list(product(item['arb'], all_names[rnd_keys[0]]['arb']))\n",
    "    arb_1_2 = [' '.join(item) for item in arb_1_2]\n",
    "    pos_arb_arb_2x2.extend(list(product(arb_1_2, repeat=2)))\n",
    "    pos_eng_arb_2x2.extend(list(product(eng_1_2, arb_1_2)))\n",
    "    \n",
    "    eng_1_3 = list(product(item['eng'], all_names[rnd_keys[1]]['eng']))\n",
    "    eng_1_3 = [' '.join(item) for item in eng_1_3]\n",
    "    pos_eng_eng_2x2.extend(list(product(eng_1_3, repeat=2)))\n",
    "    arb_1_3 = list(product(item['arb'], all_names[rnd_keys[1]]['arb']))\n",
    "    arb_1_3 = [' '.join(item) for item in arb_1_3]\n",
    "    pos_arb_arb_2x2.extend(list(product(arb_1_3, repeat=2)))\n",
    "    pos_eng_arb_2x2.extend(list(product(eng_1_3, arb_1_3)))\n",
    "    \n",
    "    eng_1_4 = list(product(item['eng'], all_names[rnd_keys[2]]['eng']))\n",
    "    eng_1_4 = [' '.join(item) for item in eng_1_4]\n",
    "    pos_eng_eng_2x2.extend(list(product(eng_1_4, repeat=2)))\n",
    "    arb_1_4 = list(product(item['arb'], all_names[rnd_keys[2]]['arb']))\n",
    "    arb_1_4 = [' '.join(item) for item in arb_1_4]\n",
    "    pos_arb_arb_2x2.extend(list(product(arb_1_4, repeat=2)))\n",
    "    pos_eng_arb_2x2.extend(list(product(eng_1_4, arb_1_4)))\n",
    "        \n",
    "    #3x3\n",
    "    eng_1_2_3 = list(product(eng_1_2, all_names[rnd_keys[1]]['eng']))\n",
    "    eng_1_2_3 = [' '.join(item) for item in eng_1_2_3]\n",
    "    pos_eng_eng_3x3.extend(list(product(eng_1_2_3, repeat=2)))\n",
    "    arb_1_2_3 = list(product(arb_1_2, all_names[rnd_keys[1]]['arb']))\n",
    "    arb_1_2_3 = [' '.join(item) for item in arb_1_2_3]\n",
    "    pos_arb_arb_3x3.extend(list(product(arb_1_2_3, repeat=2)))\n",
    "    pos_eng_arb_3x3.extend(list(product(eng_1_2_3, arb_1_2_3)))\n",
    "    pos_eng_eng_2x3.extend(list(product(eng_1_2_3, eng_1_2)))\n",
    "    pos_eng_eng_2x3.extend(list(product(eng_1_2_3, eng_1_3)))\n",
    "    pos_arb_arb_2x3.extend(list(product(arb_1_2_3, arb_1_2)))\n",
    "    pos_arb_arb_2x3.extend(list(product(arb_1_2_3, arb_1_3)))\n",
    "    pos_eng_arb_2x3.extend(list(product(eng_1_2_3, arb_1_2)))\n",
    "    pos_eng_arb_2x3.extend(list(product(eng_1_2_3, arb_1_3)))\n",
    "    \n",
    "    eng_1_2_4 = list(product(eng_1_2, all_names[rnd_keys[2]]['eng']))\n",
    "    eng_1_2_4 = [' '.join(item) for item in eng_1_2_4]\n",
    "    pos_eng_eng_3x3.extend(list(product(eng_1_2_4, repeat=2)))\n",
    "    arb_1_2_4 = list(product(arb_1_2, all_names[rnd_keys[2]]['arb']))\n",
    "    arb_1_2_4 = [' '.join(item) for item in arb_1_2_4]\n",
    "    pos_arb_arb_3x3.extend(list(product(arb_1_2_4, repeat=2)))\n",
    "    pos_eng_arb_3x3.extend(list(product(eng_1_2_4, arb_1_2_4)))\n",
    "    pos_eng_eng_2x3.extend(list(product(eng_1_2_4, eng_1_2)))\n",
    "    pos_eng_eng_2x3.extend(list(product(eng_1_2_4, eng_1_4)))\n",
    "    pos_arb_arb_2x3.extend(list(product(arb_1_2_4, arb_1_2)))\n",
    "    pos_arb_arb_2x3.extend(list(product(arb_1_2_4, arb_1_4)))\n",
    "    pos_eng_arb_2x3.extend(list(product(eng_1_2_4, arb_1_2)))\n",
    "    pos_eng_arb_2x3.extend(list(product(eng_1_2_4, arb_1_4)))\n",
    "\n",
    "    eng_1_3_4 = list(product(eng_1_3, all_names[rnd_keys[2]]['eng']))\n",
    "    eng_1_3_4 = [' '.join(item) for item in eng_1_3_4]\n",
    "    pos_eng_eng_3x3.extend(list(product(eng_1_3_4, repeat=2)))\n",
    "    arb_1_3_4 = list(product(arb_1_3, all_names[rnd_keys[2]]['arb']))\n",
    "    arb_1_3_4 = [' '.join(item) for item in arb_1_3_4]\n",
    "    pos_arb_arb_3x3.extend(list(product(arb_1_3_4, repeat=2)))\n",
    "    pos_eng_arb_3x3.extend(list(product(eng_1_3_4, arb_1_3_4)))\n",
    "    pos_eng_eng_2x3.extend(list(product(eng_1_3_4, eng_1_3)))\n",
    "    pos_eng_eng_2x3.extend(list(product(eng_1_3_4, eng_1_4)))\n",
    "    pos_arb_arb_2x3.extend(list(product(arb_1_3_4, arb_1_3)))\n",
    "    pos_arb_arb_2x3.extend(list(product(arb_1_3_4, arb_1_4)))\n",
    "    pos_eng_arb_2x3.extend(list(product(eng_1_3_4, arb_1_3)))\n",
    "    pos_eng_arb_2x3.extend(list(product(eng_1_3_4, arb_1_4)))\n",
    "    \n",
    "    #4x4\n",
    "    eng_1_2_3_4 = list(product(eng_1_2_3, all_names[rnd_keys[2]]['eng']))\n",
    "    eng_1_2_3_4 = [' '.join(item) for item in eng_1_2_3_4]\n",
    "    pos_eng_eng_4x4.extend(list(product(eng_1_2_3_4, repeat=2)))\n",
    "    arb_1_2_3_4 = list(product(arb_1_2_3, all_names[rnd_keys[2]]['arb']))\n",
    "    arb_1_2_3_4 = [' '.join(item) for item in arb_1_2_3_4]\n",
    "    pos_arb_arb_4x4.extend(list(product(arb_1_2_3_4, repeat=2)))\n",
    "    pos_eng_arb_4x4.extend(list(product(eng_1_2_3_4, arb_1_2_3_4)))\n",
    "    \n",
    "    #2x4\n",
    "    pos_eng_eng_2x4.extend(list(product(eng_1_2_3_4, eng_1_2)))\n",
    "    pos_eng_eng_2x4.extend(list(product(eng_1_2_3_4, eng_1_3)))\n",
    "    pos_eng_eng_2x4.extend(list(product(eng_1_2_3_4, eng_1_4)))\n",
    "    pos_arb_arb_2x4.extend(list(product(arb_1_2_3_4, arb_1_2)))\n",
    "    pos_arb_arb_2x4.extend(list(product(arb_1_2_3_4, arb_1_3)))\n",
    "    pos_arb_arb_2x4.extend(list(product(arb_1_2_3_4, arb_1_4)))\n",
    "    pos_eng_arb_2x4.extend(list(product(eng_1_2_3_4, arb_1_2)))\n",
    "    pos_eng_arb_2x4.extend(list(product(eng_1_2_3_4, arb_1_3)))\n",
    "    pos_eng_arb_2x4.extend(list(product(eng_1_2_3_4, arb_1_4)))\n",
    "    \n",
    "    #3x3\n",
    "    pos_eng_eng_3x4.extend(list(product(eng_1_2_3_4, eng_1_2_3)))\n",
    "    pos_eng_eng_3x4.extend(list(product(eng_1_2_3_4, eng_1_3_4)))\n",
    "    pos_eng_eng_3x4.extend(list(product(eng_1_2_3_4, eng_1_2_4)))\n",
    "    pos_arb_arb_3x4.extend(list(product(arb_1_2_3_4, arb_1_2_3)))\n",
    "    pos_arb_arb_3x4.extend(list(product(arb_1_2_3_4, arb_1_3_4)))\n",
    "    pos_arb_arb_3x4.extend(list(product(arb_1_2_3_4, arb_1_2_4)))\n",
    "    pos_eng_arb_3x4.extend(list(product(eng_1_2_3_4, arb_1_2_3)))\n",
    "    pos_eng_arb_3x4.extend(list(product(eng_1_2_3_4, arb_1_3_4)))\n",
    "    pos_eng_arb_3x4.extend(list(product(eng_1_2_3_4, arb_1_2_4)))\n",
    "\n",
    "    for pp in pos_eng_eng_2x2:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_eng_src))\n",
    "        neg_eng_eng_1_1x1_1.append([pp[0], ' '.join(pair2)])\n",
    "    \n",
    "    for pp in pos_eng_arb_2x2:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_eng_arb_1_1x1_1.append([pp[0], ' '.join(pair2)])\n",
    "    \n",
    "    for pp in pos_arb_arb_2x2:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_arb_arb_1_1x1_1.append([pp[0], ' '.join(pair2)])\n",
    "        \n",
    "    for pp in pos_eng_eng_3x3:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        pair2_shuff = pair2[:]\n",
    "        random.shuffle(pair2_shuff)\n",
    "        while pair2 == pair2_shuff:\n",
    "            random.shuffle(pair2_shuff)\n",
    "        neg_eng_eng_3x3_unordered.append([pp[0], ' '.join(pair2_shuff)])\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_eng_src))\n",
    "        neg_eng_eng_2_1x2_1.append([pp[0], ' '.join(pair2)])\n",
    "    \n",
    "    for pp in pos_eng_arb_3x3:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        pair2_shuff = pair2[:]\n",
    "        random.shuffle(pair2_shuff)\n",
    "        while pair2 == pair2_shuff:\n",
    "            random.shuffle(pair2_shuff)\n",
    "        neg_eng_arb_3x3_unordered.append([pp[0], ' '.join(pair2_shuff)])\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_eng_arb_2_1x2_1.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_arb_arb_3x3:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        pair2_shuff = pair2[:]\n",
    "        random.shuffle(pair2_shuff)\n",
    "        while pair2 == pair2_shuff:\n",
    "            random.shuffle(pair2_shuff)\n",
    "        neg_arb_arb_3x3_unordered.append([pp[0], ' '.join(pair2_shuff)])\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_arb_arb_2_1x2_1.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_eng_2x3:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_eng_src))\n",
    "        neg_eng_eng_1_1x1_2.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_arb_2x3:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_eng_arb_1_1x1_2.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_arb_arb_2x3:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_arb_arb_1_1x1_2.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_eng_2x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_eng_src))\n",
    "        neg_eng_eng_1_1x1_3.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_arb_2x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_eng_arb_1_1x1_3.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_arb_arb_2x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_arb_arb_1_1x1_3.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_eng_3x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_eng_src))\n",
    "        neg_eng_eng_2_1x2_2.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_arb_3x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_eng_arb_2_1x2_2.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_arb_arb_3x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_arb_arb_2_1x2_2.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_eng_4x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        pair2_shuff = pair2[:]\n",
    "        random.shuffle(pair2_shuff)\n",
    "        while pair2 == pair2_shuff:\n",
    "            random.shuffle(pair2_shuff)\n",
    "        neg_eng_eng_4x4_unordered.append([pp[0], ' '.join(pair2_shuff)])\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_eng_src))\n",
    "        neg_eng_eng_3_1x3_1.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_eng_arb_4x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        pair2_shuff = pair2[:]\n",
    "        random.shuffle(pair2_shuff)\n",
    "        while pair2 == pair2_shuff:\n",
    "            random.shuffle(pair2_shuff)\n",
    "        neg_eng_arb_4x4_unordered.append([pp[0], ' '.join(pair2_shuff)])\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_eng_arb_3_1x3_1.append([pp[0], ' '.join(pair2)])\n",
    "\n",
    "    for pp in pos_arb_arb_4x4:\n",
    "        pair2 = pp[1].split(' ')\n",
    "        pair2_shuff = pair2[:]\n",
    "        random.shuffle(pair2_shuff)\n",
    "        while pair2 == pair2_shuff:\n",
    "            random.shuffle(pair2_shuff)\n",
    "        neg_arb_arb_4x4_unordered.append([pp[0], ' '.join(pair2_shuff)])\n",
    "        lct = random.randrange(0, len(pair2))\n",
    "        pair2.pop(lct)\n",
    "        pair2.insert(lct, random.choice(neg_arb_src))\n",
    "        neg_arb_arb_3_1x3_1.append([pp[0], ' '.join(pair2)])\n",
    "    \n",
    "#eng_eng = [[1, pair1, pair2] for pair1, pair2 in eng_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_eng_eng_2_1x2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"/home/jupyter/notebooks/PoC/data-preparation/output/understanding_data/test_datasets/\"\n",
    "\n",
    "def prepare_dataframe_and_save(lst, similarity, language_pair, file_name):\n",
    "    folder = target_dir + language_pair + '/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    df = pd.DataFrame(lst)\n",
    "    df = pd.DataFrame(np.sort(df.values, axis=1), index=df.index, columns=df.columns).drop_duplicates().reset_index(drop=True)\n",
    "    df['similarity'] = similarity\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(folder + file_name + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 s, sys: 501 ms, total: 14.7 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#1x1\n",
    "prepare_dataframe_and_save(neg_eng_eng_1x1, 0, 'eng_eng', 'neg_eng_eng_1x1')\n",
    "prepare_dataframe_and_save(neg_eng_arb_1x1, 0, 'eng_arb', 'neg_eng_arb_1x1')\n",
    "prepare_dataframe_and_save(neg_arb_arb_1x1, 0, 'arb_arb', 'neg_arb_arb_1x1')\n",
    "prepare_dataframe_and_save(pos_eng_eng_1x1, 1, 'eng_eng', 'pos_eng_eng_1x1')\n",
    "prepare_dataframe_and_save(pos_eng_arb_1x1, 1, 'eng_arb', 'pos_eng_arb_1x1')\n",
    "prepare_dataframe_and_save(pos_arb_arb_1x1, 1, 'arb_arb', 'pos_arb_arb_1x1')\n",
    "\n",
    "#2x2\n",
    "prepare_dataframe_and_save(neg_eng_eng_1_1x1_1, 0, 'eng_eng', 'neg_eng_eng_1_1x1_1')\n",
    "prepare_dataframe_and_save(neg_eng_arb_1_1x1_1, 0, 'eng_arb', 'neg_eng_arb_1_1x1_1')\n",
    "prepare_dataframe_and_save(neg_arb_arb_1_1x1_1, 0, 'arb_arb', 'neg_arb_arb_1_1x1_1')\n",
    "prepare_dataframe_and_save(pos_eng_eng_2x2, 1, 'eng_eng', 'pos_eng_eng_2x2')\n",
    "prepare_dataframe_and_save(pos_eng_arb_2x2, 1, 'eng_arb', 'pos_eng_arb_2x2')\n",
    "prepare_dataframe_and_save(pos_arb_arb_2x2, 1, 'arb_arb', 'pos_arb_arb_2x2')\n",
    "\n",
    "#2x3\n",
    "prepare_dataframe_and_save(neg_eng_eng_1_1x1_2, 0, 'eng_eng', 'neg_eng_eng_1_1x1_2')\n",
    "prepare_dataframe_and_save(neg_eng_arb_1_1x1_2, 0, 'eng_arb', 'neg_eng_arb_1_1x1_2')\n",
    "prepare_dataframe_and_save(neg_arb_arb_1_1x1_2, 0, 'arb_arb', 'neg_arb_arb_1_1x1_2')\n",
    "prepare_dataframe_and_save(pos_eng_eng_2x3, 1, 'eng_eng', 'pos_eng_eng_2x3')\n",
    "prepare_dataframe_and_save(pos_eng_arb_2x3, 1, 'eng_arb', 'pos_eng_arb_2x3')\n",
    "prepare_dataframe_and_save(pos_arb_arb_2x3, 1, 'arb_arb', 'pos_arb_arb_2x3')\n",
    "\n",
    "#3x3\n",
    "prepare_dataframe_and_save(neg_eng_eng_3x3_unordered, 0, 'eng_eng', 'neg_eng_eng_3x3_unordered')\n",
    "prepare_dataframe_and_save(neg_eng_arb_3x3_unordered, 0, 'eng_arb', 'neg_eng_arb_3x3_unordered')\n",
    "prepare_dataframe_and_save(neg_arb_arb_3x3_unordered, 0, 'arb_arb', 'neg_arb_arb_3x3_unordered')\n",
    "prepare_dataframe_and_save(neg_eng_eng_2_1x2_1, 0, 'eng_eng', 'neg_eng_eng_2_1x2_1')\n",
    "prepare_dataframe_and_save(neg_eng_arb_2_1x2_1, 0, 'eng_arb', 'neg_eng_arb_2_1x2_1')\n",
    "prepare_dataframe_and_save(neg_arb_arb_2_1x2_1, 0, 'arb_arb', 'neg_arb_arb_2_1x2_1')\n",
    "prepare_dataframe_and_save(pos_eng_eng_3x3, 1, 'eng_eng', 'pos_eng_eng_3x3')\n",
    "prepare_dataframe_and_save(pos_eng_arb_3x3, 1, 'eng_arb', 'pos_eng_arb_3x3')\n",
    "prepare_dataframe_and_save(pos_arb_arb_3x3, 1, 'arb_arb', 'pos_arb_arb_3x3')\n",
    "\n",
    "#2x4\n",
    "prepare_dataframe_and_save(neg_eng_eng_1_1x1_3, 0, 'eng_eng', 'neg_eng_eng_1_1x1_3')\n",
    "prepare_dataframe_and_save(neg_eng_arb_1_1x1_3, 0, 'eng_arb', 'neg_eng_arb_1_1x1_3')\n",
    "prepare_dataframe_and_save(neg_arb_arb_1_1x1_3, 0, 'arb_arb', 'neg_arb_arb_1_1x1_3')\n",
    "prepare_dataframe_and_save(pos_eng_eng_2x4, 1, 'eng_eng', 'pos_eng_eng_2x4')\n",
    "prepare_dataframe_and_save(pos_eng_arb_2x4, 1, 'eng_arb', 'pos_eng_arb_2x4')\n",
    "prepare_dataframe_and_save(pos_arb_arb_2x4, 1, 'arb_arb', 'pos_arb_arb_2x4')\n",
    "\n",
    "#3x4\n",
    "\n",
    "prepare_dataframe_and_save(neg_eng_eng_2_1x2_2, 0, 'eng_eng', 'neg_eng_eng_2_1x2_2')\n",
    "prepare_dataframe_and_save(neg_eng_arb_2_1x2_2, 0, 'eng_arb', 'neg_eng_arb_2_1x2_2')\n",
    "prepare_dataframe_and_save(neg_arb_arb_2_1x2_2, 0, 'arb_arb', 'neg_arb_arb_2_1x2_2')\n",
    "prepare_dataframe_and_save(pos_eng_eng_3x4, 1, 'eng_eng', 'pos_eng_eng_3x4')\n",
    "prepare_dataframe_and_save(pos_eng_arb_3x4, 1, 'eng_arb', 'pos_eng_arb_3x4')\n",
    "prepare_dataframe_and_save(pos_arb_arb_3x4, 1, 'arb_arb', 'pos_arb_arb_3x4')\n",
    "\n",
    "#4x4\n",
    "prepare_dataframe_and_save(neg_eng_eng_4x4_unordered, 0, 'eng_eng', 'neg_eng_eng_4x4_unordered')\n",
    "prepare_dataframe_and_save(neg_eng_arb_4x4_unordered, 0, 'eng_arb', 'neg_eng_arb_4x4_unordered')\n",
    "prepare_dataframe_and_save(neg_arb_arb_4x4_unordered, 0, 'arb_arb', 'neg_arb_arb_4x4_unordered')\n",
    "prepare_dataframe_and_save(neg_eng_eng_3_1x3_1, 0, 'eng_eng', 'neg_eng_eng_3_1x3_1')\n",
    "prepare_dataframe_and_save(neg_eng_arb_3_1x3_1, 0, 'eng_arb', 'neg_eng_arb_3_1x3_1')\n",
    "prepare_dataframe_and_save(neg_arb_arb_3_1x3_1, 0, 'arb_arb', 'neg_arb_arb_3_1x3_1')\n",
    "prepare_dataframe_and_save(pos_eng_eng_4x4, 1, 'eng_eng', 'pos_eng_eng_4x4')\n",
    "prepare_dataframe_and_save(pos_eng_arb_4x4, 1, 'eng_arb', 'pos_eng_arb_4x4')\n",
    "prepare_dataframe_and_save(pos_arb_arb_4x4, 1, 'arb_arb', 'pos_arb_arb_4x4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(target_dir + 'arb_arb/neg_arb_arb_1x1.tsv',sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>مراد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآد</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>أصيل</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>اصيل</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>أصاله</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>أصالة</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>ماريه</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>مارية</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>ماريا</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>نسيم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>تسنيم</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>جميل</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>جميله</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>جميلة</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>منير</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>مُنير</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>منار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>مَنار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>منآر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>منيرة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>مرآم</td>\n",
       "      <td>منيره</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>سَعْد</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>سعد</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>سعاد</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>سُعاد</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>امير</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>أمير</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>أميرة</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>أميره</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>اميرة</td>\n",
       "      <td>مرآم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>0</td>\n",
       "      <td>دايفيد</td>\n",
       "      <td>سنتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>0</td>\n",
       "      <td>ديفيد</td>\n",
       "      <td>سنتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>0</td>\n",
       "      <td>سنتياجو</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>0</td>\n",
       "      <td>سنتياجو</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>سنتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشوا</td>\n",
       "      <td>سانتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشواه</td>\n",
       "      <td>سانتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>0</td>\n",
       "      <td>دايفيد</td>\n",
       "      <td>سانتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>0</td>\n",
       "      <td>ديفيد</td>\n",
       "      <td>سانتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>0</td>\n",
       "      <td>سانتياجو</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>0</td>\n",
       "      <td>سانتياجو</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>سانتياجو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشوا</td>\n",
       "      <td>دايفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشوا</td>\n",
       "      <td>ديفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5472</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشوا</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشوا</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>جوشوا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشواه</td>\n",
       "      <td>دايفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشواه</td>\n",
       "      <td>ديفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشواه</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5478</th>\n",
       "      <td>0</td>\n",
       "      <td>جوشواه</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>جوشواه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>0</td>\n",
       "      <td>دايفيد</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5481</th>\n",
       "      <td>0</td>\n",
       "      <td>دايفيد</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5482</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>دايفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>0</td>\n",
       "      <td>ديفيد</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>0</td>\n",
       "      <td>ديفيد</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>ديفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>ماسون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>0</td>\n",
       "      <td>ترينيتي</td>\n",
       "      <td>ميسون</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5488 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2\n",
       "0     0      مرآم      مراد\n",
       "1     0      مرآد      مرآم\n",
       "2     0      أصيل      مرآم\n",
       "3     0      اصيل      مرآم\n",
       "4     0     أصاله      مرآم\n",
       "5     0     أصالة      مرآم\n",
       "6     0     ماريه      مرآم\n",
       "7     0     مارية      مرآم\n",
       "8     0     ماريا      مرآم\n",
       "9     0      مرآم      نسيم\n",
       "10    0     تسنيم      مرآم\n",
       "11    0      جميل      مرآم\n",
       "12    0     جميله      مرآم\n",
       "13    0     جميلة      مرآم\n",
       "14    0      مرآم      منير\n",
       "15    0      مرآم     مُنير\n",
       "16    0      مرآم      منار\n",
       "17    0      مرآم     مَنار\n",
       "18    0      مرآم      منآر\n",
       "19    0      مرآم     منيرة\n",
       "20    0      مرآم     منيره\n",
       "21    0     سَعْد      مرآم\n",
       "22    0       سعد      مرآم\n",
       "23    0      سعاد      مرآم\n",
       "24    0     سُعاد      مرآم\n",
       "25    0      امير      مرآم\n",
       "26    0      أمير      مرآم\n",
       "27    0     أميرة      مرآم\n",
       "28    0     أميره      مرآم\n",
       "29    0     اميرة      مرآم\n",
       "...  ..       ...       ...\n",
       "5458  0    دايفيد   سنتياجو\n",
       "5459  0     ديفيد   سنتياجو\n",
       "5460  0   سنتياجو     ماسون\n",
       "5461  0   سنتياجو     ميسون\n",
       "5462  0   ترينيتي   سنتياجو\n",
       "5463  0     جوشوا  سانتياجو\n",
       "5464  0    جوشواه  سانتياجو\n",
       "5465  0    دايفيد  سانتياجو\n",
       "5466  0     ديفيد  سانتياجو\n",
       "5467  0  سانتياجو     ماسون\n",
       "5468  0  سانتياجو     ميسون\n",
       "5469  0   ترينيتي  سانتياجو\n",
       "5470  0     جوشوا    دايفيد\n",
       "5471  0     جوشوا     ديفيد\n",
       "5472  0     جوشوا     ماسون\n",
       "5473  0     جوشوا     ميسون\n",
       "5474  0   ترينيتي     جوشوا\n",
       "5475  0    جوشواه    دايفيد\n",
       "5476  0    جوشواه     ديفيد\n",
       "5477  0    جوشواه     ماسون\n",
       "5478  0    جوشواه     ميسون\n",
       "5479  0   ترينيتي    جوشواه\n",
       "5480  0    دايفيد     ماسون\n",
       "5481  0    دايفيد     ميسون\n",
       "5482  0   ترينيتي    دايفيد\n",
       "5483  0     ديفيد     ماسون\n",
       "5484  0     ديفيد     ميسون\n",
       "5485  0   ترينيتي     ديفيد\n",
       "5486  0   ترينيتي     ماسون\n",
       "5487  0   ترينيتي     ميسون\n",
       "\n",
       "[5488 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_dir = \"/home/jupyter/notebooks/PoC/data-preparation/output/understanding_data/\"\n",
    "\n",
    "def load_up_data_from_files():\n",
    "    global eng_eng_df, eng_arb_df, arb_arb_df, names_dict, all_eng_names, all_arb_names\n",
    "    \n",
    "    eng_eng_df = pd.read_csv(target_dir + 'eng_eng_pairs.tsv',sep='\\t', header=None)\n",
    "    eng_arb_df = pd.read_csv(target_dir + 'eng_arb_pairs.tsv',sep='\\t', header=None)\n",
    "    arb_arb_df = pd.read_csv(target_dir + 'arb_arb_pairs.tsv',sep='\\t', header=None)\n",
    "    \n",
    "    names_dict = load_obj('names_dict')\n",
    "    all_eng_names = load_obj('all_eng_names')\n",
    "    all_arb_names = load_obj('all_arb_names')\n",
    "        \n",
    "def write_data_to_output_files():\n",
    "    i = 0\n",
    "    names_dict = {}\n",
    "    all_eng_names = []\n",
    "    all_arb_names = []\n",
    "    set_len = len(top_given_names)\n",
    "    eng_eng_count = 0\n",
    "    arb_arb_count = 0\n",
    "    eng_arb_count = 0\n",
    "    \n",
    "    for index, row in top_given_names.iterrows():\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "                print(\"processed {i} out of {t}\".format(i=i, t=set_len))\n",
    "\n",
    "        eng_variants = row['eng_variants'].split(',')\n",
    "        arb_variants = row['arb_variants'].split(',')\n",
    "        #negative_eng_variants = row['negative_eng_variants'].split(',')\n",
    "        #negative_arb_variants = row['negative_arb_variants'].split(',')\n",
    "        \n",
    "        all_eng_names.extend([[elem, i] for elem in eng_variants])\n",
    "        all_arb_names.extend([[elem, i] for elem in arb_variants])\n",
    "        names_dict[i] = {'eng': eng_variants, 'arb': arb_variants}\n",
    "        \n",
    "        eng_variants_length = len(eng_variants)\n",
    "        arb_variants_length = len(arb_variants)\n",
    "        #negative_eng_variants_length = len(negative_eng_variants)\n",
    "        #negative_arb_variants_length = len(negative_arb_variants)\n",
    "        \n",
    "        maximum_eng_variants_threshold = 50\n",
    "        if(eng_variants_length > maximum_eng_variants_threshold):\n",
    "            eng_eng_pairs = []\n",
    "            percent = maximum_eng_variants_threshold**2 / eng_variants_length**2\n",
    "            randomly_selected_count = math.ceil(eng_variants_length * percent)\n",
    "            for eng in eng_variants:\n",
    "                temp = random.sample(eng_variants, randomly_selected_count)\n",
    "                temp.append(eng)\n",
    "                eng_eng = list(product([eng], temp))\n",
    "                eng_eng = [list(elem) + [str(i)] for elem in eng_eng]\n",
    "                eng_eng_pairs.extend(eng_eng)\n",
    "        else: \n",
    "            eng_eng_pairs = list(product(eng_variants, repeat=2))\n",
    "            eng_eng_pairs = [list(elem) + [str(i)] for elem in eng_eng_pairs]\n",
    "        \n",
    "        eng_eng_count += len(eng_eng_pairs)\n",
    "        append_list_to_tsv('eng_eng_pairs', eng_eng_pairs)\n",
    "\n",
    "        arb_arb_pairs = list(product(arb_variants, repeat=2))\n",
    "        arb_arb_pairs = [list(elem) + [str(i)] for elem in arb_arb_pairs]\n",
    "        arb_arb_count += len(arb_arb_pairs)\n",
    "        append_list_to_tsv('arb_arb_pairs', arb_arb_pairs)\n",
    "\n",
    "        eng_arb_pairs = set(list(product(eng_variants, arb_variants)))\n",
    "        eng_arb_pairs = [list(elem) + [str(i)] for elem in eng_arb_pairs]\n",
    "        eng_arb_count += len(eng_arb_pairs)\n",
    "        \n",
    "        '''\n",
    "        if len(eng_arb_pairs) < len(eng_eng_pairs):\n",
    "            quotient, modulo = divmod(len(eng_eng_pairs), len(eng_arb_pairs))\n",
    "            extension = random.sample(eng_arb_pairs, modulo)\n",
    "            eng_arb_pairs = [repeated for value in eng_arb_pairs for repeated in repeat(value, quotient)]\n",
    "            eng_arb_pairs.extend(extension)\n",
    "        '''\n",
    "        append_list_to_tsv('eng_arb_pairs', eng_arb_pairs)\n",
    "        \n",
    "        '''\n",
    "        needed_negatives_length = math.ceil(eng_variants_length * (eng_variants_length - 1) * 0.75)\n",
    "\n",
    "        if needed_negatives_length > negative_eng_variants_length:\n",
    "            quotient, modulo = divmod(needed_negatives_length, negative_eng_variants_length)\n",
    "            extension = random.sample(negative_eng_variants, modulo)\n",
    "            negative_eng_variants = [repeated for value in negative_eng_variants for repeated in repeat(value, quotient)]\n",
    "            negative_eng_variants.extend(extension)\n",
    "\n",
    "        if needed_negatives_length > negative_arb_variants_length:\n",
    "            quotient, modulo = divmod(needed_negatives_length, negative_arb_variants_length)\n",
    "            extension = random.sample(negative_arb_variants, modulo)\n",
    "            negative_arb_variants = [repeated for value in negative_arb_variants for repeated in repeat(value, quotient)]\n",
    "            negative_arb_variants.extend(extension)\n",
    "\n",
    "        needed_negatives_length = math.ceil(eng_variants_length * (eng_variants_length - 1) * 0.5)\n",
    "\n",
    "        random.shuffle(negative_eng_variants)\n",
    "        neg_eng_eng_pairs = list(zip(negative_eng_variants[:needed_negatives_length], cycle(eng_variants)))\n",
    "        neg_eng_eng_pairs = [list(elem) + [str(i)] for elem in neg_eng_eng_pairs]\n",
    "        append_list_to_tsv('neg_eng_eng_pairs', neg_eng_eng_pairs)\n",
    "\n",
    "        neg_arb_arb_pairs = list(zip(negative_arb_variants[:needed_negatives_length], cycle(arb_variants)))\n",
    "        neg_arb_arb_pairs = [list(elem) + [str(i)] for elem in neg_arb_arb_pairs]\n",
    "        append_list_to_tsv('neg_arb_arb_pairs', neg_arb_arb_pairs)\n",
    "\n",
    "        neg_eng_arb_pairs = list(zip(negative_eng_variants[needed_negatives_length:], cycle(arb_variants)))\n",
    "        neg_eng_arb_pairs = [list(elem) + [str(i)] for elem in neg_eng_arb_pairs]\n",
    "        append_list_to_tsv('neg_eng_arb_pairs', neg_eng_arb_pairs)\n",
    "\n",
    "        neg_eng_arb_pairs = list(zip(cycle(eng_variants), negative_arb_variants[needed_negatives_length:]))\n",
    "        neg_eng_arb_pairs = [list(elem) + [str(i)] for elem in neg_eng_arb_pairs]\n",
    "        append_list_to_tsv('neg_eng_arb_pairs', neg_eng_arb_pairs)\n",
    "        '''\n",
    "        \n",
    "    save_obj(names_dict, 'names_dict')\n",
    "    save_obj(all_eng_names, 'all_eng_names')\n",
    "    save_obj(all_arb_names, 'all_arb_names')\n",
    "    print('eng_eng count = {e_e},arb_arb count = {a_a}, eng_arb count = {e_a}'.format(e_e=eng_eng_count, a_a=arb_arb_count, e_a=eng_arb_count))\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open(target_dir + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(target_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_up_output_files()\n",
    "write_data_to_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "load_up_data_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_dict[1]['arb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    if len(df.columns) == 4:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df = df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def prepare_duplicate_df(df, desired_length):\n",
    "    print(len(df))\n",
    "    if len(df.columns) == 4:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df = df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "    \n",
    "    quotient, modulo = divmod(desired_length, len(df))\n",
    "    \n",
    "    df = pd.concat([df]*quotient, ignore_index=True)\n",
    "    \n",
    "    if modulo > 0:\n",
    "        fract = modulo / len(df)\n",
    "        df = pd.concat([df, df.sample(frac=fract)])\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    print(len(df))\n",
    "    return df\n",
    "\n",
    "def concatenate_ordered_names(row):\n",
    "    return ' '.join(row.tolist())\n",
    "\n",
    "def concatenate_reversed_names(row):\n",
    "    lst = row.tolist()\n",
    "    lst = lst[::-1]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_out_of_order_names(row):\n",
    "    lst = row.tolist()\n",
    "    if len(set(lst)) < 3:\n",
    "        return ''\n",
    "    rev = lst[:]\n",
    "    rev = rev[::-1]\n",
    "    shuff = lst[:]\n",
    "    random.shuffle(shuff)\n",
    "    while shuff == rev or shuff == lst:\n",
    "        random.shuffle(shuff)\n",
    "    return ' '.join(shuff)\n",
    "\n",
    "def concatenate_append_random_name(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    if len(lst) == 1:\n",
    "        lct = random.randrange(0, 2)\n",
    "    else:\n",
    "        lct = random.randrange(1, len(lst)+1)\n",
    "    rnd_name = ''\n",
    "    if current_languages_pairs == 'eng_eng':\n",
    "        rnd_name = random.choice(all_eng_names)[0]\n",
    "    elif current_languages_pairs == 'arb_arb':\n",
    "        rnd_name = random.choice(all_arb_names)[0]\n",
    "    elif current_languages_pairs == 'eng_arb':\n",
    "        row_index_for_eng_arb += 1\n",
    "        if current_processed_pair == 'eng':\n",
    "            if (row_index_for_eng_arb % 2 == 1) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_eng_names)[0]\n",
    "        elif current_processed_pair == 'arb':\n",
    "            if (row_index_for_eng_arb % 2 == 0) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_arb_names)[0]\n",
    "\n",
    "    lst.insert(lct, rnd_name)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_name_in_middle(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    lct = random.randrange(1, len(lst))\n",
    "    rnd_name = ''\n",
    "    if current_languages_pairs == 'eng_eng':\n",
    "        rnd_name = random.choice(all_eng_names)[0]\n",
    "    elif current_languages_pairs == 'arb_arb':\n",
    "        rnd_name = random.choice(all_arb_names)[0]\n",
    "    elif current_languages_pairs == 'eng_arb':\n",
    "        row_index_for_eng_arb += 1\n",
    "        if current_processed_pair == 'eng':\n",
    "            if (row_index_for_eng_arb % 2 == 1) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_eng_names)[0]\n",
    "        elif current_processed_pair == 'arb':\n",
    "            if (row_index_for_eng_arb % 2 == 0) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_arb_names)[0]\n",
    "\n",
    "    lst.insert(lct, rnd_name)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "\n",
    "def concatenate_append_random_two_names(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    row_index_for_eng_arb += 1\n",
    "    if len(lst) == 1:\n",
    "        lct = random.randrange(2)\n",
    "        if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "    else:\n",
    "        lct = random.randrange(1, len(lst)+1)        \n",
    "        if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "            lct = random.randrange(1, len(lst)+1)\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "            lct = random.randrange(1, len(lst)+1)\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])                \n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_two_names_in_middle(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    row_index_for_eng_arb += 1\n",
    "    lct = random.randrange(1, len(lst))        \n",
    "    if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        lct = random.randrange(1, len(lst))\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "    elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "        lct = random.randrange(1, len(lst))\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])                \n",
    "    return ' '.join(lst)\n",
    "\n",
    "\n",
    "def concatenate_append_random_three_names(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    lct = random.randrange(2)\n",
    "    if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "    elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_four_names(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = []\n",
    "    if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "    elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def process_and_save_pairs(df, number_of_parts, name_of_output_file, part1_apply_function, part2_apply_function):\n",
    "    global row_index_for_eng_arb, current_processed_pair\n",
    "    folder = target_dir + folder_name + \"/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    number_of_pairs = len(df)\n",
    "    pairs_indexes = number_of_parts * list(range(0,number_of_pairs))\n",
    "    random.shuffle(pairs_indexes)\n",
    "    name_pairs = []\n",
    "    for row in df.itertuples():\n",
    "        index,part1,part2,group = row\n",
    "        if index % 100000 == 0:\n",
    "            print(\"batch {index}\".format(index=(index / 100000)))\n",
    "        for i in range(0, number_of_parts):\n",
    "            name_pairs.append([part1, part2, pairs_indexes.pop()])\n",
    "    \n",
    "    df = pd.DataFrame(name_pairs)\n",
    "    df.columns = ['pair1', 'pair2', 'group']\n",
    "    grp = df.groupby('group')\n",
    "    if current_languages_pairs == 'eng_arb':\n",
    "        current_processed_pair = 'eng'\n",
    "        row_index_for_eng_arb = 0\n",
    "    df['pair1'] = grp['pair1'].apply(part1_apply_function)\n",
    "    if current_languages_pairs == 'eng_arb':\n",
    "        current_processed_pair = 'arb'\n",
    "        row_index_for_eng_arb = 0\n",
    "    df['pair2'] = grp['pair2'].apply(part2_apply_function)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    print(len(df))\n",
    "    df.to_csv(folder + name_of_output_file + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from queue import Queue\n",
    "selected_rnd_names_for_initial = Queue()\n",
    "\n",
    "folder_name = 'eng_eng/'\n",
    "df = prepare_df(eng_eng_df)\n",
    "current_languages_pairs = 'eng_eng'\n",
    "\n",
    "process_and_save_pairs(df, 4, 'pos_4x4_ordered_pairs', concatenate_ordered_names, concatenate_ordered_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def process_all_language_pairs(dataset, language_pair, desired_length = 0):\n",
    "    global current_languages_pairs, folder_name, row_index_for_eng_arb, selected_rnd_names_for_initial, ignore_row_index_for_eng_arb\n",
    "    \n",
    "    row_index_for_eng_arb = 0\n",
    "    ignore_row_index_for_eng_arb = True\n",
    "    selected_rnd_names_for_initial = Queue()\n",
    "    current_languages_pairs = language_pair\n",
    "    \n",
    "    if desired_length == 0:\n",
    "        df = prepare_df(dataset)\n",
    "    else:\n",
    "        df = prepare_duplicate_df(dataset, desired_length)\n",
    "    \n",
    "    folder_name = current_languages_pairs + '/'\n",
    "    '''\n",
    "    #4x4\n",
    "    process_and_save_pairs(df, 4, 'pos_4x4_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 4, 'pos_4x4_reversed_pairs', concatenate_ordered_names, concatenate_reversed_names)\n",
    "    process_and_save_pairs(df, 4, 'neg_4x4_unordered_pairs', concatenate_ordered_names, concatenate_out_of_order_names)\n",
    "    process_and_save_pairs(df, 3, 'pos_initial_3x4_pairs', concatenate_with_initial, concatenate_with_initial_name)\n",
    "    process_and_save_pairs(df, 3, 'neg_3_1x3_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    process_and_save_pairs(df, 2, 'neg_2_2x2_2_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_3x1_3_pairs', concatenate_append_random_three_names, concatenate_append_random_three_names)\n",
    "\n",
    "    #3x4\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_2x1_3_pairs', concatenate_append_random_two_names, concatenate_append_random_three_names)\n",
    "    process_and_save_pairs(df, 3, 'pos_3x4_reversed_pairs', concatenate_reversed_names, concatenate_append_random_name_in_middle)\n",
    "    \n",
    "    #2x4\n",
    "    process_and_save_pairs(df, 2, 'pos_2x4_reversed_pairs', concatenate_reversed_names, concatenate_append_random_two_names_in_middle)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_3_pairs', concatenate_append_random_name, concatenate_append_random_three_names)\n",
    "    \n",
    "    #1x4\n",
    "    process_and_save_pairs(df, 1, 'neg_1x4_pairs', concatenate_ordered_names, concatenate_append_random_four_names)\n",
    "    \n",
    "    #3x3\n",
    "    process_and_save_pairs(df, 3, 'pos_3x3_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 3, 'pos_3x3_reversed_pairs', concatenate_ordered_names, concatenate_reversed_names)\n",
    "    '''\n",
    "    process_and_save_pairs(df, 3, 'neg_3x3_unordered_pairs', concatenate_ordered_names,  concatenate_out_of_order_names)\n",
    "    '''\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_2x1_2_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "    process_and_save_pairs(df, 2, 'pos_initial_2x3_pairs', concatenate_with_initial, concatenate_with_initial_name)\n",
    "\n",
    "    #2x3\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    \n",
    "    #2x2\n",
    "    process_and_save_pairs(df, 2, 'pos_2x2_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 2, 'pos_2x2_reversed_pairs', concatenate_ordered_names, concatenate_reversed_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    process_and_save_pairs(df, 1, 'pos_1x2_initails_pairs', concatenate_with_initial, concatenate_with_initial_name)\n",
    "    \n",
    "    #1x1\n",
    "    process_and_save_pairs(df, 1, 'pos_1x1_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "\n",
    "    ignore_row_index_for_eng_arb = False\n",
    "    if current_languages_pairs == 'eng_arb':        \n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x4\n",
    "        process_and_save_pairs(df, 1, 'pos_1x4_pairs', concatenate_append_random_three_names, concatenate_append_random_three_names)\n",
    "        \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "        \n",
    "        #1x3\n",
    "        process_and_save_pairs(df, 1, 'pos_1x3_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x2\n",
    "        process_and_save_pairs(df, 1, 'pos_1x2_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    else:\n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_ordered_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x4\n",
    "        process_and_save_pairs(df, 1, 'pos_1x4_pairs', concatenate_ordered_names, concatenate_append_random_three_names)\n",
    "        \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "        \n",
    "        #1x3\n",
    "        process_and_save_pairs(df, 1, 'pos_1x3_pairs', concatenate_ordered_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x2\n",
    "        process_and_save_pairs(df, 1, 'pos_1x2_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def process_all_language_pairs(dataset, language_pair, desired_length = 0):\n",
    "    global current_languages_pairs, folder_name, row_index_for_eng_arb, selected_rnd_names_for_initial, ignore_row_index_for_eng_arb\n",
    "    \n",
    "    row_index_for_eng_arb = 0\n",
    "    ignore_row_index_for_eng_arb = True\n",
    "    selected_rnd_names_for_initial = Queue()\n",
    "    current_languages_pairs = language_pair\n",
    "    \n",
    "    if desired_length == 0:\n",
    "        df = prepare_df(dataset)\n",
    "    else:\n",
    "        df = prepare_duplicate_df(dataset, desired_length)\n",
    "    \n",
    "    folder_name = current_languages_pairs + '/'\n",
    "    \n",
    "    #4x4\n",
    "    process_and_save_pairs(df, 4, 'pos_4x4_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 4, 'neg_4x4_unordered_pairs', concatenate_ordered_names, concatenate_out_of_order_names)\n",
    "    process_and_save_pairs(df, 3, 'neg_3_1x3_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "\n",
    "    #3x4\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    \n",
    "    #2x4\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_3_pairs', concatenate_append_random_name, concatenate_append_random_three_names)\n",
    "        \n",
    "    #3x3\n",
    "    process_and_save_pairs(df, 3, 'pos_3x3_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 3, 'neg_3x3_unordered_pairs', concatenate_ordered_names,  concatenate_out_of_order_names)\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "\n",
    "    #2x3\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    \n",
    "    #2x2\n",
    "    process_and_save_pairs(df, 2, 'pos_2x2_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    \n",
    "    #1x1\n",
    "    process_and_save_pairs(df, 1, 'pos_1x1_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "\n",
    "    ignore_row_index_for_eng_arb = False\n",
    "    if current_languages_pairs == 'eng_arb':        \n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "                \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    else:\n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_ordered_names, concatenate_append_random_two_names)\n",
    "                \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_ordered_names, concatenate_append_random_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#process_all_language_pairs(eng_eng_df, 'eng_eng')\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "process_all_language_pairs(arb_arb_df, 'arb_arb', len(eng_eng_no_dup_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_all_language_pairs(eng_eng_df, 'eng_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "process_all_language_pairs(eng_arb_df, 'eng_arb', len(eng_eng_no_dup_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = target_dir + \"eng_eng/\"\n",
    "pos_3x4_initails_df = pd.read_csv(folder + 'neg_1x1_pairs.tsv',sep='\\t', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_3x4_initails_df[pos_3x4_initails_df[1] == 'MAHMOUD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_save_files(file_name, flag):\n",
    "    df = pd.read_csv(file_name + '.tsv',sep='\\t', header=None)\n",
    "    cols = df.columns.tolist()\n",
    "    if len(cols) == 4:\n",
    "        df.drop(df.columns[3], axis=1, inplace=True)\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "        df['similarity'] = flag\n",
    "        cols = df.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        df = df[cols]\n",
    "        df = df[df[1] != 'pair1']\n",
    "        df.to_csv(file_name + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "    elif len(cols) == 3:\n",
    "        df[0] = flag\n",
    "        df = df[df[1] != 'pair1']\n",
    "        df.to_csv(file_name + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_files(language_pair):\n",
    "    path = target_dir + language_pair + '/'\n",
    "'''\n",
    "    positive_files = [\n",
    "        'pos_4x4_pairs', 'pos_4x4_reversed_pairs', 'pos_initial_3x4_pairs',\n",
    "        'pos_3x4_pairs', 'pos_3x4_reversed_pairs',\n",
    "        'pos_2x4_pairs', 'pos_2x4_reversed_pairs',\n",
    "        'pos_1x4_pairs',\n",
    "        'pos_3x3_pairs', 'pos_3x3_reversed_pairs', 'pos_initial_2x3_pairs',\n",
    "        'pos_2x3_pairs',\n",
    "        'pos_1x3_pairs',\n",
    "        'pos_2x2_pairs', 'pos_2x2_reversed_pairs', 'pos_1x2_initails_pairs',\n",
    "        'pos_1x2_pairs',\n",
    "        'pos_1x1_pairs'\n",
    "    ]\n",
    "    \n",
    "    negative_files = [\n",
    "        'neg_4x4_pairs', 'neg_4x4_unordered_pairs', 'neg_3_1x3_1_pairs', 'neg_2_2x2_2_pairs', 'neg_1_3x1_3_pairs',\n",
    "        'neg_3x4_pairs', 'neg_2_1x2_2_pairs', 'neg_1_2x1_3_pairs',\n",
    "        'neg_2x4_pairs', 'neg_1_1x1_3_pairs',\n",
    "        'neg_1x4_pairs',\n",
    "        'neg_3x3_pairs', 'neg_3x3_unordered_pairs', 'neg_2_1x2_1_pairs', 'neg_1_2x1_2_pairs',\n",
    "        'neg_2x3_pairs', 'neg_1_1x1_2_pairs',\n",
    "        'neg_1x3_pairs',\n",
    "        'neg_2x2_pairs', 'neg_1_1x1_1_pairs',\n",
    "        'neg_1x2_pairs',\n",
    "        'neg_1x1_pairs'\n",
    "    ]\n",
    "'''  \n",
    "\n",
    "    positive_files = [\n",
    "        'pos_4x4_pairs', 'pos_3x4_pairs', 'pos_2x4_pairs', \n",
    "        'pos_3x3_pairs', 'pos_2x3_pairs',\n",
    "        'pos_2x2_pairs', \n",
    "        'pos_1x1_pairs'\n",
    "    ]\n",
    "    \n",
    "    negative_files = [\n",
    "        'neg_4x4_unordered_pairs', 'neg_3_1x3_1_pairs',\n",
    "        'neg_2_1x2_2_pairs',\n",
    "        'neg_1_1x1_3_pairs',\n",
    "        'neg_3x3_unordered_pairs', 'neg_2_1x2_1_pairs',\n",
    "        'neg_1_1x1_2_pairs',\n",
    "        'neg_1_1x1_1_pairs',\n",
    "        'neg_1x1_pairs'\n",
    "    ]\n",
    "    for positive_file in positive_files:\n",
    "        unify_save_files(path + positive_file, 1)\n",
    "    \n",
    "    for negative_file in negative_files:\n",
    "        unify_save_files(path + negative_file, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "unify_files('eng_eng')\n",
    "unify_files('eng_arb')\n",
    "unify_files('arb_arb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = list(names_dict.keys())\n",
    "\n",
    "def create_negative_datasets(iterable_list, pair1_lang, pair2_lang):\n",
    "    folder = target_dir + pair1_lang + '_' + pair2_lang + \"/\"\n",
    "    neg_4x4_pairs = [] #7\n",
    "    len_4x4_pair1 = 3\n",
    "    len_4x4_pair2 = 4\n",
    "\n",
    "    neg_3x4_pairs = [] #6\n",
    "    len_3x4_pair1 = 2\n",
    "    len_3x4_pair2 = 4\n",
    "\n",
    "    neg_2x4_pairs = [] #5\n",
    "    len_2x4_pair1 = 1\n",
    "    len_2x4_pair2 = 4\n",
    "\n",
    "    #neg_1x4_pairs = [] #4\n",
    "\n",
    "    neg_3x3_pairs = [] #5\n",
    "    len_3x3_pair1 = 2\n",
    "    len_3x3_pair2 = 3\n",
    "\n",
    "    neg_2x3_pairs = [] #4\n",
    "    len_2x3_pair1 = 1\n",
    "    len_2x3_pair2 = 3\n",
    "\n",
    "    neg_1x3_pairs = [] #3\n",
    "    len_1x3_pair1 = 0\n",
    "    len_1x3_pair2 = 3\n",
    "\n",
    "    neg_2x2_pairs = [] #3\n",
    "    len_2x2_pair1 = 1\n",
    "    len_2x2_pair2 = 2\n",
    "\n",
    "    neg_1x2_pairs = [] #2\n",
    "    len_1x2_pair1 = 0\n",
    "    len_1x2_pair2 = 2\n",
    "\n",
    "    neg_1x1_pairs = [] #1\n",
    "    len_1x1_pair1 = 0\n",
    "    len_1x1_pair2 = 1\n",
    "    \n",
    "    multiplier = int(len(eng_eng_no_dup_df) / len(iterable_list))\n",
    "    number_of_random_needed = multiplier * 36 + 1\n",
    "    \n",
    "    for name in iterable_list:\n",
    "        sel_keys = set(random.sample(keys, number_of_random_needed))\n",
    "        sel_keys = [key for key in sel_keys if key != name[1]]\n",
    "        pair1_keys = sel_keys[:multiplier * 10]\n",
    "        pair1_src = [random.sample(names_dict[item][pair1_lang], 1)[0] for item in pair1_keys]\n",
    "        pair2_keys = sel_keys[multiplier * 10:]\n",
    "        pair2_src = [random.sample(names_dict[item][pair2_lang], 1)[0] for item in pair2_keys]\n",
    "                \n",
    "        pair1_index = 0\n",
    "        pair2_index = 0\n",
    "\n",
    "        #4x4\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_4x4_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_4x4_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_4x4_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "    \n",
    "        #3x4\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_3x4_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_3x4_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_3x4_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "        \n",
    "        #2x4\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_2x4_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_2x4_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_2x4_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #3x3\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_3x3_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_3x3_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_3x3_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #2x3\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_2x3_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_2x3_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_2x3_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #1x3\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_1x3_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_1x3_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_1x3_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #2x2\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_2x2_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_2x2_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_2x2_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #1x2\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_1x2_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_1x2_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_1x2_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #1x1\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_1x1_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_1x1_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_1x1_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "    \n",
    "    df = pd.DataFrame(neg_4x4_pairs)\n",
    "    df.to_csv(folder + 'neg_4x4_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_3x4_pairs)\n",
    "    df.to_csv(folder + 'neg_3x4_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_2x4_pairs)\n",
    "    df.to_csv(folder + 'neg_2x4_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False)     \n",
    "    df = pd.DataFrame(neg_3x3_pairs)\n",
    "    df.to_csv(folder + 'neg_3x3_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_2x3_pairs)\n",
    "    df.to_csv(folder + 'neg_2x3_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_1x3_pairs)\n",
    "    df.to_csv(folder + 'neg_1x3_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_2x2_pairs)\n",
    "    df.to_csv(folder + 'neg_2x2_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_1x2_pairs)\n",
    "    df.to_csv(folder + 'neg_1x2_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_1x1_pairs)\n",
    "    df.to_csv(folder + 'neg_1x1_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "create_single_negative_datasets()\n",
    "#create_single_negative_datasets(all_eng_names, 'eng', 'arb')\n",
    "#create_single_negative_datasets(all_arb_names, 'arb', 'arb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "keys = list(names_dict.keys())\n",
    "neg_eng_eng_pairs = []\n",
    "neg_eng_arb_pairs = []\n",
    "neg_arb_arb_pairs = []\n",
    "print(len(keys))\n",
    "\n",
    "def create_single_negative_datasets():\n",
    "    folder = target_dir + \"fuzzy/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    eng_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_eng_names))\n",
    "    arb_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_arb_names))\n",
    "    \n",
    "    i = 0\n",
    "    for key in keys:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"batch {index}\".format(index=(i / 1000)))\n",
    "        i += 1\n",
    "        \n",
    "        eng_names = names_dict[key]['eng']\n",
    "        #eng_names_len = len(eng_names)\n",
    "        arb_names = names_dict[key]['arb']\n",
    "        #arb_names_len = len(arb_names)\n",
    "        #similar_keys1 = set([name[1] for name in all_eng_names if any(s in name[0] for s in eng_names)])\n",
    "        #similar_keys2 = set([name[1] for name in all_arb_names if any(s in name[0] for s in arb_names)])\n",
    "        #similar_keys = similar_keys1.union(similar_keys2)\n",
    "        #other_eng_names = [name[0]for name in all_eng_names if name[1] not in similar_keys and 100 > fuzz.partial_ratio(eng_names[0], name[0]) > 49]\n",
    "        #other_arb_names = [name[0] for name in all_arb_names if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "\n",
    "        '''\n",
    "        eng_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_eng_names))\n",
    "        needed_eng_eng_negatives = eng_multiplier * eng_names_len\n",
    "        if len(other_eng_names) >= needed_eng_eng_negatives:\n",
    "            eng_eng_src_names = random.sample(other_eng_names, needed_eng_eng_negatives)\n",
    "        else:\n",
    "            print('problem generating eng_eng for key:{key}'.format(key=key))\n",
    "\n",
    "        needed_eng_arb_negatives = eng_multiplier * eng_names_len\n",
    "        if len(other_arb_names) >= needed_eng_arb_negatives:\n",
    "            eng_arb_src_names = random.sample(other_arb_names, needed_eng_arb_negatives)\n",
    "        else:\n",
    "            print('problem generating eng_arb for key:{key}'.format(key=key))\n",
    "\n",
    "        arb_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_arb_names))\n",
    "        needed_arb_arb_negatives = arb_multiplier * arb_names_len\n",
    "        if len(other_arb_names) >= needed_arb_arb_negatives:\n",
    "            arb_arb_src_names = random.sample(other_arb_names, needed_arb_arb_negatives)\n",
    "        else:\n",
    "            print('problem generating arb_arb for key:{key}'.format(key=key))\n",
    "        \n",
    "        #other_eng_names.sort(key = lambda x: x[1],reverse=True)\n",
    "        #other_arb_names.sort(key = lambda x: x[1],reverse=True)\n",
    "\n",
    "        index = 0\n",
    "        '''\n",
    "        for eng in eng_names:\n",
    "            src = []\n",
    "            while len(src) < (eng_multiplier):\n",
    "                rnd = random.sample(all_eng_names, (eng_multiplier*2))\n",
    "                eng_eng_src_names = [name[0]for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(eng, name[0]) > 49]\n",
    "                src.extend(eng_eng_src_names)\n",
    "            neg_eng_eng_pairs.extend(list(product([eng], src)))\n",
    "            src = []\n",
    "            while len(src) < (eng_multiplier):\n",
    "                rnd = random.sample(all_arb_names, (eng_multiplier*2))\n",
    "                eng_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "                src.extend(eng_arb_src_names)\n",
    "            neg_eng_arb_pairs.extend(list(product([eng], src)))\n",
    "            '''\n",
    "            lower_limit = index * eng_multiplier\n",
    "            higher_limit = ((index + 1) * eng_multiplier)\n",
    "            index += 1\n",
    "            src = eng_eng_src_names[lower_limit:higher_limit]\n",
    "            #src = [item[0] for item in src]\n",
    "            neg_eng_eng_pairs.extend(list(product([eng], src)))\n",
    "            src = eng_arb_src_names[lower_limit:higher_limit]\n",
    "            #src = [item[0] for item in src]\n",
    "            neg_eng_arb_pairs.extend(list(product([eng], src)))\n",
    "            '''\n",
    "        index = 0\n",
    "        for arb in arb_names:\n",
    "            src = []\n",
    "            while len(src) < (arb_multiplier):\n",
    "                rnd = random.sample(all_arb_names, (arb_multiplier*100))\n",
    "                arb_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb, name[0]) > 49]\n",
    "                src.extend(arb_arb_src_names)\n",
    "\n",
    "            neg_arb_arb_pairs.extend(list(product([arb], src)))\n",
    "            '''\n",
    "            lower_limit = index * arb_multiplier\n",
    "            higher_limit = ((index + 1) * arb_multiplier)\n",
    "            index += 1\n",
    "            src = arb_arb_src_names[lower_limit:higher_limit]\n",
    "            #src = [item[0] for item in src]\n",
    "            neg_arb_arb_pairs.extend(list(product([arb], src)))\n",
    "            '''\n",
    "    \n",
    "    df = pd.DataFrame(neg_eng_eng_pairs)\n",
    "    df.to_csv(folder + 'neg_eng_eng_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_eng_arb_pairs)\n",
    "    df.to_csv(folder + 'neg_eng_arb_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_arb_arb_pairs)\n",
    "    df.to_csv(folder + 'neg_arb_arb_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = target_dir + \"fuzzy/\"\n",
    "df = pd.read_csv(folder + 'neg_eng_eng_pairs.tsv',sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "neg_eng_eng_pairs = []\n",
    "neg_eng_arb_pairs = []\n",
    "neg_arb_arb_pairs = []\n",
    "\n",
    "#similar_keys = [k for k in keys if k == key]\n",
    "eng_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_eng_names))\n",
    "arb_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_arb_names))\n",
    "\n",
    "key = 1\n",
    "#similar_keys1 = set([name[1] for name in all_eng_names if any(s in name[0] for s in eng_names)])\n",
    "#similar_keys2 = set([name[1] for name in all_arb_names if any(s in name[0] for s in arb_names)])\n",
    "#similar_keys = similar_keys1.union(similar_keys2)\n",
    "similar_keys = [key]\n",
    "eng_names = names_dict[key]['eng']\n",
    "arb_names = names_dict[key]['arb']\n",
    "\n",
    "index = 0\n",
    "for eng in eng_names:\n",
    "    src = []\n",
    "    while len(src) < (eng_multiplier):\n",
    "        rnd = random.sample(all_eng_names, (eng_multiplier*2))\n",
    "        eng_eng_src_names = [name[0]for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(eng_names[0], name[0]) > 49]\n",
    "        src.extend(eng_eng_src_names)\n",
    "    '''\n",
    "    lower_limit = index * eng_multiplier\n",
    "    higher_limit = ((index + 1) * eng_multiplier)\n",
    "    index += 1\n",
    "    src = eng_eng_src_names[lower_limit:higher_limit]\n",
    "    #src = [item[0] for item in src]\n",
    "    '''\n",
    "    neg_eng_eng_pairs.extend(list(product([eng], src)))\n",
    "    src = []\n",
    "    while len(src) < (eng_multiplier):\n",
    "        rnd = random.sample(all_arb_names, (eng_multiplier))\n",
    "        eng_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "        src.extend(eng_arb_src_names)\n",
    "    #src = eng_arb_src_names[lower_limit:higher_limit]\n",
    "    #src = [item[0] for item in src]\n",
    "    neg_eng_arb_pairs.extend(list(product([eng], src)))\n",
    "\n",
    "index = 0\n",
    "for arb in arb_names:\n",
    "    '''\n",
    "    lower_limit = index * arb_multiplier\n",
    "    higher_limit = ((index + 1) * arb_multiplier)\n",
    "    index += 1\n",
    "    src = arb_arb_src_names[lower_limit:higher_limit]\n",
    "    #src = [item[0] for item in src]\n",
    "    '''\n",
    "    src = []\n",
    "    while len(src) < (arb_multiplier):\n",
    "        rnd = random.sample(all_arb_names, (arb_multiplier*100))\n",
    "        arb_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "        src.extend(arb_arb_src_names)\n",
    "    \n",
    "    neg_arb_arb_pairs.extend(list(product([arb], src)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[item for sublist in t for item in sublist[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## four part names\n",
    "if len(eng_eng_df.columns) == 4:\n",
    "    eng_eng_df.drop(eng_eng_df.columns[0], axis=1, inplace=True)\n",
    "eng_eng_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "eng_eng_df = eng_eng_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_eng_df = eng_eng_df.reset_index()\n",
    "eng_eng_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "eng_eng_df['set_type'] = eng_eng_df['index'] - eng_eng_df['index'] % 4\n",
    "eng_eng_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_eng_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grp = eng_eng_df.groupby('set_type', sort=False)\n",
    "eng_eng_df['part1'] =  grp['part1'].apply(lambda x: ' '.join(x))\n",
    "eng_eng_df['part2'] = eng_eng_df.groupby('set_type')['part2'].apply(lambda x: ' '.join(x))\n",
    "eng_eng_df['group_id'] = eng_eng_df.groupby('set_type')['group_id'].apply(lambda x: ', '.join(str(x)))\n",
    "eng_eng_df = eng_eng_df.drop_duplicates(subset=['set_type'], keep='first').copy()\n",
    "eng_eng_df.drop('set_type', axis=1, inplace=True)\n",
    "eng_eng_df = eng_eng_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_eng_df['similarity'] = 1\n",
    "cols = eng_eng_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "eng_eng_df = eng_eng_df[cols]\n",
    "#eng_eng_df.columns = ['similarity', 'part1', 'part2', 'group_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(eng_eng_df.drop_duplicates(subset=[1, 2], keep='first'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(eng_eng_df))\n",
    "print(len(eng_arb_df))\n",
    "print(len(arb_arb_df))\n",
    "\n",
    "\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "eng_arb_no_dup_df = eng_arb_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "arb_arb_no_dup_df = arb_arb_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "\n",
    "print(len(eng_eng_no_dup_df))\n",
    "print(len(eng_arb_no_dup_df))\n",
    "print(len(arb_arb_no_dup_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
