{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from configparser import ConfigParser\n",
    "import pandas as pd\n",
    "\n",
    "def config(filename='prepare_data.ini', section='phonetic'):\n",
    "    parser = ConfigParser()\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    params = config()\n",
    "    conn = psycopg2.connect(**params)\n",
    "    print('Connected to the PostgreSQL database...')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def read_given_names():\n",
    "    conn = db_connect()\n",
    "    sql_result = pd.DataFrame()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM (\n",
    "                SELECT ENG, ARB, SUM(COUNT) AS COUNT FROM (\n",
    "                    SELECT ENG, ARB, COUNT FROM GIVEN_NAMES_MASTER\n",
    "                    WHERE ENG IS NOT NULL AND ENG != '' AND ARB IS NOT NULL AND ARB != ''\n",
    "                ) AS SUB GROUP BY ENG, ARB\n",
    "                ORDER BY COUNT DESC\n",
    "            ) AS S\n",
    "            UNION ALL\n",
    "            SELECT * FROM (\n",
    "                SELECT ENG, ARB, SUM(COUNT) AS COUNT FROM (\n",
    "                    SELECT ENG, ARB, COUNT FROM FAMILY_NAMES_MASTER\n",
    "                    WHERE ENG IS NOT NULL AND ENG != '' AND ARB IS NOT NULL AND ARB != ''\n",
    "                ) AS SUB GROUP BY ENG, ARB\n",
    "                ORDER BY COUNT DESC\n",
    "            ) AS S\n",
    "            UNION ALL\n",
    "            SELECT * FROM (\n",
    "                SELECT ENG, ARB, SUM(FREQ) AS COUNT FROM (\n",
    "                    SELECT ENG, ARB, FREQ FROM GIVEN_NAMES_DAN\n",
    "                    WHERE ENG IS NOT NULL AND ENG != '' AND ARB IS NOT NULL AND ARB != ''\n",
    "                ) AS SUB GROUP BY ENG, ARB\n",
    "                ORDER BY COUNT DESC\n",
    "            ) AS S\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        sql_result = pd.read_sql(query, con=conn)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return sql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from itertools import permutations, repeat, combinations, chain, product, cycle\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import requests\n",
    "import pyphi\n",
    "\n",
    "target_dir = \"/home/jupyter/notebooks/PoC/data-preparation/output/understanding_data/\"\n",
    "all_eng_given_names = []\n",
    "all_arb_given_names = []\n",
    "all_pairs = []\n",
    "\n",
    "def get_yamli_arabic_varinats(name):\n",
    "    name = ''.join(e for e in name if e.isalnum())\n",
    "    url = 'http://api.yamli.com/transliterate.ashx?tool=api&account_id=&prot=http%3A&hostname=fuzzyarabic.herokuapp.com&path=%2F&build=5447&sxhr_id=51&word=' + name\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if len(response.text) > 66:\n",
    "        arr = pyphi.jsonify.loads(response.text[62:-4])['data']\n",
    "        data = pyphi.jsonify.loads(arr)['r']\n",
    "        variants = data.split('|')\n",
    "    else:\n",
    "        variants = []\n",
    "    variants = [''.join(e for e in name if e.isalnum() and not e.isdigit()) for name in variants]\n",
    "    return variants\n",
    "\n",
    "def format_variants_list(s):\n",
    "    return [','.join(sorted(set(s.tolist())))]\n",
    "\n",
    "def format_variants_list2(s):\n",
    "    lis = s.tolist()\n",
    "    flat_list = [item for sublist in lis for item in sublist.split(',')]\n",
    "\n",
    "    return ','.join(sorted(set(flat_list)))\n",
    "\n",
    "def remove_long_variants(s):    \n",
    "    a = list(set(s['eng_variants'].split(',')))\n",
    "    l = len(s['trimmed_eng'])\n",
    "    a = [item for item in a if (l * 0.3 < len(item) <= l * 2 and item not in s['trimmed_eng']) or item == s['trimmed_eng'] ]\n",
    "    s['eng_variants'] = ','.join(sorted(set(a)))\n",
    "    \n",
    "    a = list(set(s['arb_variants'].split(',')))\n",
    "    l = len(s['trimmed_arb'])\n",
    "    a = [item for item in a if (l * 0.3 < len(item) <= l * 2 and item not in s['trimmed_arb']) or item == s['trimmed_arb'] ]\n",
    "    s['arb_variants'] = ','.join(sorted(set(a)))\n",
    "    return s\n",
    "\n",
    "def create_negative(row):\n",
    "    global all_eng_given_names, all_arb_given_names\n",
    "    \n",
    "    good_eng_variants = list(set(row['eng_variants'].split(',')))\n",
    "    good_arb_variants = list(set(row['arb_variants'].split(',')))\n",
    "\n",
    "    eng_variants_len = len(good_eng_variants)\n",
    "    arb_variants_len = len(good_arb_variants)\n",
    "    \n",
    "    '''\n",
    "    if(eng_variants_len == 1 and arb_variants_len == 1):\n",
    "        good_eng_variants = [repeated for value in good_eng_variants for repeated in repeat(value, 2)]\n",
    "        good_arb_variants = [repeated for value in good_arb_variants for repeated in repeat(value, 2)]\n",
    "    '''\n",
    "    \n",
    "    if eng_variants_len > arb_variants_len:\n",
    "        bigger_variants_count = eng_variants_len\n",
    "    else:\n",
    "        bigger_variants_count = arb_variants_len\n",
    "        \n",
    "    ### making good arabic and english variants of same length\n",
    "    '''\n",
    "    if eng_variants_len > arb_variants_len:\n",
    "        quotient, modulo = divmod(eng_variants_len, arb_variants_len)\n",
    "        extension = random.sample(good_arb_variants, modulo)\n",
    "        good_arb_variants = [repeated for value in good_arb_variants for repeated in repeat(value, quotient)]\n",
    "        good_arb_variants.extend(extension)\n",
    "    elif arb_variants_len > eng_variants_len:\n",
    "        quotient, modulo = divmod(arb_variants_len, eng_variants_len)\n",
    "        extension = random.sample(good_eng_variants, modulo)\n",
    "        good_eng_variants = [repeated for value in good_eng_variants for repeated in repeat(value, quotient)]\n",
    "        good_eng_variants.extend(extension)\n",
    "    \n",
    "    eng_variants_len = len(good_eng_variants)\n",
    "    arb_variants_len = len(good_arb_variants)\n",
    "    '''\n",
    "    \n",
    "    random_eng_negative = []\n",
    "    random_arb_negative = []\n",
    "    \n",
    "    desired_negative_length_per_set = math.ceil(bigger_variants_count * (bigger_variants_count - 1) * 0.75)\n",
    "    needed_negative_length = desired_negative_length_per_set + bigger_variants_count\n",
    "    \n",
    "    if (len(all_eng_given_names) - bigger_variants_count) > (needed_negative_length):\n",
    "        random_eng_negative = random.sample(all_eng_given_names, needed_negative_length)\n",
    "        random_eng_negative = list(np.setdiff1d(random_eng_negative, good_eng_variants, assume_unique=True))\n",
    "        random_eng_negative = random.sample(random_eng_negative, desired_negative_length_per_set)\n",
    "    else:\n",
    "        random_eng_negative = list(np.setdiff1d(all_eng_given_names, good_eng_variants, assume_unique=True))\n",
    "        '''\n",
    "        quotient, modulo = divmod(bigger_variants_count * (bigger_variants_count - 1), len(random_eng_negative))\n",
    "        extension = random.sample(random_eng_negative, modulo)\n",
    "        random_eng_negative = [repeated for value in random_eng_negative for repeated in repeat(value, quotient)]\n",
    "        random_eng_negative.extend(extension)\n",
    "        ''' \n",
    "        \n",
    "    if (len(all_arb_given_names) - bigger_variants_count) > (needed_negative_length):\n",
    "        random_arb_negative = random.sample(all_arb_given_names, needed_negative_length)\n",
    "        random_arb_negative = list(np.setdiff1d(random_arb_negative, good_arb_variants, assume_unique=True))\n",
    "        random_arb_negative = random.sample(random_arb_negative, desired_negative_length_per_set)\n",
    "    else:\n",
    "        random_arb_negative = list(np.setdiff1d(all_arb_given_names, good_arb_variants, assume_unique=True))\n",
    "        '''\n",
    "        quotient, modulo = divmod(bigger_variants_count * (bigger_variants_count - 1), len(random_arb_negative))\n",
    "        extension = random.sample(random_arb_negative, modulo)\n",
    "        random_arb_negative = [repeated for value in random_arb_negative for repeated in repeat(value, quotient)]\n",
    "        random_arb_negative.extend(extension)\n",
    "        '''\n",
    "    \n",
    "    #row['eng_variants'] = ','.join(good_eng_variants)\n",
    "    #row['arb_variants'] = ','.join(good_arb_variants)\n",
    "    row['negative_eng_variants'] = ','.join(random_eng_negative)\n",
    "    row['negative_arb_variants'] = ','.join(random_arb_negative)\n",
    "\n",
    "    return row\n",
    "\n",
    "def build_top_given_names(min_count):\n",
    "    global top_given_names, all_eng_given_names, all_arb_given_names, low_accuracy_names\n",
    "    given_names = read_given_names()\n",
    "        \n",
    "    given_names['trimmed_eng'] = given_names['eng'].map(lambda x: x.strip(' '))\n",
    "    given_names['trimmed_arb'] = given_names['arb'].map(lambda x: x.strip(' '))\n",
    "    given_names['count'] = given_names.groupby(['trimmed_eng', 'trimmed_arb'])['count'].transform('sum')\n",
    "    given_names = given_names.drop_duplicates(subset=['trimmed_eng', 'trimmed_arb'], keep='first')\n",
    "    \n",
    "    low_accuracy_threshold = 50\n",
    "    if min_count < low_accuracy_threshold:\n",
    "        top_given_names = given_names[given_names['count'] >= low_accuracy_threshold]\n",
    "        top_given_names = top_given_names.copy()\n",
    "        low_accuracy_names = given_names[(given_names['count'] >= min_count) & (given_names['count'] < low_accuracy_threshold)]\n",
    "        low_accuracy_names = low_accuracy_names.copy()\n",
    "    else:\n",
    "        top_given_names = given_names[given_names['count'] >= min_count]\n",
    "        top_given_names = top_given_names.copy()\n",
    "        low_accuracy_names = pd.DataFrame(columns=['eng', 'arb', 'count', 'trimmed_eng', 'trimmed_arb', 'eng_variants', 'arb_variants'])\n",
    "    \n",
    "    all_eng_given_names = list(set(top_given_names['eng'].tolist()))\n",
    "    all_arb_given_names = list(set(top_given_names['arb'].tolist()))\n",
    "    random.shuffle(all_eng_given_names)\n",
    "    random.shuffle(all_arb_given_names)\n",
    "\n",
    "def group_names_both():\n",
    "    global top_given_names\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng'].transform(format_variants_list)\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb'].transform(format_variants_list)\n",
    "    \n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_arb')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng_variants'].transform(format_variants_list2)\n",
    "\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "    \n",
    "    if not low_accuracy_names.empty:\n",
    "        low_accuracy_names['eng_variants'] = low_accuracy_names.groupby('trimmed_arb')['eng'].transform(format_variants_list)\n",
    "        low_accuracy_names['arb_variants'] = low_accuracy_names.groupby('trimmed_eng')['arb'].transform(format_variants_list)\n",
    "        top_given_names = pd.concat([top_given_names, low_accuracy_names])\n",
    "\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_arb')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng_variants'].transform(format_variants_list2)\n",
    "    \n",
    "    '''\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_arb')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng_variants'].transform(format_variants_list2)\n",
    "\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "    top_given_names['arb_variants'] = top_given_names.groupby('trimmed_arb')['arb_variants'].transform(format_variants_list2)\n",
    "    top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng_variants'].transform(format_variants_list2)\n",
    "    '''\n",
    "\n",
    "def clean_up_output_files():\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    ## clearing files content\n",
    "    open(target_dir + 'eng_eng_pairs.tsv', 'w').close()\n",
    "    open(target_dir + 'eng_arb_pairs.tsv', 'w').close()\n",
    "    open(target_dir + 'arb_arb_pairs.tsv', 'w').close()\n",
    "    open(target_dir + 'names_dict.pkl', 'w').close()\n",
    "    open(target_dir + 'all_eng_names.pkl', 'w').close()\n",
    "    open(target_dir + 'all_arb_names.pkl', 'w').close()\n",
    "\n",
    "def load_up_data_from_files():\n",
    "    global eng_eng_df, eng_arb_df, arb_arb_df, eng_arb_negative_df, eng_eng_nagative_df, arb_arb_negative_df, names_dict, all_eng_names, all_arb_names\n",
    "    \n",
    "    eng_eng_df = pd.read_csv(target_dir + 'eng_eng_pairs.tsv',sep='\\t', header=None)\n",
    "    eng_arb_df = pd.read_csv(target_dir + 'eng_arb_pairs.tsv',sep='\\t', header=None)\n",
    "    arb_arb_df = pd.read_csv(target_dir + 'arb_arb_pairs.tsv',sep='\\t', header=None)\n",
    "    #eng_arb_negative_df = pd.read_csv(target_dir + 'neg_eng_arb_pairs.tsv',sep='\\t', header=None)\n",
    "    #eng_eng_nagative_df = pd.read_csv(target_dir + 'neg_eng_eng_pairs.tsv',sep='\\t', header=None)\n",
    "    #arb_arb_negative_df = pd.read_csv(target_dir + 'neg_arb_arb_pairs.tsv',sep='\\t', header=None)\n",
    "    \n",
    "    names_dict = load_obj('names_dict')\n",
    "    all_eng_names = load_obj('all_eng_names')\n",
    "    all_arb_names = load_obj('all_arb_names')\n",
    "    \n",
    "def append_list_to_tsv(file_name, list_to_be_added, folder = ''):\n",
    "    folder = target_dir + folder + \"/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    df = pd.DataFrame(list_to_be_added)\n",
    "    df.to_csv(folder + file_name + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'a', header=False)    \n",
    "    \n",
    "def write_data_to_output_files():\n",
    "    i = 0\n",
    "    names_dict = {}\n",
    "    all_eng_names = []\n",
    "    all_arb_names = []\n",
    "    set_len = len(top_given_names)\n",
    "    eng_eng_count = 0\n",
    "    arb_arb_count = 0\n",
    "    eng_arb_count = 0\n",
    "    \n",
    "    for index, row in top_given_names.iterrows():\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "                print(\"processed {i} out of {t}\".format(i=i, t=set_len))\n",
    "\n",
    "        eng_variants = row['eng_variants'].split(',')\n",
    "        arb_variants = row['arb_variants'].split(',')\n",
    "        #negative_eng_variants = row['negative_eng_variants'].split(',')\n",
    "        #negative_arb_variants = row['negative_arb_variants'].split(',')\n",
    "        \n",
    "        all_eng_names.extend([[elem, i] for elem in eng_variants])\n",
    "        all_arb_names.extend([[elem, i] for elem in arb_variants])\n",
    "        names_dict[i] = {'eng': eng_variants, 'arb': arb_variants}\n",
    "        \n",
    "        eng_variants_length = len(eng_variants)\n",
    "        arb_variants_length = len(arb_variants)\n",
    "        #negative_eng_variants_length = len(negative_eng_variants)\n",
    "        #negative_arb_variants_length = len(negative_arb_variants)\n",
    "        \n",
    "        maximum_eng_variants_threshold = 50\n",
    "        if(eng_variants_length > maximum_eng_variants_threshold):\n",
    "            eng_eng_pairs = []\n",
    "            percent = maximum_eng_variants_threshold**2 / eng_variants_length**2\n",
    "            randomly_selected_count = math.ceil(eng_variants_length * percent)\n",
    "            for eng in eng_variants:\n",
    "                temp = random.sample(eng_variants, randomly_selected_count)\n",
    "                temp.append(eng)\n",
    "                eng_eng = list(product([eng], temp))\n",
    "                eng_eng = [list(elem) + [str(i)] for elem in eng_eng]\n",
    "                eng_eng_pairs.extend(eng_eng)\n",
    "        else: \n",
    "            eng_eng_pairs = list(product(eng_variants, repeat=2))\n",
    "            eng_eng_pairs = [list(elem) + [str(i)] for elem in eng_eng_pairs]\n",
    "        \n",
    "        eng_eng_count += len(eng_eng_pairs)\n",
    "        append_list_to_tsv('eng_eng_pairs', eng_eng_pairs)\n",
    "\n",
    "        arb_arb_pairs = list(product(arb_variants, repeat=2))\n",
    "        arb_arb_pairs = [list(elem) + [str(i)] for elem in arb_arb_pairs]\n",
    "        arb_arb_count += len(arb_arb_pairs)\n",
    "        append_list_to_tsv('arb_arb_pairs', arb_arb_pairs)\n",
    "\n",
    "        eng_arb_pairs = set(list(product(eng_variants, arb_variants)))\n",
    "        eng_arb_pairs = [list(elem) + [str(i)] for elem in eng_arb_pairs]\n",
    "        eng_arb_count += len(eng_arb_pairs)\n",
    "        \n",
    "        '''\n",
    "        if len(eng_arb_pairs) < len(eng_eng_pairs):\n",
    "            quotient, modulo = divmod(len(eng_eng_pairs), len(eng_arb_pairs))\n",
    "            extension = random.sample(eng_arb_pairs, modulo)\n",
    "            eng_arb_pairs = [repeated for value in eng_arb_pairs for repeated in repeat(value, quotient)]\n",
    "            eng_arb_pairs.extend(extension)\n",
    "        '''\n",
    "        append_list_to_tsv('eng_arb_pairs', eng_arb_pairs)\n",
    "        \n",
    "        '''\n",
    "        needed_negatives_length = math.ceil(eng_variants_length * (eng_variants_length - 1) * 0.75)\n",
    "\n",
    "        if needed_negatives_length > negative_eng_variants_length:\n",
    "            quotient, modulo = divmod(needed_negatives_length, negative_eng_variants_length)\n",
    "            extension = random.sample(negative_eng_variants, modulo)\n",
    "            negative_eng_variants = [repeated for value in negative_eng_variants for repeated in repeat(value, quotient)]\n",
    "            negative_eng_variants.extend(extension)\n",
    "\n",
    "        if needed_negatives_length > negative_arb_variants_length:\n",
    "            quotient, modulo = divmod(needed_negatives_length, negative_arb_variants_length)\n",
    "            extension = random.sample(negative_arb_variants, modulo)\n",
    "            negative_arb_variants = [repeated for value in negative_arb_variants for repeated in repeat(value, quotient)]\n",
    "            negative_arb_variants.extend(extension)\n",
    "\n",
    "        needed_negatives_length = math.ceil(eng_variants_length * (eng_variants_length - 1) * 0.5)\n",
    "\n",
    "        random.shuffle(negative_eng_variants)\n",
    "        neg_eng_eng_pairs = list(zip(negative_eng_variants[:needed_negatives_length], cycle(eng_variants)))\n",
    "        neg_eng_eng_pairs = [list(elem) + [str(i)] for elem in neg_eng_eng_pairs]\n",
    "        append_list_to_tsv('neg_eng_eng_pairs', neg_eng_eng_pairs)\n",
    "\n",
    "        neg_arb_arb_pairs = list(zip(negative_arb_variants[:needed_negatives_length], cycle(arb_variants)))\n",
    "        neg_arb_arb_pairs = [list(elem) + [str(i)] for elem in neg_arb_arb_pairs]\n",
    "        append_list_to_tsv('neg_arb_arb_pairs', neg_arb_arb_pairs)\n",
    "\n",
    "        neg_eng_arb_pairs = list(zip(negative_eng_variants[needed_negatives_length:], cycle(arb_variants)))\n",
    "        neg_eng_arb_pairs = [list(elem) + [str(i)] for elem in neg_eng_arb_pairs]\n",
    "        append_list_to_tsv('neg_eng_arb_pairs', neg_eng_arb_pairs)\n",
    "\n",
    "        neg_eng_arb_pairs = list(zip(cycle(eng_variants), negative_arb_variants[needed_negatives_length:]))\n",
    "        neg_eng_arb_pairs = [list(elem) + [str(i)] for elem in neg_eng_arb_pairs]\n",
    "        append_list_to_tsv('neg_eng_arb_pairs', neg_eng_arb_pairs)\n",
    "        '''\n",
    "        \n",
    "    save_obj(names_dict, 'names_dict')\n",
    "    save_obj(all_eng_names, 'all_eng_names')\n",
    "    save_obj(all_arb_names, 'all_arb_names')\n",
    "    print('eng_eng count = {e_e},arb_arb count = {a_a}, eng_arb count = {e_a}'.format(e_e=eng_eng_count, a_a=arb_arb_count, e_a=eng_arb_count))\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open(target_dir + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(target_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the PostgreSQL database...\n",
      "159005\n",
      "94389\n",
      "70375\n",
      "CPU times: user 3.64 s, sys: 94.9 ms, total: 3.73 s\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_top_given_names(20)\n",
    "print(len(top_given_names))\n",
    "print(len(set(all_eng_given_names)))\n",
    "print(len(set(all_arb_given_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 0\n",
    "top_given_names = pd.read_csv(target_dir + 'all_names_with_yamli.tsv', sep='\\t', header=None)\n",
    "top_given_names.columns=['eng', 'arb', 'count', 'trimmed_eng', 'trimmed_arb', 'yamli_arb_variants']\n",
    "names = list(set((top_given_names[top_given_names.isnull().any(axis=1)]['trimmed_eng']).tolist()))\n",
    "set_len = len(names)\n",
    "print(set_len)\n",
    "for name in names:\n",
    "    arb_variants = get_yamli_arabic_varinats(name)\n",
    "    top_given_names.loc[top_given_names['trimmed_eng'] == name, 'yamli_arb_variants'] = ','.join(arb_variants)\n",
    "    i += 1\n",
    "    if i % 3000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=set_len))\n",
    "        top_given_names.to_csv(target_dir + 'all_names_with_yamli.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "        \n",
    "top_given_names.to_csv(target_dir + 'all_names_with_yamli.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb'].transform(format_variants_list)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['arb_variants', 'yamli_arb_variants', 'eng'], keep='first').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 0\n",
    "top_given_names = pd.read_csv(target_dir + 'all_names_with_yamli.tsv', sep='\\t', header=None)\n",
    "top_given_names.columns=['eng', 'arb', 'count', 'trimmed_eng', 'trimmed_arb', 'yamli_arb_variants', 'arb_variants', 'eng_variants']\n",
    "names = list(set((top_given_names['trimmed_arb']).tolist()))\n",
    "set_len = len(names)\n",
    "print(set_len)\n",
    "for name in names:\n",
    "    matched_rows = top_given_names[top_given_names['trimmed_arb'] == name]\n",
    "    eng_variants = (matched_rows['eng']).tolist()\n",
    "    arb_variants = (matched_rows['arb_variants']).tolist()\n",
    "    arb_variants = list(set([name for sublist in arb_variants for name in sublist.split(',')]))\n",
    "    yamli_variants = (matched_rows['yamli_arb_variants']).tolist()\n",
    "    yamli_variants = list(set([name for sublist in yamli_variants for name in sublist.split(',')]))\n",
    "\n",
    "    top_given_names.loc[top_given_names['trimmed_arb'] == name, 'eng_variants'] = ','.join(eng_variants)\n",
    "    top_given_names.loc[top_given_names['trimmed_arb'] == name, 'arb_variants'] = ','.join(arb_variants)\n",
    "    top_given_names.loc[top_given_names['trimmed_arb'] == name, 'yamli_arb_variants'] = ','.join(yamli_variants)\n",
    "    i += 1\n",
    "    if i % 3000 == 0:\n",
    "        print(\"processed {i} out of {t}\".format(i=i, t=set_len))\n",
    "        top_given_names.to_csv(target_dir + 'all_names_with_yamli.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "        \n",
    "top_given_names.to_csv(target_dir + 'all_names_with_yamli.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['arb_variants', 'yamli_arb_variants', 'eng_variants'], keep='first').copy()\n",
    "top_given_names.to_csv(target_dir + 'all_names_with_yamli.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_given_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "top_given_names = pd.read_csv(target_dir + 'all_names_with_yamli.tsv', sep='\\t', header=None)\n",
    "top_given_names.columns=['eng', 'arb', 'count', 'trimmed_eng', 'trimmed_arb', 'yamli_arb_variants', 'arb_variants', 'eng_variants']\n",
    "\n",
    "temp_df = top_given_names.reset_index().copy()\n",
    "all_eng_variants = temp_df[['index', 'eng_variants']]\n",
    "all_eng_variants = [tuple([index, eng.split(',')]) for index, eng in all_eng_variants.values]\n",
    "all_arb_variants = temp_df[['index', 'arb_variants']]\n",
    "all_arb_variants = [tuple([index, arb.split(',')]) for index, arb in all_arb_variants.values]\n",
    "all_yamli_variants = temp_df[['index', 'yamli_arb_variants']]\n",
    "all_yamli_variants = [tuple([index, yamli.split(',')]) for index, yamli in all_yamli_variants.values]\n",
    "indexes = (temp_df['index']).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 813 ms, sys: 36.4 ms, total: 850 ms\n",
      "Wall time: 849 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_given_names = pd.read_csv(target_dir + 'all_names_with_yamli_merged.tsv', sep='\\t', header=None)\n",
    "top_given_names.columns=['eng', 'arb', 'count', 'trimmed_eng', 'trimmed_arb', 'yamli_arb_variants', 'arb_variants', 'eng_variants']\n",
    "\n",
    "temp_df = top_given_names.reset_index().copy()\n",
    "all_eng_variants = temp_df[['index', 'eng_variants']]\n",
    "all_eng_variants = [tuple([index, eng.split(',')]) for index, eng in all_eng_variants.values if index > 10000]\n",
    "all_arb_variants = temp_df[['index', 'arb_variants']]\n",
    "all_arb_variants = [tuple([index, arb.split(',')]) for index, arb in all_arb_variants.values if index > 10000]\n",
    "all_yamli_variants = temp_df[['index', 'yamli_arb_variants']]\n",
    "all_yamli_variants = [tuple([index, yamli.split(',')]) for index, yamli in all_yamli_variants.values if index > 10000]\n",
    "indexes = [i for i in (temp_df['index']).tolist() if i > 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485960.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_given_names.loc[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 11000, remaining:55326, eng_variants_length:55326\n",
      "processed 12000, remaining:54325, eng_variants_length:54325\n",
      "processed 13000, remaining:53325, eng_variants_length:53325\n",
      "processed 14000, remaining:52325, eng_variants_length:52325\n",
      "processed 15000, remaining:51324, eng_variants_length:51324\n",
      "processed 16000, remaining:50324, eng_variants_length:50324\n",
      "processed 17000, remaining:49323, eng_variants_length:49323\n",
      "processed 18000, remaining:48323, eng_variants_length:48323\n",
      "processed 19000, remaining:47323, eng_variants_length:47323\n",
      "processed 20000, remaining:46323, eng_variants_length:46323\n",
      "processed 21000, remaining:45323, eng_variants_length:45323\n",
      "processed 22000, remaining:44323, eng_variants_length:44323\n",
      "processed 23000, remaining:43323, eng_variants_length:43323\n",
      "processed 24000, remaining:42323, eng_variants_length:42323\n",
      "processed 25000, remaining:41323, eng_variants_length:41323\n",
      "processed 26000, remaining:40323, eng_variants_length:40323\n",
      "processed 27000, remaining:39323, eng_variants_length:39323\n",
      "processed 28000, remaining:38323, eng_variants_length:38323\n",
      "processed 29000, remaining:37323, eng_variants_length:37323\n",
      "processed 30000, remaining:36323, eng_variants_length:36323\n",
      "processed 31000, remaining:35323, eng_variants_length:35323\n",
      "processed 32000, remaining:34323, eng_variants_length:34323\n",
      "processed 33000, remaining:33323, eng_variants_length:33323\n",
      "processed 34000, remaining:32323, eng_variants_length:32323\n",
      "processed 35000, remaining:31323, eng_variants_length:31323\n",
      "processed 36000, remaining:30323, eng_variants_length:30323\n",
      "processed 37000, remaining:29323, eng_variants_length:29323\n",
      "processed 38000, remaining:28323, eng_variants_length:28323\n",
      "processed 39000, remaining:27323, eng_variants_length:27323\n",
      "processed 40000, remaining:26323, eng_variants_length:26323\n",
      "processed 41000, remaining:25323, eng_variants_length:25323\n",
      "processed 42000, remaining:24323, eng_variants_length:24323\n",
      "processed 43000, remaining:23323, eng_variants_length:23323\n",
      "processed 44000, remaining:22323, eng_variants_length:22323\n",
      "processed 45000, remaining:21323, eng_variants_length:21323\n",
      "processed 46000, remaining:20323, eng_variants_length:20323\n",
      "processed 47000, remaining:19323, eng_variants_length:19323\n",
      "processed 48000, remaining:18323, eng_variants_length:18323\n",
      "processed 49000, remaining:17323, eng_variants_length:17323\n",
      "processed 50000, remaining:16323, eng_variants_length:16323\n",
      "processed 51000, remaining:15323, eng_variants_length:15323\n",
      "processed 52000, remaining:14323, eng_variants_length:14323\n",
      "processed 53000, remaining:13323, eng_variants_length:13323\n",
      "processed 54000, remaining:12323, eng_variants_length:12323\n",
      "processed 55000, remaining:11323, eng_variants_length:11323\n",
      "processed 56000, remaining:10323, eng_variants_length:10323\n",
      "processed 57000, remaining:9323, eng_variants_length:9323\n",
      "processed 58000, remaining:8323, eng_variants_length:8323\n",
      "processed 59000, remaining:7323, eng_variants_length:7323\n",
      "processed 60000, remaining:6323, eng_variants_length:6323\n",
      "processed 61000, remaining:5323, eng_variants_length:5323\n",
      "processed 62000, remaining:4323, eng_variants_length:4323\n",
      "processed 63000, remaining:3323, eng_variants_length:3323\n",
      "processed 64000, remaining:2323, eng_variants_length:2323\n",
      "processed 65000, remaining:1323, eng_variants_length:1323\n",
      "processed 66000, remaining:323, eng_variants_length:323\n",
      "CPU times: user 2h 56min 33s, sys: 2.7 s, total: 2h 56min 36s\n",
      "Wall time: 2h 56min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i= 10001\n",
    "while len(indexes) > 0:\n",
    "    current_index = indexes[0]\n",
    "    row = top_given_names.loc[current_index]\n",
    "    eng_variants = row['eng_variants'].split(',')\n",
    "    arb_variants = row['arb_variants'].split(',')\n",
    "    yamli_variants = row['yamli_arb_variants'].split(',')\n",
    "    matched_eng = [(i, eng) for i, eng in all_eng_variants if any(s in eng for s in eng_variants)]\n",
    "    matched_arb = [(i, arb) for i, arb in all_arb_variants if any(s in arb for s in arb_variants)]\n",
    "    matched_yamli = [(i, yamli) for i, yamli in all_yamli_variants if any(s in yamli for s in yamli_variants)]\n",
    "    \n",
    "    matched_eng_indexes = [i for i, eng in matched_eng]\n",
    "    matched_arb_indexes = [i for i, arb in matched_arb]\n",
    "    matched_yamli_indexes = [i for i, yamli in matched_yamli]\n",
    "    \n",
    "    matched1 = set(matched_eng_indexes).intersection(matched_arb_indexes)\n",
    "    matched2 = set(matched_eng_indexes).intersection(matched_yamli_indexes)\n",
    "    matched3 = set(matched_arb_indexes).intersection(matched_yamli_indexes)\n",
    "    matched = set(matched1).union(matched2).union(matched3)        \n",
    "    \n",
    "    all_eng_variants = [(i, eng) for i, eng in all_eng_variants if i not in matched]\n",
    "    all_arb_variants = [(i, eng) for i, eng in all_arb_variants if i not in matched]\n",
    "    all_yamli_variants = [(i, eng) for i, eng in all_yamli_variants if i not in matched]\n",
    "    \n",
    "    if current_index not in matched:\n",
    "        print(eng_variants)\n",
    "        print(matched_eng)\n",
    "        print(current_index)\n",
    "        print(matched)\n",
    "        print(top_given_names.loc[current_index])\n",
    "        print(top_given_names.iloc[current_index])\n",
    "    \n",
    "    matched_rows = top_given_names.loc[matched,:]\n",
    "    matched_eng_variants = matched_rows['eng_variants'].tolist()\n",
    "    matched_eng_variants = list(set([eng for sublist in matched_eng_variants for eng in sublist.split(',')]))\n",
    "    matched_arb_variants =  matched_rows['arb_variants'].tolist()\n",
    "    matched_arb_variants = list(set([arb for sublist in matched_arb_variants for arb in sublist.split(',')]))\n",
    "    matched_yamli_variants = matched_rows['yamli_arb_variants'].tolist()\n",
    "    matched_yamli_variants = list(set([name for sublist in matched_yamli_variants for name in sublist.split(',')]))\n",
    "    \n",
    "    top_given_names.loc[current_index, 'eng_variants'] = ','.join(matched_eng_variants)\n",
    "    top_given_names.loc[current_index, 'arb_variants'] = ','.join(matched_arb_variants)\n",
    "    top_given_names.loc[current_index, 'yamli_arb_variants'] = ','.join(matched_yamli_variants)\n",
    "    \n",
    "    for e in matched:\n",
    "        indexes.remove(e)\n",
    "        if e != current_index:\n",
    "            top_given_names.drop(e, inplace=True)\n",
    "    \n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(\"processed {i}, remaining:{o}, eng_variants_length:{l}\".format(i=i, o=len(indexes),l=len(all_eng_variants)))\n",
    "        #top_given_names.to_csv(target_dir + 'all_names_with_yamli_merged.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "        \n",
    "top_given_names.to_csv(target_dir + 'all_names_with_yamli_merged.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 12.1 ms, total: 4.84 s\n",
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "for index, row in top_given_names.iterrows():\n",
    "    lll = row[6].split(',')\n",
    "    hhhh  = row[7].split(',')\n",
    "    if len(lll) > 1:\n",
    "        i+=1\n",
    "        #print( lll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_given_names = pd.read_csv(target_dir + 'all_names_with_yamli_merged.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'محاميت,محاميد,مشامض,موحد,مشامت,موحمض,مخامات,موخمت,موهاميد,مهموض,مإهمد,محمود,محماد,مهامض,موهامض,مهميت,معحامض,مخامت,محض,موخمات,مهميط,ماهمت,معحماد,محامود,موهامود,مخيميط,معهامد,موهيض,موخماد,مهميد,معحماض,موخامد,مهمض,ماحمد,مووهامد,مخامض,مخميد,مخامط,موهميت,مقحمد,مهامات,مووهمد,موحامد,محيميد,محمة,موهمود,مشامة,مخيمة,مواهمد,موشماض,مهماد,مهمات,محميد,موحميت,مشامط,مخمة,مهمة,موهامط,معهامض,محامية,مهمت,مشمض,مواهامد,موشامت,موكهمد,محاماة,موهمت,موحيد,مووحمد,مهمد,موشمض,معحمض,محمية,مشماض,معهماد,موشامد,موشمات,معحامد,معهمض,مهمود,محمط,مهميض,موخامت,معهمد,مشيمد,مخيمات,مهاميد,موهاميت,موخمض,مشمة,موخامض,موهاد,موهميد,محيمد,مئهمد,ماهمد,محموض,مشامد,ميحمد,موحامت,موخمة,محماة,موشمط,موهمط,موهماد,محماض,محموود,موخامات,معهيمد,موحاد,مهض,مخاماد,مخماد,موهد,مشماد,موهمض,مخامة,محد,موهامد,مخمد,موشامات,محامة,معهماض,مكهمد,مهامة,موحامض,موحماد,مخمض,موهمد,ميهمد,مشاماد,موحمط,موخمد,مشمط,موخمط,موشماد,مشميد,مشمت,موحض,موهميض,مشمات,موهاماد,محميض,موشامض,معحمد,موشمت,موحمة,مهماض,مهامد,مخيمت,محميت,محمد,مشمد,موهض,موشميد,موشمد,مهاماد,موحمود,مخمت,محامد,موهامت,مخمات,مخمط,موهامة,مووهد,موهيد,موهمة,مواحمد,معهاماد,مهاميت,مخامد,موحمد,موحميد,محمت,محمض,مهامود,مشامات,موحمت,موشمة,مهد'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_given_names.loc[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in matched:\n",
    "    row = top_given_names.iloc[index]\n",
    "    print(row['eng_variants'])\n",
    "    print(row['arb_variants'])\n",
    "    print(row['yamli_arb_variants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matched_yamli_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matched_eng_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_gi    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_given_names_without_yamli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_given_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng'].transform(format_variants_list)\n",
    "top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb'].transform(format_variants_list)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "top_given_names['arb_variants'] = top_given_names.groupby('trimmed_eng')['arb_variants'].transform(format_variants_list2)\n",
    "top_given_names['eng_variants'] = top_given_names.groupby('trimmed_eng')['eng_variants'].transform(format_variants_list2)\n",
    "top_given_names['arb_variants'] = top_given_names.groupby('trimmed_arb')['arb_variants'].transform(format_variants_list2)\n",
    "top_given_names['eng_variants'] = top_given_names.groupby('trimmed_arb')['eng_variants'].transform(format_variants_list2)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## round 1\n",
    "\n",
    "group_names_both()\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "top_given_names['eng_variants'] = top_given_names.groupby('arb_variants')['eng_variants'].transform(format_variants_list2)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "top_given_names['arb_variants'] = top_given_names.groupby('eng_variants')['arb_variants'].transform(format_variants_list2)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "## round 2\n",
    "#top_given_names = top_given_names.apply(remove_long_variants, axis=1)\n",
    "#top_given_names = top_given_names.apply(create_negative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_given_names.iloc[5]['eng_variants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_up_output_files()\n",
    "write_data_to_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_given_names = pd.read_csv(target_dir + 'all_names_with_yamli.tsv', sep='\\t', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "load_up_data_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>SOHAIL</td>\n",
       "      <td>سهيل</td>\n",
       "      <td>195943.0</td>\n",
       "      <td>SOHAIL</td>\n",
       "      <td>سهيل</td>\n",
       "      <td>صهيل,سحيل,سهيل,سهايل,ثوهيل,سوحيل,سحايل,صهايل,ث...</td>\n",
       "      <td>سهيل</td>\n",
       "      <td>SOHAIL,SUHAIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1         2       3     4  \\\n",
       "82  SOHAIL  سهيل  195943.0  SOHAIL  سهيل   \n",
       "\n",
       "                                                    5     6              7  \n",
       "82  صهيل,سحيل,سهيل,سهايل,ثوهيل,سوحيل,سحايل,صهايل,ث...  سهيل  SOHAIL,SUHAIL  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_given_names[top_given_names[0] == 'SOHAIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    if len(df.columns) == 4:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df = df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def prepare_duplicate_df(df, desired_length):\n",
    "    print(len(df))\n",
    "    if len(df.columns) == 4:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df = df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "    \n",
    "    quotient, modulo = divmod(desired_length, len(df))\n",
    "    \n",
    "    df = pd.concat([df]*quotient, ignore_index=True)\n",
    "    \n",
    "    if modulo > 0:\n",
    "        fract = modulo / len(df)\n",
    "        df = pd.concat([df, df.sample(frac=fract)])\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    print(len(df))\n",
    "    return df\n",
    "\n",
    "def concatenate_ordered_names(row):\n",
    "    return ' '.join(row.tolist())\n",
    "\n",
    "def concatenate_reversed_names(row):\n",
    "    lst = row.tolist()\n",
    "    lst = lst[::-1]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_out_of_order_names(row):\n",
    "    lst = row.tolist()\n",
    "    if len(set(lst)) < 3:\n",
    "        return ''\n",
    "    rev = lst[:]\n",
    "    rev = rev[::-1]\n",
    "    shuff = lst[:]\n",
    "    random.shuffle(shuff)\n",
    "    while shuff == rev or shuff == lst:\n",
    "        random.shuffle(shuff)\n",
    "    return ' '.join(shuff)\n",
    "\n",
    "def concatenate_with_initial(row):\n",
    "    global current_processed_pair, selected_rnd_names_for_initial, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    lct = random.randrange(len(lst)+1)\n",
    "    rnd_initial = ''\n",
    "    if current_languages_pairs == 'eng_eng':\n",
    "        rnd_name = random.choice(all_eng_names)[0]\n",
    "        selected_rnd_names_for_initial.put((rnd_name, lct))\n",
    "        rnd_initial = rnd_name[0] + random.choice(['', ' ', '.', '. '])\n",
    "    elif current_languages_pairs == 'arb_arb':\n",
    "        rnd_name = random.choice(all_arb_names)[0]\n",
    "        selected_rnd_names_for_initial.put((rnd_name, lct))\n",
    "        rnd_initial = rnd_name[0] + random.choice(['', ' ', '.', '. '])\n",
    "    elif current_languages_pairs == 'eng_arb':\n",
    "        row_index_for_eng_arb += 1\n",
    "        eng_arb_factor = row_index_for_eng_arb % 4\n",
    "        if eng_arb_factor == 0 or eng_arb_factor == 1:\n",
    "            temp = random.choice(all_eng_names)\n",
    "            first_pair = temp[0]\n",
    "            group = temp[1]\n",
    "            #second_pair = random.choice([item for item in all_arb_names if item[1] == group])[0]\n",
    "            second_pair = random.choice(names_dict[group]['arb'])\n",
    "        else:\n",
    "            temp = random.choice(all_arb_names)\n",
    "            second_pair = temp[0]\n",
    "            group = temp[1]\n",
    "            #first_pair = random.choice([item for item in all_eng_names if item[1] == group])[0]\n",
    "            first_pair = random.choice(names_dict[group]['eng'])\n",
    "            \n",
    "        if eng_arb_factor == 0 or eng_arb_factor == 2:\n",
    "            selected_rnd_names_for_initial.put((second_pair, lct))\n",
    "            rnd_initial = first_pair[0] + random.choice(['', ' ', '.', '. '])\n",
    "        else:\n",
    "            selected_rnd_names_for_initial.put((second_pair[0] + random.choice(['', ' ', '.', '. ']), lct))\n",
    "            rnd_initial = first_pair\n",
    "        \n",
    "    lst.insert(lct, rnd_initial)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_with_initial_name(row):\n",
    "    global selected_rnd_names_for_initial\n",
    "    lst = row.tolist()\n",
    "    name, lct = selected_rnd_names_for_initial.get()\n",
    "    lst.insert(lct, name)\n",
    "    \n",
    "    if random.choice(range(1, 5)) % 4 == 0:\n",
    "        lst = lst[::-1]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_name(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    if len(lst) == 1:\n",
    "        lct = random.randrange(0, 2)\n",
    "    else:\n",
    "        lct = random.randrange(1, len(lst)+1)\n",
    "    rnd_name = ''\n",
    "    if current_languages_pairs == 'eng_eng':\n",
    "        rnd_name = random.choice(all_eng_names)[0]\n",
    "    elif current_languages_pairs == 'arb_arb':\n",
    "        rnd_name = random.choice(all_arb_names)[0]\n",
    "    elif current_languages_pairs == 'eng_arb':\n",
    "        row_index_for_eng_arb += 1\n",
    "        if current_processed_pair == 'eng':\n",
    "            if (row_index_for_eng_arb % 2 == 1) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_eng_names)[0]\n",
    "        elif current_processed_pair == 'arb':\n",
    "            if (row_index_for_eng_arb % 2 == 0) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_arb_names)[0]\n",
    "\n",
    "    lst.insert(lct, rnd_name)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_name_in_middle(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    lct = random.randrange(1, len(lst))\n",
    "    rnd_name = ''\n",
    "    if current_languages_pairs == 'eng_eng':\n",
    "        rnd_name = random.choice(all_eng_names)[0]\n",
    "    elif current_languages_pairs == 'arb_arb':\n",
    "        rnd_name = random.choice(all_arb_names)[0]\n",
    "    elif current_languages_pairs == 'eng_arb':\n",
    "        row_index_for_eng_arb += 1\n",
    "        if current_processed_pair == 'eng':\n",
    "            if (row_index_for_eng_arb % 2 == 1) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_eng_names)[0]\n",
    "        elif current_processed_pair == 'arb':\n",
    "            if (row_index_for_eng_arb % 2 == 0) or ignore_row_index_for_eng_arb:\n",
    "                rnd_name = random.choice(all_arb_names)[0]\n",
    "\n",
    "    lst.insert(lct, rnd_name)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "\n",
    "def concatenate_append_random_two_names(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    row_index_for_eng_arb += 1\n",
    "    if len(lst) == 1:\n",
    "        lct = random.randrange(2)\n",
    "        if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "    else:\n",
    "        lct = random.randrange(1, len(lst)+1)        \n",
    "        if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "            lct = random.randrange(1, len(lst)+1)\n",
    "            lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "            lct = random.randrange(1, len(lst)+1)\n",
    "            lst.insert(lct, random.choice(all_arb_names)[0])                \n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_two_names_in_middle(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    row_index_for_eng_arb += 1\n",
    "    lct = random.randrange(1, len(lst))        \n",
    "    if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        lct = random.randrange(1, len(lst))\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "    elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "        lct = random.randrange(1, len(lst))\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])                \n",
    "    return ' '.join(lst)\n",
    "\n",
    "\n",
    "def concatenate_append_random_three_names(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = row.tolist()\n",
    "    lct = random.randrange(2)\n",
    "    if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "        lst.insert(lct, random.choice(all_eng_names)[0])\n",
    "    elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "        lst.insert(lct, random.choice(all_arb_names)[0])\n",
    "\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def concatenate_append_random_four_names(row):\n",
    "    global current_processed_pair, row_index_for_eng_arb, ignore_row_index_for_eng_arb\n",
    "    lst = []\n",
    "    if current_languages_pairs == 'eng_eng' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'eng' and ((row_index_for_eng_arb % 2 == 1 or ignore_row_index_for_eng_arb))):\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "        lst.append(random.choice(all_eng_names)[0])\n",
    "    elif current_languages_pairs == 'arb_arb' or (current_languages_pairs == 'eng_arb' and current_processed_pair == 'arb' and ((row_index_for_eng_arb % 2 == 0 or ignore_row_index_for_eng_arb))):\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "        lst.append(random.choice(all_arb_names)[0])\n",
    "\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def process_and_save_pairs(df, number_of_parts, name_of_output_file, part1_apply_function, part2_apply_function):\n",
    "    global row_index_for_eng_arb, current_processed_pair\n",
    "    folder = target_dir + folder_name + \"/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    number_of_pairs = len(df)\n",
    "    pairs_indexes = number_of_parts * list(range(0,number_of_pairs))\n",
    "    random.shuffle(pairs_indexes)\n",
    "    name_pairs = []\n",
    "    for row in df.itertuples():\n",
    "        index,part1,part2,group = row\n",
    "        if index % 100000 == 0:\n",
    "            print(\"batch {index}\".format(index=(index / 100000)))\n",
    "        for i in range(0, number_of_parts):\n",
    "            name_pairs.append([part1, part2, pairs_indexes.pop()])\n",
    "    \n",
    "    df = pd.DataFrame(name_pairs)\n",
    "    df.columns = ['pair1', 'pair2', 'group']\n",
    "    grp = df.groupby('group')\n",
    "    if current_languages_pairs == 'eng_arb':\n",
    "        current_processed_pair = 'eng'\n",
    "        row_index_for_eng_arb = 0\n",
    "    df['pair1'] = grp['pair1'].apply(part1_apply_function)\n",
    "    if current_languages_pairs == 'eng_arb':\n",
    "        current_processed_pair = 'arb'\n",
    "        row_index_for_eng_arb = 0\n",
    "    df['pair2'] = grp['pair2'].apply(part2_apply_function)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    print(len(df))\n",
    "    df.to_csv(folder + name_of_output_file + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.choice(all_eng_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from queue import Queue\n",
    "selected_rnd_names_for_initial = Queue()\n",
    "\n",
    "folder_name = 'eng_eng/'\n",
    "df = prepare_df(eng_eng_df)\n",
    "current_languages_pairs = 'eng_eng'\n",
    "\n",
    "process_and_save_pairs(df, 4, 'pos_4x4_ordered_pairs', concatenate_ordered_names, concatenate_ordered_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def process_all_language_pairs(dataset, language_pair, desired_length = 0):\n",
    "    global current_languages_pairs, folder_name, row_index_for_eng_arb, selected_rnd_names_for_initial, ignore_row_index_for_eng_arb\n",
    "    \n",
    "    row_index_for_eng_arb = 0\n",
    "    ignore_row_index_for_eng_arb = True\n",
    "    selected_rnd_names_for_initial = Queue()\n",
    "    current_languages_pairs = language_pair\n",
    "    \n",
    "    if desired_length == 0:\n",
    "        df = prepare_df(dataset)\n",
    "    else:\n",
    "        df = prepare_duplicate_df(dataset, desired_length)\n",
    "    \n",
    "    folder_name = current_languages_pairs + '/'\n",
    "    '''\n",
    "    #4x4\n",
    "    process_and_save_pairs(df, 4, 'pos_4x4_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 4, 'pos_4x4_reversed_pairs', concatenate_ordered_names, concatenate_reversed_names)\n",
    "    process_and_save_pairs(df, 4, 'neg_4x4_unordered_pairs', concatenate_ordered_names, concatenate_out_of_order_names)\n",
    "    process_and_save_pairs(df, 3, 'pos_initial_3x4_pairs', concatenate_with_initial, concatenate_with_initial_name)\n",
    "    process_and_save_pairs(df, 3, 'neg_3_1x3_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    process_and_save_pairs(df, 2, 'neg_2_2x2_2_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_3x1_3_pairs', concatenate_append_random_three_names, concatenate_append_random_three_names)\n",
    "\n",
    "    #3x4\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_2x1_3_pairs', concatenate_append_random_two_names, concatenate_append_random_three_names)\n",
    "    process_and_save_pairs(df, 3, 'pos_3x4_reversed_pairs', concatenate_reversed_names, concatenate_append_random_name_in_middle)\n",
    "    \n",
    "    #2x4\n",
    "    process_and_save_pairs(df, 2, 'pos_2x4_reversed_pairs', concatenate_reversed_names, concatenate_append_random_two_names_in_middle)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_3_pairs', concatenate_append_random_name, concatenate_append_random_three_names)\n",
    "    \n",
    "    #1x4\n",
    "    process_and_save_pairs(df, 1, 'neg_1x4_pairs', concatenate_ordered_names, concatenate_append_random_four_names)\n",
    "    \n",
    "    #3x3\n",
    "    process_and_save_pairs(df, 3, 'pos_3x3_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 3, 'pos_3x3_reversed_pairs', concatenate_ordered_names, concatenate_reversed_names)\n",
    "    '''\n",
    "    process_and_save_pairs(df, 3, 'neg_3x3_unordered_pairs', concatenate_ordered_names,  concatenate_out_of_order_names)\n",
    "    '''\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_2x1_2_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "    process_and_save_pairs(df, 2, 'pos_initial_2x3_pairs', concatenate_with_initial, concatenate_with_initial_name)\n",
    "\n",
    "    #2x3\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    \n",
    "    #2x2\n",
    "    process_and_save_pairs(df, 2, 'pos_2x2_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 2, 'pos_2x2_reversed_pairs', concatenate_ordered_names, concatenate_reversed_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    process_and_save_pairs(df, 1, 'pos_1x2_initails_pairs', concatenate_with_initial, concatenate_with_initial_name)\n",
    "    \n",
    "    #1x1\n",
    "    process_and_save_pairs(df, 1, 'pos_1x1_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "\n",
    "    ignore_row_index_for_eng_arb = False\n",
    "    if current_languages_pairs == 'eng_arb':        \n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x4\n",
    "        process_and_save_pairs(df, 1, 'pos_1x4_pairs', concatenate_append_random_three_names, concatenate_append_random_three_names)\n",
    "        \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "        \n",
    "        #1x3\n",
    "        process_and_save_pairs(df, 1, 'pos_1x3_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x2\n",
    "        process_and_save_pairs(df, 1, 'pos_1x2_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    else:\n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_ordered_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x4\n",
    "        process_and_save_pairs(df, 1, 'pos_1x4_pairs', concatenate_ordered_names, concatenate_append_random_three_names)\n",
    "        \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "        \n",
    "        #1x3\n",
    "        process_and_save_pairs(df, 1, 'pos_1x3_pairs', concatenate_ordered_names, concatenate_append_random_two_names)\n",
    "        \n",
    "        #1x2\n",
    "        process_and_save_pairs(df, 1, 'pos_1x2_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def process_all_language_pairs(dataset, language_pair, desired_length = 0):\n",
    "    global current_languages_pairs, folder_name, row_index_for_eng_arb, selected_rnd_names_for_initial, ignore_row_index_for_eng_arb\n",
    "    \n",
    "    row_index_for_eng_arb = 0\n",
    "    ignore_row_index_for_eng_arb = True\n",
    "    selected_rnd_names_for_initial = Queue()\n",
    "    current_languages_pairs = language_pair\n",
    "    \n",
    "    if desired_length == 0:\n",
    "        df = prepare_df(dataset)\n",
    "    else:\n",
    "        df = prepare_duplicate_df(dataset, desired_length)\n",
    "    \n",
    "    folder_name = current_languages_pairs + '/'\n",
    "    \n",
    "    #4x4\n",
    "    process_and_save_pairs(df, 4, 'pos_4x4_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 4, 'neg_4x4_unordered_pairs', concatenate_ordered_names, concatenate_out_of_order_names)\n",
    "    process_and_save_pairs(df, 3, 'neg_3_1x3_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "\n",
    "    #3x4\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    \n",
    "    #2x4\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_3_pairs', concatenate_append_random_name, concatenate_append_random_three_names)\n",
    "        \n",
    "    #3x3\n",
    "    process_and_save_pairs(df, 3, 'pos_3x3_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 3, 'neg_3x3_unordered_pairs', concatenate_ordered_names,  concatenate_out_of_order_names)\n",
    "    process_and_save_pairs(df, 2, 'neg_2_1x2_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "\n",
    "    #2x3\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_2_pairs', concatenate_append_random_name, concatenate_append_random_two_names)\n",
    "    \n",
    "    #2x2\n",
    "    process_and_save_pairs(df, 2, 'pos_2x2_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "    process_and_save_pairs(df, 1, 'neg_1_1x1_1_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    \n",
    "    #1x1\n",
    "    process_and_save_pairs(df, 1, 'pos_1x1_pairs', concatenate_ordered_names, concatenate_ordered_names)\n",
    "\n",
    "    ignore_row_index_for_eng_arb = False\n",
    "    if current_languages_pairs == 'eng_arb':        \n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_append_random_two_names, concatenate_append_random_two_names)\n",
    "                \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_append_random_name, concatenate_append_random_name)\n",
    "    else:\n",
    "        #3x4\n",
    "        process_and_save_pairs(df, 3, 'pos_3x4_pairs', concatenate_ordered_names, concatenate_append_random_name)\n",
    "        \n",
    "        #2x4\n",
    "        process_and_save_pairs(df, 2, 'pos_2x4_pairs', concatenate_ordered_names, concatenate_append_random_two_names)\n",
    "                \n",
    "        #2x3\n",
    "        process_and_save_pairs(df, 2, 'pos_2x3_pairs', concatenate_ordered_names, concatenate_append_random_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#process_all_language_pairs(eng_eng_df, 'eng_eng')\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "process_all_language_pairs(arb_arb_df, 'arb_arb', len(eng_eng_no_dup_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_all_language_pairs(eng_eng_df, 'eng_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "process_all_language_pairs(eng_arb_df, 'eng_arb', len(eng_eng_no_dup_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = target_dir + \"eng_eng/\"\n",
    "pos_3x4_initails_df = pd.read_csv(folder + 'neg_1x1_pairs.tsv',sep='\\t', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_3x4_initails_df[pos_3x4_initails_df[1] == 'MAHMOUD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_save_files(file_name, flag):\n",
    "    df = pd.read_csv(file_name + '.tsv',sep='\\t', header=None)\n",
    "    cols = df.columns.tolist()\n",
    "    if len(cols) == 4:\n",
    "        df.drop(df.columns[3], axis=1, inplace=True)\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "        df['similarity'] = flag\n",
    "        cols = df.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        df = df[cols]\n",
    "        df = df[df[1] != 'pair1']\n",
    "        df.to_csv(file_name + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "    elif len(cols) == 3:\n",
    "        df[0] = flag\n",
    "        df = df[df[1] != 'pair1']\n",
    "        df.to_csv(file_name + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_files(language_pair):\n",
    "    path = target_dir + language_pair + '/'\n",
    "'''\n",
    "    positive_files = [\n",
    "        'pos_4x4_pairs', 'pos_4x4_reversed_pairs', 'pos_initial_3x4_pairs',\n",
    "        'pos_3x4_pairs', 'pos_3x4_reversed_pairs',\n",
    "        'pos_2x4_pairs', 'pos_2x4_reversed_pairs',\n",
    "        'pos_1x4_pairs',\n",
    "        'pos_3x3_pairs', 'pos_3x3_reversed_pairs', 'pos_initial_2x3_pairs',\n",
    "        'pos_2x3_pairs',\n",
    "        'pos_1x3_pairs',\n",
    "        'pos_2x2_pairs', 'pos_2x2_reversed_pairs', 'pos_1x2_initails_pairs',\n",
    "        'pos_1x2_pairs',\n",
    "        'pos_1x1_pairs'\n",
    "    ]\n",
    "    \n",
    "    negative_files = [\n",
    "        'neg_4x4_pairs', 'neg_4x4_unordered_pairs', 'neg_3_1x3_1_pairs', 'neg_2_2x2_2_pairs', 'neg_1_3x1_3_pairs',\n",
    "        'neg_3x4_pairs', 'neg_2_1x2_2_pairs', 'neg_1_2x1_3_pairs',\n",
    "        'neg_2x4_pairs', 'neg_1_1x1_3_pairs',\n",
    "        'neg_1x4_pairs',\n",
    "        'neg_3x3_pairs', 'neg_3x3_unordered_pairs', 'neg_2_1x2_1_pairs', 'neg_1_2x1_2_pairs',\n",
    "        'neg_2x3_pairs', 'neg_1_1x1_2_pairs',\n",
    "        'neg_1x3_pairs',\n",
    "        'neg_2x2_pairs', 'neg_1_1x1_1_pairs',\n",
    "        'neg_1x2_pairs',\n",
    "        'neg_1x1_pairs'\n",
    "    ]\n",
    "'''  \n",
    "\n",
    "    positive_files = [\n",
    "        'pos_4x4_pairs', 'pos_3x4_pairs', 'pos_2x4_pairs', \n",
    "        'pos_3x3_pairs', 'pos_2x3_pairs',\n",
    "        'pos_2x2_pairs', \n",
    "        'pos_1x1_pairs'\n",
    "    ]\n",
    "    \n",
    "    negative_files = [\n",
    "        'neg_4x4_unordered_pairs', 'neg_3_1x3_1_pairs',\n",
    "        'neg_2_1x2_2_pairs',\n",
    "        'neg_1_1x1_3_pairs',\n",
    "        'neg_3x3_unordered_pairs', 'neg_2_1x2_1_pairs',\n",
    "        'neg_1_1x1_2_pairs',\n",
    "        'neg_1_1x1_1_pairs',\n",
    "        'neg_1x1_pairs'\n",
    "    ]\n",
    "    for positive_file in positive_files:\n",
    "        unify_save_files(path + positive_file, 1)\n",
    "    \n",
    "    for negative_file in negative_files:\n",
    "        unify_save_files(path + negative_file, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "unify_files('eng_eng')\n",
    "unify_files('eng_arb')\n",
    "unify_files('arb_arb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = list(names_dict.keys())\n",
    "\n",
    "def create_negative_datasets(iterable_list, pair1_lang, pair2_lang):\n",
    "    folder = target_dir + pair1_lang + '_' + pair2_lang + \"/\"\n",
    "    neg_4x4_pairs = [] #7\n",
    "    len_4x4_pair1 = 3\n",
    "    len_4x4_pair2 = 4\n",
    "\n",
    "    neg_3x4_pairs = [] #6\n",
    "    len_3x4_pair1 = 2\n",
    "    len_3x4_pair2 = 4\n",
    "\n",
    "    neg_2x4_pairs = [] #5\n",
    "    len_2x4_pair1 = 1\n",
    "    len_2x4_pair2 = 4\n",
    "\n",
    "    #neg_1x4_pairs = [] #4\n",
    "\n",
    "    neg_3x3_pairs = [] #5\n",
    "    len_3x3_pair1 = 2\n",
    "    len_3x3_pair2 = 3\n",
    "\n",
    "    neg_2x3_pairs = [] #4\n",
    "    len_2x3_pair1 = 1\n",
    "    len_2x3_pair2 = 3\n",
    "\n",
    "    neg_1x3_pairs = [] #3\n",
    "    len_1x3_pair1 = 0\n",
    "    len_1x3_pair2 = 3\n",
    "\n",
    "    neg_2x2_pairs = [] #3\n",
    "    len_2x2_pair1 = 1\n",
    "    len_2x2_pair2 = 2\n",
    "\n",
    "    neg_1x2_pairs = [] #2\n",
    "    len_1x2_pair1 = 0\n",
    "    len_1x2_pair2 = 2\n",
    "\n",
    "    neg_1x1_pairs = [] #1\n",
    "    len_1x1_pair1 = 0\n",
    "    len_1x1_pair2 = 1\n",
    "    \n",
    "    multiplier = int(len(eng_eng_no_dup_df) / len(iterable_list))\n",
    "    number_of_random_needed = multiplier * 36 + 1\n",
    "    \n",
    "    for name in iterable_list:\n",
    "        sel_keys = set(random.sample(keys, number_of_random_needed))\n",
    "        sel_keys = [key for key in sel_keys if key != name[1]]\n",
    "        pair1_keys = sel_keys[:multiplier * 10]\n",
    "        pair1_src = [random.sample(names_dict[item][pair1_lang], 1)[0] for item in pair1_keys]\n",
    "        pair2_keys = sel_keys[multiplier * 10:]\n",
    "        pair2_src = [random.sample(names_dict[item][pair2_lang], 1)[0] for item in pair2_keys]\n",
    "                \n",
    "        pair1_index = 0\n",
    "        pair2_index = 0\n",
    "\n",
    "        #4x4\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_4x4_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_4x4_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_4x4_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "    \n",
    "        #3x4\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_3x4_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_3x4_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_3x4_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "        \n",
    "        #2x4\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_2x4_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_2x4_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_2x4_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #3x3\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_3x3_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_3x3_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_3x3_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #2x3\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_2x3_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_2x3_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_2x3_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #1x3\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_1x3_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_1x3_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_1x3_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #2x2\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_2x2_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                random.shuffle(pair1)\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_2x2_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_2x2_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #1x2\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_1x2_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_1x2_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_1x2_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "\n",
    "        #1x1\n",
    "        for i in range(0, multiplier):\n",
    "            pair1 = [name[0]]\n",
    "            pair2 = []\n",
    "            for y in range(0, len_1x1_pair1):\n",
    "                pair1.append(pair1_src[pair1_index])\n",
    "                pair1_index += 1\n",
    "            for y in range(0, len_1x1_pair2):\n",
    "                pair2.append(pair2_src[pair2_index])\n",
    "                pair2_index += 1\n",
    "            neg_1x1_pairs.append([(' '.join(pair1)), (' '.join(pair2))])\n",
    "    \n",
    "    df = pd.DataFrame(neg_4x4_pairs)\n",
    "    df.to_csv(folder + 'neg_4x4_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_3x4_pairs)\n",
    "    df.to_csv(folder + 'neg_3x4_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_2x4_pairs)\n",
    "    df.to_csv(folder + 'neg_2x4_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False)     \n",
    "    df = pd.DataFrame(neg_3x3_pairs)\n",
    "    df.to_csv(folder + 'neg_3x3_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_2x3_pairs)\n",
    "    df.to_csv(folder + 'neg_2x3_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_1x3_pairs)\n",
    "    df.to_csv(folder + 'neg_1x3_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_2x2_pairs)\n",
    "    df.to_csv(folder + 'neg_2x2_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_1x2_pairs)\n",
    "    df.to_csv(folder + 'neg_1x2_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_1x1_pairs)\n",
    "    df.to_csv(folder + 'neg_1x1_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "create_single_negative_datasets()\n",
    "#create_single_negative_datasets(all_eng_names, 'eng', 'arb')\n",
    "#create_single_negative_datasets(all_arb_names, 'arb', 'arb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "keys = list(names_dict.keys())\n",
    "neg_eng_eng_pairs = []\n",
    "neg_eng_arb_pairs = []\n",
    "neg_arb_arb_pairs = []\n",
    "print(len(keys))\n",
    "\n",
    "def create_single_negative_datasets():\n",
    "    folder = target_dir + \"fuzzy/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    eng_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_eng_names))\n",
    "    arb_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_arb_names))\n",
    "    \n",
    "    i = 0\n",
    "    for key in keys:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"batch {index}\".format(index=(i / 1000)))\n",
    "        i += 1\n",
    "        \n",
    "        eng_names = names_dict[key]['eng']\n",
    "        #eng_names_len = len(eng_names)\n",
    "        arb_names = names_dict[key]['arb']\n",
    "        #arb_names_len = len(arb_names)\n",
    "        #similar_keys1 = set([name[1] for name in all_eng_names if any(s in name[0] for s in eng_names)])\n",
    "        #similar_keys2 = set([name[1] for name in all_arb_names if any(s in name[0] for s in arb_names)])\n",
    "        #similar_keys = similar_keys1.union(similar_keys2)\n",
    "        similar_keys = [key]\n",
    "        #other_eng_names = [name[0]for name in all_eng_names if name[1] not in similar_keys and 100 > fuzz.partial_ratio(eng_names[0], name[0]) > 49]\n",
    "        #other_arb_names = [name[0] for name in all_arb_names if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "\n",
    "        '''\n",
    "        eng_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_eng_names))\n",
    "        needed_eng_eng_negatives = eng_multiplier * eng_names_len\n",
    "        if len(other_eng_names) >= needed_eng_eng_negatives:\n",
    "            eng_eng_src_names = random.sample(other_eng_names, needed_eng_eng_negatives)\n",
    "        else:\n",
    "            print('problem generating eng_eng for key:{key}'.format(key=key))\n",
    "\n",
    "        needed_eng_arb_negatives = eng_multiplier * eng_names_len\n",
    "        if len(other_arb_names) >= needed_eng_arb_negatives:\n",
    "            eng_arb_src_names = random.sample(other_arb_names, needed_eng_arb_negatives)\n",
    "        else:\n",
    "            print('problem generating eng_arb for key:{key}'.format(key=key))\n",
    "\n",
    "        arb_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_arb_names))\n",
    "        needed_arb_arb_negatives = arb_multiplier * arb_names_len\n",
    "        if len(other_arb_names) >= needed_arb_arb_negatives:\n",
    "            arb_arb_src_names = random.sample(other_arb_names, needed_arb_arb_negatives)\n",
    "        else:\n",
    "            print('problem generating arb_arb for key:{key}'.format(key=key))\n",
    "        \n",
    "        #other_eng_names.sort(key = lambda x: x[1],reverse=True)\n",
    "        #other_arb_names.sort(key = lambda x: x[1],reverse=True)\n",
    "\n",
    "        index = 0\n",
    "        '''\n",
    "        for eng in eng_names:\n",
    "            src = []\n",
    "            indx = 0\n",
    "            while (len(src) < eng_multiplier) and indx < 150:\n",
    "                indx += 1\n",
    "                rnd = random.sample(all_eng_names, (eng_multiplier*2))\n",
    "                eng_eng_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(eng, name[0]) > 49]\n",
    "                src.extend(eng_eng_src_names)\n",
    "            \n",
    "            if indx >= 150 and len(src) < (eng_multiplier):\n",
    "                print(\"no eng negative added for name: {name}, key: {key}, added: {count}\".format(name=eng, key=key, count=len(src)))\n",
    "                rnd = random.sample(all_eng_names, (eng_multiplier - len(src)))\n",
    "                eng_eng_src_names = [name[0] for name in rnd if name[1] not in similar_keys]\n",
    "                src.extend(eng_eng_src_names)\n",
    "                \n",
    "            neg_eng_eng_pairs.extend(list(product([eng], src)))\n",
    "            src = []\n",
    "            indx = 0\n",
    "            while (len(src) < eng_multiplier) and indx < 150:\n",
    "                indx += 1\n",
    "                rnd = random.sample(all_arb_names, (eng_multiplier*2))\n",
    "                eng_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "                src.extend(eng_arb_src_names)\n",
    "            \n",
    "            if indx >= 150 and len(src) < (eng_multiplier):\n",
    "                print(\"no arb negative added for name: {name}, key: {key}, added: {count}\".format(name=eng, key=key, count=len(src)))\n",
    "                rnd = random.sample(all_arb_names, (eng_multiplier - len(src)))\n",
    "                eng_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys]\n",
    "                src.extend(eng_arb_src_names)\n",
    "            neg_eng_arb_pairs.extend(list(product([eng], src)))\n",
    "            '''\n",
    "            lower_limit = index * eng_multiplier\n",
    "            higher_limit = ((index + 1) * eng_multiplier)\n",
    "            index += 1\n",
    "            src = eng_eng_src_names[lower_limit:higher_limit]\n",
    "            #src = [item[0] for item in src]\n",
    "            neg_eng_eng_pairs.extend(list(product([eng], src)))\n",
    "            src = eng_arb_src_names[lower_limit:higher_limit]\n",
    "            #src = [item[0] for item in src]\n",
    "            neg_eng_arb_pairs.extend(list(product([eng], src)))\n",
    "            '''\n",
    "        #index = 0\n",
    "        for arb in arb_names:\n",
    "            src = []\n",
    "            indx = 0\n",
    "            while (len(src) < arb_multiplier) and indx < 150:\n",
    "                indx += 1\n",
    "                rnd = random.sample(all_arb_names, (arb_multiplier*3))\n",
    "                arb_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb, name[0]) > 49]\n",
    "                src.extend(arb_arb_src_names)\n",
    "            \n",
    "            if indx >= 150 and len(src) < (arb_multiplier):\n",
    "                print(\"no arb negative added for name: {name}, key: {key}, added: {count}\".format(name=arb, key=key, count=len(src)))\n",
    "                rnd = random.sample(all_arb_names, (arb_multiplier - len(src)))\n",
    "                arb_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys]\n",
    "                src.extend(arb_arb_src_names)\n",
    "\n",
    "            neg_arb_arb_pairs.extend(list(product([arb], src)))\n",
    "            '''\n",
    "            lower_limit = index * arb_multiplier\n",
    "            higher_limit = ((index + 1) * arb_multiplier)\n",
    "            index += 1\n",
    "            src = arb_arb_src_names[lower_limit:higher_limit]\n",
    "            #src = [item[0] for item in src]\n",
    "            neg_arb_arb_pairs.extend(list(product([arb], src)))\n",
    "            '''\n",
    "    \n",
    "    df = pd.DataFrame(neg_eng_eng_pairs)\n",
    "    df.to_csv(folder + 'neg_eng_eng_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_eng_arb_pairs)\n",
    "    df.to_csv(folder + 'neg_eng_arb_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) \n",
    "    df = pd.DataFrame(neg_arb_arb_pairs)\n",
    "    df.to_csv(folder + 'neg_arb_arb_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE, mode = 'w', header=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = target_dir + \"eng_eng/\"\n",
    "file_name = 'pos_2x4_pairs.tsv'\n",
    "df = pd.read_csv(folder + file_name,sep='\\t', header=None)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "neg_eng_eng_pairs = []\n",
    "neg_eng_arb_pairs = []\n",
    "neg_arb_arb_pairs = []\n",
    "\n",
    "#similar_keys = [k for k in keys if k == key]\n",
    "eng_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_eng_names))\n",
    "arb_multiplier = math.ceil(len(eng_eng_no_dup_df) / len(all_arb_names))\n",
    "\n",
    "key = 1\n",
    "#similar_keys1 = set([name[1] for name in all_eng_names if any(s in name[0] for s in eng_names)])\n",
    "#similar_keys2 = set([name[1] for name in all_arb_names if any(s in name[0] for s in arb_names)])\n",
    "#similar_keys = similar_keys1.union(similar_keys2)\n",
    "similar_keys = [key]\n",
    "eng_names = names_dict[key]['eng']\n",
    "arb_names = names_dict[key]['arb']\n",
    "\n",
    "index = 0\n",
    "for eng in eng_names:\n",
    "    src = []\n",
    "    while len(src) < (eng_multiplier):\n",
    "        rnd = random.sample(all_eng_names, (eng_multiplier*2))\n",
    "        eng_eng_src_names = [name[0]for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(eng_names[0], name[0]) > 49]\n",
    "        src.extend(eng_eng_src_names)\n",
    "    '''\n",
    "    lower_limit = index * eng_multiplier\n",
    "    higher_limit = ((index + 1) * eng_multiplier)\n",
    "    index += 1\n",
    "    src = eng_eng_src_names[lower_limit:higher_limit]\n",
    "    #src = [item[0] for item in src]\n",
    "    '''\n",
    "    neg_eng_eng_pairs.extend(list(product([eng], src)))\n",
    "    src = []\n",
    "    while len(src) < (eng_multiplier):\n",
    "        rnd = random.sample(all_arb_names, (eng_multiplier))\n",
    "        eng_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "        src.extend(eng_arb_src_names)\n",
    "    #src = eng_arb_src_names[lower_limit:higher_limit]\n",
    "    #src = [item[0] for item in src]\n",
    "    neg_eng_arb_pairs.extend(list(product([eng], src)))\n",
    "\n",
    "index = 0\n",
    "for arb in arb_names:\n",
    "    '''\n",
    "    lower_limit = index * arb_multiplier\n",
    "    higher_limit = ((index + 1) * arb_multiplier)\n",
    "    index += 1\n",
    "    src = arb_arb_src_names[lower_limit:higher_limit]\n",
    "    #src = [item[0] for item in src]\n",
    "    '''\n",
    "    src = []\n",
    "    while len(src) < (arb_multiplier):\n",
    "        rnd = random.sample(all_arb_names, (arb_multiplier*100))\n",
    "        arb_arb_src_names = [name[0] for name in rnd if name[1] not in similar_keys and 100 > fuzz.partial_ratio(arb_names[0], name[0]) > 49]\n",
    "        src.extend(arb_arb_src_names)\n",
    "    \n",
    "    neg_arb_arb_pairs.extend(list(product([arb], src)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_arb_arb_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[item for sublist in t for item in sublist[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if len(eng_eng_df.columns) == 4:\n",
    "    eng_eng_df.drop(eng_eng_df.columns[0], axis=1, inplace=True)\n",
    "eng_eng_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "eng_eng_df = eng_eng_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_eng_df = eng_eng_df.reset_index()\n",
    "eng_eng_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "\n",
    "if len(eng_arb_df.columns) == 4:\n",
    "    eng_arb_df.drop(eng_arb_df.columns[0], axis=1, inplace=True)\n",
    "eng_arb_df = eng_arb_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "eng_arb_df = eng_arb_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_arb_df = eng_arb_df.reset_index()\n",
    "eng_arb_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "\n",
    "if len(arb_arb_df.columns) == 4:\n",
    "    arb_arb_df.drop(arb_arb_df.columns[0], axis=1, inplace=True)\n",
    "arb_arb_df = arb_arb_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "arb_arb_df = arb_arb_df.sample(frac=1).reset_index(drop=True)\n",
    "arb_arb_df = arb_arb_df.reset_index()\n",
    "arb_arb_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "\n",
    "if len(eng_arb_negative_df.columns) == 4:\n",
    "    eng_arb_negative_df.drop(eng_arb_negative_df.columns[0], axis=1, inplace=True)\n",
    "eng_arb_negative_df = eng_arb_negative_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "eng_arb_negative_df = eng_arb_negative_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_arb_negative_df = eng_arb_negative_df.reset_index()\n",
    "eng_arb_negative_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "\n",
    "if len(eng_eng_nagative_df.columns) == 4:\n",
    "    eng_eng_nagative_df.drop(eng_eng_nagative_df.columns[0], axis=1, inplace=True)\n",
    "eng_eng_nagative_df = eng_eng_nagative_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "eng_eng_nagative_df = eng_eng_nagative_df.reset_index(drop=True)\n",
    "eng_eng_nagative_df = eng_eng_nagative_df.reset_index()\n",
    "eng_eng_nagative_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "\n",
    "if len(arb_arb_negative_df.columns) == 4:\n",
    "    arb_arb_negative_df.drop(arb_arb_negative_df.columns[0], axis=1, inplace=True)\n",
    "arb_arb_negative_df = arb_arb_negative_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "arb_arb_negative_df = arb_arb_negative_df.sample(frac=1).reset_index(drop=True)\n",
    "arb_arb_negative_df = arb_arb_negative_df.reset_index()\n",
    "arb_arb_negative_df.columns = ['index', 'part1', 'part2', 'group_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## four part names\n",
    "if len(eng_eng_df.columns) == 4:\n",
    "    eng_eng_df.drop(eng_eng_df.columns[0], axis=1, inplace=True)\n",
    "eng_eng_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first').copy()\n",
    "eng_eng_df = eng_eng_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_eng_df = eng_eng_df.reset_index()\n",
    "eng_eng_df.columns = ['index', 'part1', 'part2', 'group_id']\n",
    "eng_eng_df['set_type'] = eng_eng_df['index'] - eng_eng_df['index'] % 4\n",
    "eng_eng_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_eng_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grp = eng_eng_df.groupby('set_type', sort=False)\n",
    "eng_eng_df['part1'] =  grp['part1'].apply(lambda x: ' '.join(x))\n",
    "eng_eng_df['part2'] = eng_eng_df.groupby('set_type')['part2'].apply(lambda x: ' '.join(x))\n",
    "eng_eng_df['group_id'] = eng_eng_df.groupby('set_type')['group_id'].apply(lambda x: ', '.join(str(x)))\n",
    "eng_eng_df = eng_eng_df.drop_duplicates(subset=['set_type'], keep='first').copy()\n",
    "eng_eng_df.drop('set_type', axis=1, inplace=True)\n",
    "eng_eng_df = eng_eng_df.sample(frac=1).reset_index(drop=True)\n",
    "eng_eng_df['similarity'] = 1\n",
    "cols = eng_eng_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "eng_eng_df = eng_eng_df[cols]\n",
    "#eng_eng_df.columns = ['similarity', 'part1', 'part2', 'group_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(eng_eng_df.drop_duplicates(subset=[1, 2], keep='first'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(eng_eng_df))\n",
    "print(len(eng_arb_df))\n",
    "print(len(arb_arb_df))\n",
    "\n",
    "\n",
    "eng_eng_no_dup_df = eng_eng_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "eng_arb_no_dup_df = eng_arb_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "arb_arb_no_dup_df = arb_arb_df.drop_duplicates(subset=[1, 2], keep='first')\n",
    "\n",
    "print(len(eng_eng_no_dup_df))\n",
    "print(len(eng_arb_no_dup_df))\n",
    "print(len(arb_arb_no_dup_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = set(eng_arb_df[eng_arb_df[3] == 2][2].tolist())\n",
    "for name in names:\n",
    "    print(name)\n",
    "    print(transliterateString(name.lower(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_arb_df[eng_arb_df[3] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "','.join(set(eng_arb_df[eng_arb_df[3] == 6][1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buck2uni = {\"'\": u\"\\u0621\", # hamza-on-the-line\n",
    "            \"|\": u\"\\u0622\", # madda\n",
    "            \">\": u\"\\u0623\", # hamza-on-'alif\n",
    "            \"&\": u\"\\u0624\", # hamza-on-waaw\n",
    "            \"<\": u\"\\u0625\", # hamza-under-'alif\n",
    "            \"}\": u\"\\u0626\", # hamza-on-yaa'\n",
    "            \"a\": u\"\\u0627\", # bare 'alif\n",
    "            \"b\": u\"\\u0628\", # baa'\n",
    "            \"p\": u\"\\u0629\", # taa' marbuuTa\n",
    "            \"t\": u\"\\u062A\", # taa'\n",
    "            \"v\": u\"\\u062B\", # thaa'\n",
    "            \"j\": u\"\\u062C\", # jiim\n",
    "            \"h\": u\"\\u062D\", # Haa'\n",
    "            \"x\": u\"\\u062E\", # khaa'\n",
    "            \"d\": u\"\\u062F\", # daal\n",
    "            \"*\": u\"\\u0630\", # dhaal\n",
    "            \"r\": u\"\\u0631\", # raa'\n",
    "            \"z\": u\"\\u0632\", # zaay\n",
    "            \"s\": u\"\\u0633\", # siin\n",
    "            \"sh\": u\"\\u0634\", # shiin\n",
    "            \"s\": u\"\\u0635\", # Saad\n",
    "            \"d\": u\"\\u0636\", # Daad\n",
    "            \"t\": u\"\\u0637\", # Taa'\n",
    "            \"z\": u\"\\u0638\", # Zaa' (DHaa')\n",
    "            \"e\": u\"\\u0639\", # cayn\n",
    "            \"g\": u\"\\u063A\", # ghayn\n",
    "            \"_\": u\"\\u0640\", # taTwiil\n",
    "            \"f\": u\"\\u0641\", # faa'\n",
    "            \"q\": u\"\\u0642\", # qaaf\n",
    "            \"k\": u\"\\u0643\", # kaaf\n",
    "            \"l\": u\"\\u0644\", # laam\n",
    "            \"m\": u\"\\u0645\", # miim\n",
    "            \"n\": u\"\\u0646\", # nuun\n",
    "            \"h\": u\"\\u0647\", # haa'\n",
    "            \"w\": u\"\\u0648\", # waaw\n",
    "            \"Y\": u\"\\u0649\", # 'alif maqSuura\n",
    "            \"y\": u\"\\u064A\", # yaa'\n",
    "            \"f\": u\"\\u064B\", # fatHatayn\n",
    "            \"n\": u\"\\u064C\", # Dammatayn\n",
    "            \"k\": u\"\\u064D\", # kasratayn\n",
    "            \"a\": u\"\\u064E\", # fatHa\n",
    "            \"u\": u\"\\u064F\", # Damma\n",
    "            \"i\": u\"\\u0650\", # kasra\n",
    "            \"~\": u\"\\u0651\", # shaddah\n",
    "            \"o\": u\"\\u0652\", # sukuun\n",
    "            \"`\": u\"\\u0670\", # dagger 'alif\n",
    "            \"{\": u\"\\u0671\", # waSla\n",
    "}\n",
    "\n",
    "def transString(string, reverse=0):\n",
    "    '''Given a Unicode string, transliterate into Buckwalter. To go from\n",
    "    Buckwalter back to Unicode, set reverse=1'''\n",
    "\n",
    "    for k, v in buck2uni.items():\n",
    "      if not reverse:\n",
    "            string = string.replace(v, k)\n",
    "      else:\n",
    "            string = string.replace(k, v)\n",
    "\n",
    "    return string\n",
    "\n",
    "print(transString(u'موتشاماد'))\n",
    "print(transString('mrHbA', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, getopt, codecs, os, re\n",
    "\n",
    "# Declare a dictionary with Buckwalter's ASCII symbols as the keys, and\n",
    "# their unicode equivalents as values.\n",
    "\n",
    "buck2uni = {\"'\": u\"\\u0621\", # hamza-on-the-line\n",
    "            \"|\": u\"\\u0622\", # madda\n",
    "            \">\": u\"\\u0623\", # hamza-on-'alif\n",
    "            \"&\": u\"\\u0624\", # hamza-on-waaw\n",
    "            \"<\": u\"\\u0625\", # hamza-under-'alif\n",
    "            \"}\": u\"\\u0626\", # hamza-on-yaa'\n",
    "            \"a\": u\"\\u0627\", # bare 'alif\n",
    "            \"b\": u\"\\u0628\", # baa'\n",
    "            \"p\": u\"\\u0629\", # taa' marbuuTa\n",
    "            \"t\": u\"\\u062A\", # taa'\n",
    "            \"v\": u\"\\u062B\", # thaa'\n",
    "            \"j\": u\"\\u062C\", # jiim\n",
    "            \"h\": u\"\\u062D\", # Haa'\n",
    "            \"x\": u\"\\u062E\", # khaa'\n",
    "            \"d\": u\"\\u062F\", # daal\n",
    "            \"*\": u\"\\u0630\", # dhaal\n",
    "            \"r\": u\"\\u0631\", # raa'\n",
    "            \"z\": u\"\\u0632\", # zaay\n",
    "            \"s\": u\"\\u0633\", # siin\n",
    "            \"sh\": u\"\\u0634\", # shiin\n",
    "            \"s\": u\"\\u0635\", # Saad\n",
    "            \"d\": u\"\\u0636\", # Daad\n",
    "            \"t\": u\"\\u0637\", # Taa'\n",
    "            \"z\": u\"\\u0638\", # Zaa' (DHaa')\n",
    "            \"e\": u\"\\u0639\", # cayn\n",
    "            \"g\": u\"\\u063A\", # ghayn\n",
    "            \"_\": u\"\\u0640\", # taTwiil\n",
    "            \"f\": u\"\\u0641\", # faa'\n",
    "            \"q\": u\"\\u0642\", # qaaf\n",
    "            \"k\": u\"\\u0643\", # kaaf\n",
    "            \"l\": u\"\\u0644\", # laam\n",
    "            \"m\": u\"\\u0645\", # miim\n",
    "            \"n\": u\"\\u0646\", # nuun\n",
    "            \"h\": u\"\\u0647\", # haa'\n",
    "            \"w\": u\"\\u0648\", # waaw\n",
    "            \"Y\": u\"\\u0649\", # 'alif maqSuura\n",
    "            \"y\": u\"\\u064A\", # yaa'\n",
    "            \"f\": u\"\\u064B\", # fatHatayn\n",
    "            \"n\": u\"\\u064C\", # Dammatayn\n",
    "            \"k\": u\"\\u064D\", # kasratayn\n",
    "            \"a\": u\"\\u064E\", # fatHa\n",
    "            \"u\": u\"\\u064F\", # Damma\n",
    "            \"i\": u\"\\u0650\", # kasra\n",
    "            \"~\": u\"\\u0651\", # shaddah\n",
    "            \"o\": u\"\\u0652\", # sukuun\n",
    "            \"`\": u\"\\u0670\", # dagger 'alif\n",
    "            \"{\": u\"\\u0671\", # waSla\n",
    "}\n",
    "\n",
    "# For a reverse transliteration (Unicode -> Buckwalter), a dictionary\n",
    "# which is the reverse of the above buck2uni is essential.\n",
    "\n",
    "uni2buck = {}\n",
    "\n",
    "# Iterate through all the items in the buck2uni dict.\n",
    "for (key, value) in buck2uni.items():\n",
    "    # The value from buck2uni becomes a key in uni2buck, and vice\n",
    "    # versa for the keys.\n",
    "    uni2buck[value] = key\n",
    "\n",
    "def transliterateString(inString, reverse=1):\n",
    "\n",
    "\tout = \"\"\n",
    "\t\n",
    "\t# For normal Buckwalter -> Unicode transliteration..\n",
    "\tif not reverse:\n",
    "\n",
    "\t\t# Loop over each character in the string, inString.\n",
    "\t\tfor char in inString:\n",
    "\t\t\t# Look up current char in the dictionary to get its\n",
    "\t\t\t# respective value. If there is no match, e.g., chars like\n",
    "\t\t\t# spaces, then just stick with the current char without any\n",
    "\t\t\t# conversion.\n",
    "\t\t\tout = out + buck2uni.get(char, char)\n",
    "\t\n",
    "\t# Same as above, just in the other direction.\n",
    "\telse:\n",
    "\n",
    "\t\tfor char in inString:\n",
    "\t\t\tout = out + uni2buck.get(char, char)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "print(transliterateString(u'موتشاماد'))\n",
    "print(transliterateString('mrHbA', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#eng_eng_df = eng_eng_df.sample(frac=1).reset_index(drop=True)\n",
    "#eng_eng_df = eng_eng_df.sample(frac=1).reset_index()\n",
    "\n",
    "\n",
    "eng_eng_all_pos_4x4_ordered = eng_eng_df[0:500].copy()\n",
    "'''\n",
    "eng_eng_all_pos_4x4_unordered = \n",
    "eng_eng_all_pos_3x4_initails = \n",
    "eng_eng_all_pos_3x4_ordered = \n",
    "eng_eng_all_pos_3x4_unordered = \n",
    "eng_eng_all_pos_2x4_ordered = \n",
    "eng_eng_all_pos_2x4_unordered = \n",
    "eng_eng_all_pos_1x4_any_order = \n",
    "eng_eng_all_pos_3x3_ordered = \n",
    "eng_eng_all_pos_3x3_unordered = \n",
    "eng_eng_all_pos_2x3_initails = \n",
    "eng_eng_all_pos_2x3_any_order = \n",
    "eng_eng_all_pos_1x3_any_order = \n",
    "eng_eng_all_pos_2x2_ordered = \n",
    "eng_eng_all_pos_2x2_unordered = \n",
    "eng_eng_all_pos_1x2_initails = \n",
    "eng_eng_all_pos_1x2_any_order = \n",
    "eng_eng_all_pos_1x1 = \n",
    "\n",
    "\n",
    "eng_eng_4neg_x_4neg = \n",
    "eng_eng_3neg_1pos_x_3neg_1pos = \n",
    "eng_eng_2neg_2pos_x_2neg_2pos = \n",
    "eng_eng_2neg_x_2neg =\n",
    "eng_eng_1neg_1pos_x_1neg_1pos = \n",
    "eng_eng_1neg_x_1neg = \n",
    "'''\n",
    "'''\n",
    "index = 0\n",
    "group_id = 1\n",
    "eng_eng_df['group'] = 1\n",
    "\n",
    "\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 52 == 0 | , 'set_type'] = eng_eng_df['index']\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 1, 'set_type'] = eng_eng_df['index'] - 1\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 2, 'set_type'] = eng_eng_df['index'] - 2\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 3, 'set_type'] = eng_eng_df['index'] - 3\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 4, 'set_type'] = eng_eng_df['index']\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 5, 'set_type'] = eng_eng_df['index'] - 1\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 6, 'set_type'] = eng_eng_df['index'] - 2\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 7, 'set_type'] = eng_eng_df['index']\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 8, 'set_type'] = eng_eng_df['index'] - 1\n",
    "eng_eng_df.loc[eng_eng_df['index'] % 10 == 9, 'set_type'] = eng_eng_df['index']\n",
    "eng_eng_df.sort_values('group')\n",
    "'''\n",
    "'''\n",
    "for i, row in eng_eng_df.iterrows():\n",
    "    index += 1\n",
    "    if index % 10 == 4 or index % 10 == 7 or index % 10 == 8 or index % 10 == 0:\n",
    "        group_id += 1\n",
    "    eng_eng_df.loc[i,'group'] = group_id\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_eng_all_pos_4x4_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "frames = [eng_ara_df, eng_eng_df, ara_ara_df, eng_arb_negative_df, eng_eng_nagative_df, ara_ara_negative_df]\n",
    "all_pairs_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_given_names['eng_variants'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_given_names['eng_variants'].map(len).max())\n",
    "\n",
    "lengths = top_given_names[top_given_names['eng_variants'].str.len() >= 1]['eng_variants'].tolist()\n",
    "lengths = [len(sublist.split(',')) for sublist in lengths]\n",
    "print(max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import csv\n",
    "all_pairs_df.to_csv('/home/jupyter/notebooks/PoC/data-preparation/output/understanding_data/all_pairs.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['good_eng_variants', 'good_arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottom_given_names = given_names[given_names['count'] < 10]\n",
    "bottom_given_names = bottom_given_names.copy()\n",
    "bottom_given_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(top_given_names[top_given_names['eng'] == \"MOHAMMED\"]['eng_variants'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "bottom_given_names['eng_variants'] = bottom_given_names.groupby(['trimmed_arb'])['trimmed_eng'].transform(format_variants_list)\n",
    "bottom_given_names['arb_variants'] = bottom_given_names.groupby(['trimmed_eng'])['trimmed_arb'].transform(format_variants_list)\n",
    "'''\n",
    "\n",
    "print(bottom_given_names.shape)\n",
    "bottom_given_names = bottom_given_names.drop_duplicates(subset=['trimmed_eng', 'trimmed_arb'], keep='first').copy()\n",
    "print(bottom_given_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "frames = [eng_ara_df, eng_eng_df, ara_ara_df, eng_arb_negative_df, eng_eng_nagative_df, ara_ara_negative_df]\n",
    "all_pairs_df = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def create_negative_both(row):\n",
    "    global top_given_names, all_eng_given_names, all_arb_given_names\n",
    "    \n",
    "    eng_variants = list(set(row['eng_variants'].split(',')))\n",
    "    arb_variants = list(set(row['arb_variants'].split(',')))\n",
    "    \n",
    "    #print(eng_variants)\n",
    "    #print(arb_variants)\n",
    "    \n",
    "    matched_variants = top_given_names[top_given_names['trimmed_eng'].isin(eng_variants)\n",
    "                                       | top_given_names['trimmed_arb'].isin(arb_variants)]\n",
    "    \n",
    "    eng_matches = list(set(matched_variants['eng_variants'].tolist()))\n",
    "    arb_matches = list(set(matched_variants['arb_variants'].tolist()))\n",
    "    \n",
    "    \n",
    "    flat_eng_matches = list(set([item for sublist in eng_matches for item in sublist.split(',')]))\n",
    "    flat_arb_matches = list(set([item for sublist in arb_matches for item in sublist.split(',')]))\n",
    "    \n",
    "    #print(flat_eng_matches)\n",
    "    #print(flat_arb_matches)\n",
    "    \n",
    "    matched_variants = top_given_names[top_given_names['trimmed_eng'].isin(flat_eng_matches)\n",
    "                                       | top_given_names['trimmed_arb'].isin(flat_arb_matches)]\n",
    "    \n",
    "    eng_matches = list(set(matched_variants['eng_variants'].tolist()))\n",
    "    arb_matches = list(set(matched_variants['arb_variants'].tolist()))\n",
    "    \n",
    "    #print(eng_matches)\n",
    "    #print(arb_matches)\n",
    "    \n",
    "    eng_variants = sorted(set([item for sublist in eng_matches for item in sublist.split(',')])) \n",
    "    arb_variants = sorted(set([item for sublist in arb_matches for item in sublist.split(',')]))\n",
    "    #print(eng_variants)\n",
    "    #print(arb_variants)\n",
    "    \n",
    "    eng_word_length = len(row['trimmed_eng'])\n",
    "    good_eng_variants = [item for item in eng_variants if (eng_word_length * 0.3 < len(item) <= eng_word_length * 2 and item not in row['trimmed_eng']) or item == row['trimmed_eng']]\n",
    "    arb_word_length = len(row['trimmed_arb'])\n",
    "    good_arb_variants = [item for item in arb_variants if (arb_word_length * 0.3 < len(item) <= arb_word_length * 2 and item not in row['trimmed_arb']) or item == row['trimmed_arb']]\n",
    "\n",
    "    eng_variants_len = len(good_eng_variants)\n",
    "    arb_variants_len = len(good_arb_variants)\n",
    "    \n",
    "    ### making good arabic and english variants of same length\n",
    "    if eng_variants_len > arb_variants_len:\n",
    "        quotient, modulo = divmod(eng_variants_len, arb_variants_len)\n",
    "        extension = random.sample(good_arb_variants, modulo)\n",
    "        good_arb_variants = [repeated for value in good_arb_variants for repeated in repeat(value, quotient)]\n",
    "        good_arb_variants.extend(extension)\n",
    "    elif arb_variants_len > eng_variants_len:\n",
    "        quotient, modulo = divmod(arb_variants_len, eng_variants_len)\n",
    "        extension = random.sample(good_eng_variants, modulo)\n",
    "        good_eng_variants = [repeated for value in good_eng_variants for repeated in repeat(value, quotient)]\n",
    "        good_eng_variants.extend(extension)\n",
    "    \n",
    "    eng_variants_len = len(good_eng_variants)\n",
    "    arb_variants_len = len(good_arb_variants)\n",
    "    \n",
    "    random_eng_negative = []\n",
    "    random_arb_negative = []\n",
    "    \n",
    "    if (len(all_eng_given_names) - eng_variants_len) > (eng_variants_len * eng_variants_len):\n",
    "        random_eng_negative = random.sample(all_eng_given_names, (eng_variants_len * eng_variants_len))\n",
    "        random_eng_negative = list(np.setdiff1d(random_eng_negative, good_eng_variants, assume_unique=True))\n",
    "    \n",
    "    else:\n",
    "        random_eng_negative = list(np.setdiff1d(all_eng_given_names, good_eng_variants, assume_unique=True))\n",
    "        '''\n",
    "        quotient, modulo = divmod(eng_variants_len * (eng_variants_len - 1), len(random_eng_negative))\n",
    "        extension = random.sample(random_eng_negative, modulo)\n",
    "        random_eng_negative = [repeated for value in random_eng_negative for repeated in repeat(value, quotient)]\n",
    "        random_eng_negative.extend(extension)\n",
    "        ''' \n",
    "    if (len(all_arb_given_names) - arb_variants_len) > (arb_variants_len * arb_variants_len):\n",
    "        random_arb_negative = random.sample(all_arb_given_names, (arb_variants_len * arb_variants_len))\n",
    "        random_arb_nagative = list(np.setdiff1d(random_arb_negative, good_arb_variants, assume_unique=True))\n",
    "    \n",
    "    else:\n",
    "        random_arb_negative = list(np.setdiff1d(all_arb_given_names, good_arb_variants, assume_unique=True))\n",
    "        '''\n",
    "        quotient, modulo = divmod(arb_variants_len * (arb_variants_len - 1), len(random_arb_negative))\n",
    "        extension = random.sample(random_arb_negative, modulo)\n",
    "        random_arb_negative = [repeated for value in random_arb_negative for repeated in repeat(value, quotient)]\n",
    "        random_arb_negative.extend(extension)\n",
    "        '''\n",
    "    \n",
    "    row['eng_variants'] = ','.join(eng_variants)\n",
    "    row['arb_variants'] = ','.join(arb_variants)\n",
    "    row['good_eng_variants'] = ','.join(good_eng_variants)\n",
    "    row['good_arb_variants'] = ','.join(good_arb_variants)\n",
    "    row['negative_eng_variants'] = ','.join(random_eng_negative)\n",
    "    row['negative_arb_variants'] = ','.join(random_arb_negative)\n",
    "    \n",
    "    return row\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "def format_list_both(row):\n",
    "    global top_given_names\n",
    "    \n",
    "    eng_variants = list(set(row['eng_variants'].split(',')))\n",
    "    arb_variants = list(set(row['arb_variants'].split(',')))\n",
    "    \n",
    "    matched_variants = top_given_names[top_given_names['trimmed_eng'].isin(eng_variants)\n",
    "                                       | top_given_names['trimmed_arb'].isin(arb_variants)]\n",
    "    \n",
    "    eng_matches = list(set(matched_variants['eng_variants'].tolist()))\n",
    "    arb_matches = list(set(matched_variants['arb_variants'].tolist()))\n",
    "    \n",
    "    \n",
    "    flat_eng_matches = list(set([item for sublist in eng_matches for item in sublist.split(',')]))\n",
    "    flat_arb_matches = list(set([item for sublist in arb_matches for item in sublist.split(',')]))\n",
    "        \n",
    "    matched_variants = top_given_names[top_given_names['trimmed_eng'].isin(flat_eng_matches)\n",
    "                                       | top_given_names['trimmed_arb'].isin(flat_arb_matches)]\n",
    "    \n",
    "    eng_matches = list(set(matched_variants['eng_variants'].tolist()))\n",
    "    arb_matches = list(set(matched_variants['arb_variants'].tolist()))\n",
    "    \n",
    "    eng_variants = sorted(set([item for sublist in eng_matches for item in sublist.split(',')])) \n",
    "    arb_variants = sorted(set([item for sublist in arb_matches for item in sublist.split(',')]))\n",
    "    \n",
    "    row['eng_variants'] = ','.join(eng_variants)\n",
    "    row['arb_variants'] = ','.join(arb_variants)\n",
    "    \n",
    "    return row\n",
    "\n",
    "def format_list_both2(row):\n",
    "    global top_given_names\n",
    "    \n",
    "    eng_variants = list(set(row['eng_variants'].split(',')))\n",
    "    arb_variants = list(set(row['arb_variants'].split(',')))\n",
    "    \n",
    "    matched_variants = top_given_names[top_given_names['trimmed_eng'].isin(eng_variants)\n",
    "                                       | top_given_names['trimmed_arb'].isin(arb_variants)]\n",
    "    \n",
    "    eng_matches = list(set(matched_variants['eng_variants'].tolist()))\n",
    "    arb_matches = list(set(matched_variants['arb_variants'].tolist()))\n",
    "    \n",
    "    eng_variants = list(set([item for sublist in eng_matches for item in sublist.split(',')]))\n",
    "    arb_variants = list(set([item for sublist in arb_matches for item in sublist.split(',')]))\n",
    "            \n",
    "    row['eng_variants'] = ','.join(eng_variants)\n",
    "    row['arb_variants'] = ','.join(arb_variants)\n",
    "    \n",
    "    return row\n",
    "    \n",
    "%%time\n",
    "\n",
    "## round 1\n",
    "print(top_given_names.shape)\n",
    "top_given_names['eng_variants'] = top_given_names.groupby(['trimmed_arb'])['trimmed_eng'].transform(format_variants_list)\n",
    "top_given_names['arb_variants'] = top_given_names.groupby(['trimmed_eng'])['trimmed_arb'].transform(format_variants_list)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "## round 2\n",
    "\n",
    "top_given_names = top_given_names.apply(format_list_both2, axis=1)\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "top_given_names = top_given_names.apply(format_list_both2, axis=1)\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "top_given_names = top_given_names.apply(remove_long_variants_both, axis=1)\n",
    "top_given_names = top_given_names.apply(create_negative, axis=1)\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def format_variants_list2(s):\n",
    "    lis = s.tolist()\n",
    "    flat_list = [item for sublist in lis for item in sublist.split(',')]\n",
    "\n",
    "    return ','.join(sorted(set(flat_list)))\n",
    "\n",
    "\n",
    "def format_list(s, df, colum_name, second_column_name):\n",
    "    a = set(s.split(','))\n",
    "    a = sorted(a)\n",
    "    ddd = df[df[colum_name].isin(a)][second_column_name].tolist()\n",
    "    flat_list = [item for sublist in ddd for item in sublist.split(',')]\n",
    "    ssss = ','.join(sorted(set(flat_list)))\n",
    "    return ssss\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "%%time\n",
    "## round 3\n",
    "top_given_names['eng_variants'] = top_given_names.groupby(['arb_variants'])['eng_variants'].transform(format_variants_list2)\n",
    "top_given_names['arb_variants'] = top_given_names.groupby(['eng_variants'])['arb_variants'].transform(format_variants_list2)\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "\n",
    "## round 4 optional\n",
    "top_given_names['eng_variants'] = top_given_names['eng_variants'].apply(lambda x: format_list(x, top_given_names, 'trimmed_eng', 'eng_variants'))\n",
    "top_given_names['arb_variants'] = top_given_names['arb_variants'].apply(lambda x: format_list(x, top_given_names, 'trimmed_arb', 'arb_variants'))\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "\n",
    "## round 5 optional\n",
    "top_given_names['eng_variants'] = top_given_names.groupby(['arb_variants'])['eng_variants'].transform(format_variants_list2)\n",
    "top_given_names['arb_variants'] = top_given_names.groupby(['eng_variants'])['arb_variants'].transform(format_variants_list2)\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "top_given_names['eng_variants'] = top_given_names.apply(lambda x: remove_long_variants(x, 'trimmed_eng', 'eng_variants'), axis=1)\n",
    "top_given_names['arb_variants'] = top_given_names.apply(lambda x: remove_long_variants(x, 'trimmed_arb', 'arb_variants'), axis=1)\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "top_given_names['eng_variants'] = top_given_names['eng_variants'].apply(lambda x: format_list(x, top_given_names, 'trimmed_eng', 'eng_variants'))\n",
    "top_given_names['arb_variants'] = top_given_names['arb_variants'].apply(lambda x: format_list(x, top_given_names, 'trimmed_arb', 'arb_variants'))\n",
    "\n",
    "print(top_given_names.shape)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first').copy()\n",
    "print(top_given_names.shape)\n",
    "\n",
    "\n",
    "top_given_names['arb_variants'] = top_given_names.groupby(['trimmed_eng'])['trimmed_arb'].transform(lambda x: ','.join(x))\n",
    "top_given_names['arb_variants'] = top_given_names['arb_variants'].apply(format_variants_list)\n",
    "top_given_names = top_given_names.drop_duplicates(subset=['eng_variants', 'arb_variants'], keep='first')\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
