{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "\n",
    "def config(filename='prepare_data.ini', section='phonetic'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    # read connection parameters\n",
    "    params = config()\n",
    "\n",
    "    # connect to the PostgreSQL server\n",
    "    conn = psycopg2.connect(**params)\n",
    "    print('Connected to the PostgreSQL database...')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def read_dataframe():\n",
    "    conn = db_connect()\n",
    "    result = None\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM TESTING_DATA.NAME_VECTORS\n",
    "            \"\"\"\n",
    "        result = pd.read_sql(query, con=conn, index_col='id')\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')\n",
    "            \n",
    "    print(\"data retrieved from database with size: {size}\".format(size= result.shape))\n",
    "    return result\n",
    "\n",
    "def store_dataframe(df):\n",
    "    conn = db_connect()\n",
    "    try:\n",
    "        result = df.to_sql(\"TESTING_DATA.NAME_VECTORS\", con=conn, chunksize=20000)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')\n",
    "            \n",
    "    print(\"data retrieved from database with size: {size}\".format(size= result.shape))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "eng_gnames = pd.DataFrame()\n",
    "eng_fnames = pd.DataFrame()\n",
    "arb_gnames = pd.DataFrame()\n",
    "arb_fnames = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_english_variantions(validate = False): \n",
    "    global arabic_total_result, english_total_result\n",
    "    iterable_list = arabic_total_result[:]\n",
    "    for arabic_name in iterable_list:\n",
    "        english_list = get_english_variants(arabic_name)\n",
    "        english_total_result += get_top_frequency_names(english_list)\n",
    "    english_total_result = list(set(english_total_result))\n",
    "    \n",
    "    if validate:\n",
    "        iterable_list = english_total_result[:]\n",
    "        for english_name in iterable_list:\n",
    "            if not validate_english_name_by_arabic_variations(english_name, arabic_total_result):\n",
    "                print(\"removing name: {name}\".format(name=english_name))\n",
    "                english_total_result.remove(english_name)\n",
    "        english_total_result.sort()\n",
    "        \n",
    "                \n",
    "def get_arabic_variantions(validate = False): \n",
    "    global arabic_total_result, english_total_result\n",
    "    iterable_list = english_total_result[:]\n",
    "    for english_name in iterable_list:\n",
    "        arabic_list = get_arabic_variants(english_name)\n",
    "        arabic_total_result += get_top_frequency_names(arabic_list)\n",
    "    arabic_total_result = list(set(arabic_total_result))\n",
    "\n",
    "    if validate:\n",
    "        iterable_list = arabic_total_result[:]\n",
    "        for arabic_name in iterable_list:\n",
    "            if not validate_arabic_name_by_english_variations(arabic_name, english_total_result):\n",
    "                print(\"removing name: {name}\".format(name=arabic_name))\n",
    "                arabic_total_result.remove(arabic_name)\n",
    "\n",
    "        arabic_total_result.sort()\n",
    "             \n",
    "            \n",
    "def get_english_variants(arabic_name):\n",
    "    global df\n",
    "    \n",
    "    if df.empty:\n",
    "        df = read_dataframe()   \n",
    "    \n",
    "    result = {}\n",
    "    a = df[df['arb'] == arabic_name]\n",
    "    b = a[['eng', 'count']].groupby('eng').sum()\n",
    "    result = b.to_dict()['count']\n",
    "    return result\n",
    "\n",
    "def get_arabic_variants(english_name): \n",
    "    global df\n",
    "    \n",
    "    if df.empty:\n",
    "        df = read_dataframe()\n",
    "    \n",
    "    result = {}\n",
    "    a = df[df['eng'] == english_name]\n",
    "    b = a[['arb', 'count']].groupby('arb').sum()\n",
    "    result = b.to_dict()['count']\n",
    "    \n",
    "    return result\n",
    "\n",
    "def validate_arabic_name_by_english_variations(arabic_name, valid_english_variations):\n",
    "    english_variations = get_english_variants(arabic_name)\n",
    "    total_valid_count = 0\n",
    "    total_invalid_count = 0\n",
    "    \n",
    "    total = sum(english_variations.values())\n",
    "    if total < 3:\n",
    "        return False\n",
    "\n",
    "    for key, val in english_variations.items():\n",
    "        if key in valid_english_variations:\n",
    "            total_valid_count += val\n",
    "        else:\n",
    "            total_invalid_count += val\n",
    "    \n",
    "    #print(\"for {name}: valid: {valid}, invalid: {invalid}\".format(name=arabic_name, valid=total_valid_count, invalid=total_invalid_count))\n",
    "    \n",
    "    if total_valid_count < 3:\n",
    "        return False\n",
    "\n",
    "    if total_valid_count > total_invalid_count or total_valid_count > 100:\n",
    "        return True\n",
    "    \n",
    "    #print(\"english variations for {name} are: {dic}\".format(name=arabic_name, dic=english_variations))\n",
    "    return False\n",
    "    \n",
    "def validate_english_name_by_arabic_variations(english_name, valid_arabic_variations):\n",
    "    arabic_variations = get_arabic_variants(english_name)\n",
    "    total_valid_count = 0\n",
    "    total_invalid_count = 0\n",
    "    \n",
    "    total = sum(arabic_variations.values())\n",
    "    if total < 3:\n",
    "        return False\n",
    "    \n",
    "    for key, val in arabic_variations.items():\n",
    "        if key in valid_arabic_variations:\n",
    "            total_valid_count += val\n",
    "        else:\n",
    "            total_invalid_count += val\n",
    "    \n",
    "    #print(\"for {name}: valid: {valid}, invalid: {invalid}\".format(name=arabic_name, valid=total_valid_count, invalid=total_invalid_count))\n",
    "\n",
    "    if total_valid_count < 3:\n",
    "        return False\n",
    "    \n",
    "    if total_valid_count > total_invalid_count or total_valid_count > 100:\n",
    "        return True\n",
    "    \n",
    "    #print(\"arabic variations for {name} are: {dic}\".format(name=english_name, dic=arabic_variations))\n",
    "    return False\n",
    "\n",
    "def get_top_frequency_names(list):\n",
    "    total = sum(list.values())\n",
    "    lower_accepted_frequency = 100\n",
    "    threshold = 10\n",
    "    \n",
    "    max_value = max(list.values())\n",
    "    if total > 6561:\n",
    "        threshold = 1\n",
    "    else:\n",
    "        threshold -= total**(1./4.)\n",
    "        \n",
    "    #print(\"threshold: {thre}, total: {tot}\".format(thre=threshold, tot=total))\n",
    "    matched_list = [key for key, val in list.items() \n",
    "                    if len(key) > 2 and \n",
    "                    key not in top_noise_data and \n",
    "                    (val / total * 100 > threshold or val >= lower_accepted_frequency)]\n",
    "    #print(\"top matched_list: {thre}\".format(thre=matched_list))\n",
    "    not_matched_list = [ (key, val) for key, val in list.items() if val / total * 100 <= threshold and val < lower_accepted_frequency]\n",
    "    matched_list_with_composite = [key for key, val in list.items() \n",
    "                                   if any(match in key and len(key) < len(match) * 2  for match in matched_list)]\n",
    "    if(len(matched_list_with_composite) - len(matched_list) > 3):\n",
    "        return matched_list\n",
    "    \n",
    "    return matched_list_with_composite\n",
    "\n",
    "def get_random_names(names, number=20):\n",
    "    rnd = []\n",
    "    rnd.extend(np.random.randint(low=0, high=int(np.floor(len(names) * 0.01)), size=int(np.ceil(number * 0.5))))\n",
    "    rnd.extend(np.random.randint(low=int(np.floor(len(names) * 0.01)+1), high=int(np.floor(len(names) * 0.5)), size=int(np.ceil(number * 0.3))))\n",
    "    rnd.extend(np.random.randint(low=int(np.floor(len(names) * 0.5)+1), high=len(names), size=int(np.ceil(number * 0.2))))\n",
    "    return [names.loc[rnd[num],'name'] for num in range(number)]\n",
    "    \n",
    "def rnd_english_given_names(number=20):\n",
    "    global eng_gnames\n",
    "    \n",
    "    if eng_gnames.empty:\n",
    "        eng_gnames = read_eng_given_names()\n",
    "\n",
    "    return get_random_names(eng_gnames, number)\n",
    "\n",
    "def rnd_arabic_given_names(number=20):\n",
    "    global arb_gnames\n",
    "    \n",
    "    if arb_gnames.empty:\n",
    "        arb_gnames = read_arb_given_names()\n",
    "\n",
    "    return get_random_names(arb_gnames, number)\n",
    "\n",
    "def rnd_english_family_names(number=20):\n",
    "    global eng_fnames\n",
    "    \n",
    "    if eng_fnames.empty:\n",
    "        eng_fnames = read_eng_family_names()\n",
    "\n",
    "    return get_random_names(eng_fnames, number)\n",
    "\n",
    "def rnd_arabic_family_names(number=20):\n",
    "    global arb_fnames\n",
    "    \n",
    "    if arb_fnames.empty:\n",
    "        arb_fnames = read_arb_family_names()\n",
    "\n",
    "    return get_random_names(arb_fnames, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = db_connect()\n",
    "print(con)\n",
    "df = read_dataframe()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%inline matplotlib\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_graph(graph):\n",
    "\n",
    "    # extract nodes from graph\n",
    "    nodes = set([n1 for n1, n2 in graph] + [n2 for n1, n2 in graph])\n",
    "\n",
    "    # create networkx graph\n",
    "    G=nx.Graph()\n",
    "\n",
    "    # add nodes\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "\n",
    "    # add edges\n",
    "    for edge in graph:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # draw graph\n",
    "    pos = nx.shell_layout(G)\n",
    "    nx.draw(G, pos)\n",
    "\n",
    "    # show graph\n",
    "    plt.show()\n",
    "\n",
    "# draw example\n",
    "graph = [(20, 21),(21, 22),(22, 23), (23, 24),(24, 25), (25, 20)]\n",
    "draw_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%inline matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_map = {'e': 1.0,\n",
    "           'a': 0.1}\n",
    "\n",
    "\n",
    "attr_dict = nx.get_node_attributes(G,'t')\n",
    "values = [val_map[attr_dict[node]] for node in nx.get_node_attributes(G,'t')]\n",
    "\n",
    "pos=nx.spring_layout(G) # positions for all nodes\n",
    "nx.draw(G,pos=pos, node_color=values)\n",
    "nx.draw_networkx_labels(G,pos=pos)\n",
    "nx.draw_networkx_edge_labels(G,pos=pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from configparser import ConfigParser\n",
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def config(filename='prepare_data.ini', section='phonetic'):\n",
    "    parser = ConfigParser()\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    params = config()\n",
    "    conn = psycopg2.connect(**params)\n",
    "    print('Connected to the PostgreSQL database...')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def read_dataframe(query):\n",
    "    conn = db_connect()\n",
    "    result = None\n",
    "    try:\n",
    "        result = pd.read_sql(query, con=conn)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_graph(query, pickle_file):\n",
    "    df = read_dataframe(query)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    for index1, row1 in df.iterrows():\n",
    "        for index2, row2 in df.iterrows():\n",
    "            name1 = row1['gn']\n",
    "            name2 = row2['gn']\n",
    "            \n",
    "            if not G.has_edge(name1, name2):\n",
    "                G.add_edge(name1, name2, weight=random.random())\n",
    "    nx.write_gpickle(G,pickle_file)\n",
    "\n",
    "def generate_given_names_graph():\n",
    "    query = \"\"\"\n",
    "            SELECT DISTINCT(ENG) AS GN FROM testing_data.GIVEN_NAMES_MASTER\n",
    "            WHERE ENG IS NOT NULL AND ENG != ''\n",
    "            LIMIT 10000\n",
    "            \"\"\"\n",
    "            \n",
    "    generate_graph(query, \"/home/jupyter/notebooks/PoC/data-preparation/pickle/test_gn_similarity_graph.gpickle\")\n",
    "\n",
    "def read_given_names_graph():\n",
    "    return nx.read_gpickle(\"/home/jupyter/notebooks/PoC/data-preparation/pickle/test_gn_similarity_graph.gpickle\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_given_names_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from numpy.linalg import solve, norm\n",
    "from numpy.random import rand\n",
    "import random\n",
    "import psycopg2\n",
    "from configparser import ConfigParser\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def config(filename='prepare_data.ini', section='phonetic'):\n",
    "    parser = ConfigParser()\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    params = config()\n",
    "    conn = psycopg2.connect(**params)\n",
    "    print('Connected to the PostgreSQL database...')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def read_dataframe(query):\n",
    "    conn = db_connect()\n",
    "    result = None\n",
    "    try:\n",
    "        result = pd.read_sql(query, con=conn)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_mx(query, pickle_file):\n",
    "    df = read_dataframe(query)\n",
    "    A = lil_matrix((10000, 10000))\n",
    "\n",
    "    for index1, row1 in df.iterrows():\n",
    "        for index2, row2 in df.iterrows():\n",
    "            name1 = row1['gn']\n",
    "            name2 = row2['gn']\n",
    "            similarity = random.random()\n",
    "            \n",
    "            if (similarity >= 0.6):\n",
    "                A[index1, index2] = similarity\n",
    "\n",
    "def generate_given_names_mx():\n",
    "    query = \"\"\"\n",
    "            SELECT DISTINCT(ENG) AS GN FROM testing_data.GIVEN_NAMES_MASTER\n",
    "            WHERE ENG IS NOT NULL AND ENG != ''\n",
    "            LIMIT 10000\n",
    "            \"\"\"\n",
    "            \n",
    "    generate_mx(query, \"/home/jupyter/notebooks/PoC/data-preparation/pickle/test_gn_similarity_graph.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_given_names_mx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "sentence_m = np.array([1, 1, 1, 1, 0, 2, 0, 0, 0]) \n",
    "sentence_h = np.array([2, 2, 2, 2, 0, 4, 0, 0, 0])\n",
    "sentence_w = np.array([0, 0, 0, 1, 3, 0, 1, 1, 4])\n",
    "\n",
    "print(cos_sim(sentence_m, sentence_h))\n",
    "print(cos_sim(sentence_m, sentence_w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sentence_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = np.random.randint(10, size=8).astype(dtype=np.float32)\n",
    "print(np.array2string(v, precision=0, separator=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_vector = read_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(name_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in name_vector.iterrows():\n",
    "    vect = np.fromstring(row['vector'].strip('[]'), dtype=int, sep=',')\n",
    "    row['vector'] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in name_vector.iterrows():\n",
    "    cos = cos_sim(v, row['vector'])\n",
    "    if cos > 0.99:\n",
    "        print(\"{a} : {b} -> {c}\".format(a=v, b=row['vector'], c=cos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.array([[]])\n",
    "i = 0\n",
    "for index, row in name_vector.iterrows():\n",
    "    if dataset.size == 0 :\n",
    "        dataset = np.array([row['vector']])\n",
    "    else :\n",
    "        dataset = np.vstack([dataset, row['vector']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.astype(dtype=np.float32)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "bck = copy.deepcopy(dataset)\n",
    "print(bck[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import falconn\n",
    "import timeit\n",
    "import math\n",
    "\n",
    "dataset = copy.deepcopy(bck)\n",
    "\n",
    "number_of_queries = 1000\n",
    "# we build only 50 tables, increasing this quantity will improve the query time\n",
    "# at a cost of slower preprocessing and larger memory footprint, feel free to\n",
    "# play with this number\n",
    "number_of_tables = 50\n",
    "\n",
    "\n",
    "\n",
    "# It's important not to use doubles, unless they are strictly necessary.\n",
    "# If your dataset consists of doubles, convert it to floats using `astype`.\n",
    "assert dataset.dtype == np.float32\n",
    "\n",
    "# Choose random data points to be queries.\n",
    "print('Generating queries')\n",
    "np.random.seed(4057218)\n",
    "np.random.shuffle(dataset)\n",
    "queries = dataset[len(dataset) - number_of_queries:]\n",
    "dataset = dataset[:len(dataset) - number_of_queries]\n",
    "print('Done')\n",
    "\n",
    "# Perform linear scan using NumPy to get answers to the queries.\n",
    "print('Solving queries using linear scan')\n",
    "t1 = timeit.default_timer()\n",
    "answers = []\n",
    "for query in queries:\n",
    "    answers.append(np.dot(dataset, query).argmax())\n",
    "t2 = timeit.default_timer()\n",
    "print('Done')\n",
    "print('Linear scan time: {} per query'.format((t2 - t1) / float(\n",
    "    len(queries))))\n",
    "\n",
    "# Center the dataset and the queries: this improves the performance of LSH quite a bit.\n",
    "print('Centering the dataset and queries')\n",
    "center = np.mean(dataset, axis=0)\n",
    "dataset -= center\n",
    "queries -= center\n",
    "print('Done')\n",
    "\n",
    "params_cp = falconn.LSHConstructionParameters()\n",
    "params_cp.dimension = len(dataset[0])\n",
    "params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
    "params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
    "params_cp.l = number_of_tables\n",
    "# we set one rotation, since the data is dense enough,\n",
    "# for sparse data set it to 2\n",
    "params_cp.num_rotations = 1\n",
    "params_cp.seed = 5721840\n",
    "# we want to use all the available threads to set up\n",
    "params_cp.num_setup_threads = 0\n",
    "params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
    "# we build 18-bit hashes so that each table has\n",
    "# 2^18 bins; this is a good choise since 2^18 is of the same\n",
    "# order of magnitude as the number of data points\n",
    "falconn.compute_number_of_hash_functions(18, params_cp)\n",
    "\n",
    "print('Constructing the LSH table')\n",
    "t1 = timeit.default_timer()\n",
    "table = falconn.LSHIndex(params_cp)\n",
    "table.setup(dataset)\n",
    "t2 = timeit.default_timer()\n",
    "print('Done')\n",
    "print('Construction time: {}'.format(t2 - t1))\n",
    "\n",
    "query_object = table.construct_query_object()\n",
    "\n",
    "# find the smallest number of probes to achieve accuracy 0.9\n",
    "# using the binary search\n",
    "print('Choosing number of probes')\n",
    "number_of_probes = number_of_tables\n",
    "\n",
    "def evaluate_number_of_probes(number_of_probes):\n",
    "    query_object.set_num_probes(number_of_probes)\n",
    "    score = 0\n",
    "    for (i, query) in enumerate(queries):\n",
    "        if answers[i] in query_object.get_candidates_with_duplicates(\n",
    "                query):\n",
    "            score += 1\n",
    "    return float(score) / len(queries)\n",
    "\n",
    "while True:\n",
    "    accuracy = evaluate_number_of_probes(number_of_probes)\n",
    "    print('{} -> {}'.format(number_of_probes, accuracy))\n",
    "    if accuracy >= 0.9:\n",
    "        break\n",
    "    number_of_probes = number_of_probes * 2\n",
    "if number_of_probes > number_of_tables:\n",
    "    left = number_of_probes // 2\n",
    "    right = number_of_probes\n",
    "    while right - left > 1:\n",
    "        number_of_probes = (left + right) // 2\n",
    "        accuracy = evaluate_number_of_probes(number_of_probes)\n",
    "        print('{} -> {}'.format(number_of_probes, accuracy))\n",
    "        if accuracy >= 0.9:\n",
    "            right = number_of_probes\n",
    "        else:\n",
    "            left = number_of_probes\n",
    "    number_of_probes = right\n",
    "print('Done')\n",
    "print('{} probes'.format(number_of_probes))\n",
    "\n",
    "# final evaluation\n",
    "t1 = timeit.default_timer()\n",
    "score = 0\n",
    "for (i, query) in enumerate(queries):\n",
    "    if query_object.find_nearest_neighbor(query) == answers[i]:\n",
    "        score += 1\n",
    "t2 = timeit.default_timer()\n",
    "\n",
    "print('Query time: {}'.format((t2 - t1) / len(queries)))\n",
    "print('Precision: {}'.format(float(score) / len(queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.spatial import distance\n",
    "\n",
    "print(v)\n",
    "print(dataset[1])\n",
    "\n",
    "dist = 0\n",
    "for u in dataset:\n",
    "    d = distance.cosine(v, u)\n",
    "    #d = distance.euclidean(v, u)\n",
    "    if (d > dist):\n",
    "        dist = d\n",
    "    \n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(dataset, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the LSH table\n",
      "Done\n",
      "Construction time: 388.4927228866145\n",
      "Done\n",
      "50 probes\n",
      "query:            [ 0.63783312  0.00409418  0.50211561  0.30761099  0.22072244 -0.37277198\n",
      " -0.11898448  0.75035644  0.78763163 -0.82036281 -0.36739284 -0.6848284\n",
      "  0.91254425 -0.79613221  0.96386838 -0.78057832  0.30549878 -0.69954884\n",
      "  0.18261124  0.2515724  -0.95707363  0.73961365 -0.91854197 -0.80992085\n",
      "  0.32259238 -0.91326624 -0.67281902  0.14103541  0.91712958 -0.36514041\n",
      "  0.27035442 -0.94641179 -0.75013936  0.84753209 -0.49573171  0.54618055\n",
      "  0.97861308 -0.81299049  0.89324182  0.60030675 -0.36409688 -0.48318368\n",
      " -0.44730324 -0.3910405   0.09788305 -0.59802473 -0.17750613  0.19272159\n",
      " -0.63840258  0.19285232 -0.2698209  -0.47305384  0.66874039  0.14557534\n",
      " -0.85832685 -0.92510468  0.44801518  0.145027   -0.3924512   0.16517544]\n",
      "nearest_neighbor: [ 0.28347668 -0.34237945  0.24419194 -0.22370352  0.22561137  0.73121858\n",
      " -0.15567179 -0.10226461  0.36341268 -0.90843171 -0.78412026  0.66123503\n",
      "  0.75294632 -0.45857     0.32941011 -0.84380901  0.68819404  0.2749548\n",
      " -0.6074174  -0.22753589 -0.58513808  0.71721578 -0.05143839 -0.01295014\n",
      " -0.64428514 -0.81140471 -0.7114135  -0.48323563  0.90772879 -0.66745514\n",
      " -0.70856088 -0.20266846  0.39601815  0.62602097 -0.55086058  0.37238812\n",
      "  0.29060727 -0.67069942  0.8335855   0.23725946 -0.71086842 -0.85649526\n",
      " -0.85202104 -0.3672238   0.5312053  -0.81098437 -0.42431611  0.5091306\n",
      " -0.4684982   0.14601579  0.58946282  0.47169149  0.79551333  0.24178073\n",
      " -0.51108843 -0.88670564 -0.49497938  0.4537648   0.68229115 -0.31669143]\n",
      "Query time: 0.006911959499120712\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import falconn\n",
    "import timeit\n",
    "import math\n",
    "import copy\n",
    "\n",
    "#dataset = copy.deepcopy(bck)\n",
    "dataset = copy.deepcopy(random_vectors)\n",
    "dataset = dataset.astype(dtype=np.float32)\n",
    "\n",
    "# we build only 50 tables, increasing this quantity will improve the query time\n",
    "# at a cost of slower preprocessing and larger memory footprint, feel free to\n",
    "# play with this number\n",
    "number_of_tables = 50\n",
    "\n",
    "# It's important not to use doubles, unless they are strictly necessary.\n",
    "# If your dataset consists of doubles, convert it to floats using `astype`.\n",
    "assert dataset.dtype == np.float32\n",
    "\n",
    "\n",
    "params_cp = falconn.LSHConstructionParameters()\n",
    "params_cp.dimension = len(dataset[0])\n",
    "params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
    "params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
    "params_cp.l = number_of_tables\n",
    "# we set one rotation, since the data is dense enough,\n",
    "# for sparse data set it to 2\n",
    "params_cp.num_rotations = 2\n",
    "params_cp.seed = 5721840\n",
    "# we want to use all the available threads to set up\n",
    "params_cp.num_setup_threads = 0\n",
    "params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
    "# we build 24-bit hashes so that each table has\n",
    "# 2^24 bins; this is a good choise since 2^24 is of the same\n",
    "# order of magnitude as the number of data points\n",
    "falconn.compute_number_of_hash_functions(24, params_cp)\n",
    "\n",
    "print('Constructing the LSH table')\n",
    "t1 = timeit.default_timer()\n",
    "table = falconn.LSHIndex(params_cp)\n",
    "table.setup(dataset)\n",
    "t2 = timeit.default_timer()\n",
    "print('Done')\n",
    "print('Construction time: {}'.format(t2 - t1))\n",
    "\n",
    "query_object = table.construct_query_object()\n",
    "\n",
    "number_of_probes = number_of_tables\n",
    "#number_of_probes = 10\n",
    "\n",
    "print('Done')\n",
    "print('{} probes'.format(number_of_probes))\n",
    "\n",
    "# final evaluation\n",
    "t1 = timeit.default_timer()\n",
    "response = query_object.find_nearest_neighbor(query)\n",
    "print(\"query:            {q}\".format(q=query))\n",
    "print(\"nearest_neighbor: {nn}\".format(nn=dataset[response]))\n",
    "t2 = timeit.default_timer()\n",
    "\n",
    "print('Query time: {}'.format((t2 - t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 2* np.random.random_sample(60) -1\n",
    "query = query.astype(dtype=np.float32)\n",
    "#dataset -= np.mean(dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:            [-0.90300858 -0.5539453  -0.47259071 -0.51987958  0.72921056  0.03061111\n",
      "  0.899382    0.02109051 -0.48245111  0.80966711  0.2980493  -0.92306358\n",
      " -0.31926629  0.35113055 -0.18029171  0.21239281  0.38495556 -0.93474603\n",
      " -0.56777173  0.72636497  0.94998837 -0.24052756  0.11417535 -0.97955108\n",
      "  0.26304597 -0.85263938 -0.74149936  0.49073747 -0.44872081  0.58801275\n",
      "  0.79450661 -0.42468438  0.04616422 -0.69894981 -0.06934123 -0.20130967\n",
      " -0.38877609 -0.51590067  0.60183126 -0.57862329 -0.54407114  0.13731219\n",
      "  0.54318547  0.65324581  0.95106143  0.55723077  0.69782883  0.92653692\n",
      " -0.47858879 -0.10922093  0.28426006 -0.17109036  0.78654563  0.13084193\n",
      "  0.54457498  0.63398337 -0.36922461 -0.25434035 -0.43515331 -0.88885134]\n",
      "11411745\n",
      "Query time: 0.004988847300410271\n"
     ]
    }
   ],
   "source": [
    "# final evaluation\n",
    "t1 = timeit.default_timer()\n",
    "response = query_object.find_nearest_neighbor(query)\n",
    "print(\"query:            {q}\".format(q=query))\n",
    "#print(\"nearest_neighbor: {nn}\".format(nn=dataset[response]))\n",
    "print(response)\n",
    "t2 = timeit.default_timer()\n",
    "\n",
    "print('Query time: {}'.format((t2 - t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:          [-0.90300858 -0.5539453  -0.47259071 -0.51987958  0.72921056  0.03061111\n",
      "  0.899382    0.02109051 -0.48245111  0.80966711  0.2980493  -0.92306358\n",
      " -0.31926629  0.35113055 -0.18029171  0.21239281  0.38495556 -0.93474603\n",
      " -0.56777173  0.72636497  0.94998837 -0.24052756  0.11417535 -0.97955108\n",
      "  0.26304597 -0.85263938 -0.74149936  0.49073747 -0.44872081  0.58801275\n",
      "  0.79450661 -0.42468438  0.04616422 -0.69894981 -0.06934123 -0.20130967\n",
      " -0.38877609 -0.51590067  0.60183126 -0.57862329 -0.54407114  0.13731219\n",
      "  0.54318547  0.65324581  0.95106143  0.55723077  0.69782883  0.92653692\n",
      " -0.47858879 -0.10922093  0.28426006 -0.17109036  0.78654563  0.13084193\n",
      "  0.54457498  0.63398337 -0.36922461 -0.25434035 -0.43515331 -0.88885134]\n",
      "[15962089, 18412310, 20850940, 11451692, 17743573, 19443808, 11411745, 22993825, 2340536, 21709933, 21169797, 23372709, 23842807, 16176509, 21263416, 12875144, 21722886, 1087897]\n",
      "Query time: 0.008274488151073456\n",
      "25000000\n"
     ]
    }
   ],
   "source": [
    "# final evaluation\n",
    "t1 = timeit.default_timer()\n",
    "response = query_object.find_near_neighbors(query, 20)\n",
    "print(\"query:          {q}\".format(q=query))\n",
    "#print(\"near neighbors: {nn}\".format(nn=dataset[response]))\n",
    "print(response)\n",
    "t2 = timeit.default_timer()\n",
    "\n",
    "print('Query time: {}'.format((t2 - t1)))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "random_vectors = 2 * np.random.random_sample((25, 60)) - 1 \n",
    "random_vectors = random_vectors.astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(random_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(random_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = copy.deepcopy(random_vectors)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(random_vectors, open( \"pickle/random_vectors_25M.pickle\", \"wb\"), protocol=4)\n",
    "#random_vectors = pickle.load( open(\"pickle/random_vectors.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aaa\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# GET /queryscotches\n",
    "print(json.dumps(\"aaa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adf = pd.read_csv(\n",
    "    '/home/jupyter/notebooks/PoC/data-preparation/output/transliteration_datasets/arb_positive_trans.tsv', \n",
    "    delimiter='\\t', \n",
    "    index_col=0,\n",
    "    usecols=[0,1,2])\n",
    "edf = pd.read_csv(\n",
    "    '/home/jupyter/notebooks/PoC/data-preparation/output/transliteration_datasets/eng_positive_trans.tsv',\n",
    "    delimiter='\\t', \n",
    "    index_col=0,\n",
    "    usecols=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>arb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOHAMMED</td>\n",
       "      <td>محمد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OMAR</td>\n",
       "      <td>عمر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HASSAN</td>\n",
       "      <td>حسن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MARIAM</td>\n",
       "      <td>مريم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IBRAHIM</td>\n",
       "      <td>ابرهيم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MARIA</td>\n",
       "      <td>مارية</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SALEH</td>\n",
       "      <td>صالح</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALI</td>\n",
       "      <td>علي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KHALED</td>\n",
       "      <td>خالد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KAMAL</td>\n",
       "      <td>كمال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HUSSEIN</td>\n",
       "      <td>حسين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SALIM</td>\n",
       "      <td>سليم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MOHAMED</td>\n",
       "      <td>موهامد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SAEED</td>\n",
       "      <td>سعيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ABBAS</td>\n",
       "      <td>عباس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SAAD</td>\n",
       "      <td>سعد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ELIAS</td>\n",
       "      <td>الياس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FAISAL</td>\n",
       "      <td>فيصل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FATMA</td>\n",
       "      <td>فاطمه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NASSER</td>\n",
       "      <td>ناصر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ADEL</td>\n",
       "      <td>عادل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MAHMOUD</td>\n",
       "      <td>محمود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ADNAN</td>\n",
       "      <td>عدنان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SALAH</td>\n",
       "      <td>صلاح</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SULTAN</td>\n",
       "      <td>سلطان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SALEM</td>\n",
       "      <td>سالم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MARIE</td>\n",
       "      <td>ماري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MOHAMMAD</td>\n",
       "      <td>موهاماد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DANIEL</td>\n",
       "      <td>دانيل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JEAN</td>\n",
       "      <td>جين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571410</th>\n",
       "      <td>DALUSONG</td>\n",
       "      <td>دالوسونج</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571411</th>\n",
       "      <td>DALUWATHU MULLA</td>\n",
       "      <td>دالواتو مولاجى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571413</th>\n",
       "      <td>QIONIBARAVI</td>\n",
       "      <td>قيونيبارافي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571414</th>\n",
       "      <td>ABDELSHAHEED</td>\n",
       "      <td>عبدالشهيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571415</th>\n",
       "      <td>SEEADE</td>\n",
       "      <td>سيئدى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571416</th>\n",
       "      <td>ABD EL GADIR</td>\n",
       "      <td>عبدالقادر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571417</th>\n",
       "      <td>ELHIBIR</td>\n",
       "      <td>الحبر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571418</th>\n",
       "      <td>D ALWIS</td>\n",
       "      <td>دى الويس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571420</th>\n",
       "      <td>EL SABHI</td>\n",
       "      <td>السبحى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571422</th>\n",
       "      <td>EL RAMI</td>\n",
       "      <td>الرامى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571423</th>\n",
       "      <td>SEDUADUA</td>\n",
       "      <td>ضوا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571424</th>\n",
       "      <td>AL ABODI</td>\n",
       "      <td>العبودى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571425</th>\n",
       "      <td>DAMADWI</td>\n",
       "      <td>مضوى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571427</th>\n",
       "      <td>DAMANATHI</td>\n",
       "      <td>دماناثى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571428</th>\n",
       "      <td>ALGHWAZAY</td>\n",
       "      <td>الغوازى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571430</th>\n",
       "      <td>KAVALIAUSKAS</td>\n",
       "      <td>كاكا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571431</th>\n",
       "      <td>YDHIN</td>\n",
       "      <td>الدين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571432</th>\n",
       "      <td>DAMASKINA</td>\n",
       "      <td>سنا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571436</th>\n",
       "      <td>AL ABOODI</td>\n",
       "      <td>العبودى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571443</th>\n",
       "      <td>DAMAYANTHI KALA</td>\n",
       "      <td>داماينتى كالانس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571444</th>\n",
       "      <td>DAMAYATNHI</td>\n",
       "      <td>دامايانيت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571446</th>\n",
       "      <td>MALAGAMMANA</td>\n",
       "      <td>مالاجامانا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571448</th>\n",
       "      <td>DAMBBULAGALA</td>\n",
       "      <td>دامبولوجالا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571449</th>\n",
       "      <td>OHI</td>\n",
       "      <td>اوهى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571450</th>\n",
       "      <td>MOUSA ABBAS</td>\n",
       "      <td>موسى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571451</th>\n",
       "      <td>DAMBRAUSKAITE</td>\n",
       "      <td>برس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571452</th>\n",
       "      <td>KAPULANDE</td>\n",
       "      <td>كابولاندى جودا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571453</th>\n",
       "      <td>GARNES</td>\n",
       "      <td>جارنس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571458</th>\n",
       "      <td>ELZAKY</td>\n",
       "      <td>الزاكى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439881</th>\n",
       "      <td>test</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439882 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    eng              arb\n",
       "0              MOHAMMED             محمد\n",
       "1                  OMAR              عمر\n",
       "2                HASSAN              حسن\n",
       "3                MARIAM             مريم\n",
       "4               IBRAHIM           ابرهيم\n",
       "5                 MARIA            مارية\n",
       "6                 SALEH             صالح\n",
       "7                   ALI              علي\n",
       "8                KHALED             خالد\n",
       "9                 KAMAL             كمال\n",
       "10              HUSSEIN             حسين\n",
       "11                SALIM             سليم\n",
       "12              MOHAMED           موهامد\n",
       "13                SAEED             سعيد\n",
       "14                ABBAS             عباس\n",
       "15                 SAAD              سعد\n",
       "16                ELIAS            الياس\n",
       "17               FAISAL             فيصل\n",
       "18                FATMA            فاطمه\n",
       "19               NASSER             ناصر\n",
       "20                 ADEL             عادل\n",
       "21              MAHMOUD            محمود\n",
       "22                ADNAN            عدنان\n",
       "23                SALAH             صلاح\n",
       "24               SULTAN            سلطان\n",
       "25                SALEM             سالم\n",
       "26                MARIE             ماري\n",
       "27             MOHAMMAD          موهاماد\n",
       "28               DANIEL            دانيل\n",
       "29                 JEAN              جين\n",
       "...                 ...              ...\n",
       "571410         DALUSONG         دالوسونج\n",
       "571411  DALUWATHU MULLA   دالواتو مولاجى\n",
       "571413      QIONIBARAVI      قيونيبارافي\n",
       "571414     ABDELSHAHEED        عبدالشهيد\n",
       "571415           SEEADE            سيئدى\n",
       "571416     ABD EL GADIR        عبدالقادر\n",
       "571417          ELHIBIR            الحبر\n",
       "571418          D ALWIS         دى الويس\n",
       "571420         EL SABHI           السبحى\n",
       "571422          EL RAMI           الرامى\n",
       "571423         SEDUADUA              ضوا\n",
       "571424         AL ABODI          العبودى\n",
       "571425          DAMADWI             مضوى\n",
       "571427        DAMANATHI          دماناثى\n",
       "571428        ALGHWAZAY          الغوازى\n",
       "571430     KAVALIAUSKAS             كاكا\n",
       "571431            YDHIN            الدين\n",
       "571432        DAMASKINA              سنا\n",
       "571436        AL ABOODI          العبودى\n",
       "571443  DAMAYANTHI KALA  داماينتى كالانس\n",
       "571444       DAMAYATNHI        دامايانيت\n",
       "571446      MALAGAMMANA       مالاجامانا\n",
       "571448     DAMBBULAGALA      دامبولوجالا\n",
       "571449              OHI             اوهى\n",
       "571450      MOUSA ABBAS             موسى\n",
       "571451    DAMBRAUSKAITE              برس\n",
       "571452        KAPULANDE   كابولاندى جودا\n",
       "571453           GARNES            جارنس\n",
       "571458           ELZAKY           الزاكى\n",
       "439881             test             test\n",
       "\n",
       "[439882 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf.loc[len(edf)] = ['test', 'test']\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_pickle() got an unexpected keyword argument 'protocol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-88dedb1c38cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ae.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: to_pickle() got an unexpected keyword argument 'protocol'"
     ]
    }
   ],
   "source": [
    "adf.to_pickle(protocol=2, path='ae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edf.to_pickle('ea.pkl', protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adf = pd.read_pickle('ae.pkl')\n",
    "edf = pd.read_pickle('ea.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>arb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [eng, arb]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf.loc[edf['eng'] == 'OMASR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arb</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>صالح</td>\n",
       "      <td>SALEH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    arb    eng\n",
       "6  صالح  SALEH"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf.loc[adf['arb'] == 'صالح']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 65, 77, 90, 65]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"HAMZA\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 65, 77, 90, 65]\n"
     ]
    }
   ],
   "source": [
    "batch_tokens = []\n",
    "batch_tokens.append(list(\"HAMZA\".encode('utf-8')))\n",
    "\n",
    "for tokens in batch_tokens:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "print (io.DEFAULT_BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
